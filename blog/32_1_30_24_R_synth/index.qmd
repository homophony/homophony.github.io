---
title: "Midi and synthesis in R"
author: "Matt Crump"
date: 1/30/24
description: "Trying out a few R packages to handle MIDI data in dataframes, and play it with fluid synth."
image: "cover.jpg"
comments:
  giscus: 
    repo: CrumpLab/crumplab_comments
categories: 
  - midi
  - fluidsynth
  - rstats
execute: 
  echo: true
  message: false
  warning: false
format:
  html:
    code-fold: true
    code-summary: "Show the code"
---

```{python, eval = FALSE}
from diffusers import DiffusionPipeline
from transformers import set_seed
from PIL import Image
import torch
import random
import ssl
import os
ssl._create_default_https_context = ssl._create_unverified_context

#locate library
#model_id = "./stable-diffusion-v1-5"
model_id = "dreamshaper-xl-turbo"

pipeline = DiffusionPipeline.from_pretrained(
	pretrained_model_name_or_path = "../../../../bigFiles/huggingface/dreamshaper-xl-turbo/"
)

pipeline = pipeline.to("mps")

# Recommended if your computer has < 64 GB of RAM
pipeline.enable_attention_slicing("max")

prompt = "computer music. musical computer. music represented as bits going into the fabric of the universe. 80s cartoon retro."

for s in range(30):
  for n in [5,10]:
    seed = s+21
    num_steps = n+1
    set_seed(seed)
    
    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)
    
    image_name = "images/synth_{}_{}.jpeg"
    
    image_save = image.images[0].save(image_name.format(seed,num_steps))

```

![](cover.jpg){width="50%" fig-align="left"}

::: column-margin
computer music. musical computer. music represented as bits going into the fabric of the universe. 80s cartoon retro. - Dreamshaper v7
:::

```{r}
#| echo: false
html_tag_audio <- function(file, type = c("wav")) {
  type <- match.arg(type)
  htmltools::tags$audio(
    controls = "",
    htmltools::tags$source(
      src = file,
      type = glue::glue("audio/{type}", type = type)
    )
  )
}

```

I'm going to be running a cognition experiment or two this semester that will involve creating musical stimuli. I would like programmatic control over that, so I'm delighted to learn that there are existing R packages that will help me with a few things. 

I'm just testing a few things out here.

## Reading in MIDI with pyramidi

It looks like I can read in MIDI data to a dataframe with [pyramidi](https://urswilke.github.io/pyramidi/).

Requires some python stuff, but it is working.

```{r}
library(pyramidi)
library(dplyr)
library(tidyr)
library(purrr)
library(ggplot2)
library(zeallot)


midifile <- MidiFramer$new("Top Gun Theme.mid")

knitr::kable(midifile$df_notes_wide[1:10,])
```



## Writing Midi

Haven't tried this yet.

```{r}

```


## Synthesizing midi to wav and mp3

[raudiomate](https://urswilke.github.io/raudiomate/)

also need [fluid synth](https://www.fluidsynth.org)

And, apparently fluid synth needs sound fonts. Got this one <https://member.keymusician.com/Member/FluidR3_GM/index.html>

I could not get raudiomate to work. The `processx:run` command kept putting quotes where they didn't belong. 

Used the `av` package to turn the wav into an mp3

```{r, eval = F}

system("fluidsynth -F out.wav ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 'Top Gun Theme.mid'")

av::av_audio_convert("out.wav","out.mp3")
```
`r html_tag_audio("out.mp3", type = "wav")`

This all took way longer than I expected. Mostly fiddling with python packages and paths to things. But, I declare victory because it made the Top Gun theme song into an mp3.

