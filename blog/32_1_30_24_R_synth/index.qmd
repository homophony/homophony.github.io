---
title: "Midi and synthesis in R"
author: "Matt Crump"
date: 1/30/24
description: "Trying out a few R packages to handle MIDI data in dataframes, and play it with fluid synth."
image: "cover.jpg"
comments:
  giscus: 
    repo: CrumpLab/crumplab_comments
categories: 
  - midi
  - fluidsynth
  - rstats
execute: 
  echo: true
  message: false
  warning: false
format:
  html:
    code-fold: true
    code-summary: "Show the code"
---

```{python, eval = FALSE}
from diffusers import DiffusionPipeline
from transformers import set_seed
from PIL import Image
import torch
import random
import ssl
import os
ssl._create_default_https_context = ssl._create_unverified_context

#locate library
#model_id = "./stable-diffusion-v1-5"
model_id = "dreamshaper-xl-turbo"

pipeline = DiffusionPipeline.from_pretrained(
	pretrained_model_name_or_path = "../../../../bigFiles/huggingface/dreamshaper-xl-turbo/"
)

pipeline = pipeline.to("mps")

# Recommended if your computer has < 64 GB of RAM
pipeline.enable_attention_slicing("max")

prompt = "computer music. musical computer. music represented as bits going into the fabric of the universe. 80s cartoon retro."

for s in range(30):
  for n in [5,10]:
    seed = s+21
    num_steps = n+1
    set_seed(seed)
    
    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)
    
    image_name = "images/synth_{}_{}.jpeg"
    
    image_save = image.images[0].save(image_name.format(seed,num_steps))

```

![](cover.jpg){width="50%" fig-align="left"}

::: column-margin
computer music. musical computer. music represented as bits going into the fabric of the universe. 80s cartoon retro. - Dreamshaper v7
:::

```{r}
#| echo: false
html_tag_audio <- function(file, type = c("wav")) {
  type <- match.arg(type)
  htmltools::tags$audio(
    controls = "",
    htmltools::tags$source(
      src = file,
      type = glue::glue("audio/{type}", type = type)
    )
  )
}

```

I'm going to be running a cognition experiment or two this semester that will involve creating musical stimuli. I would like programmatic control over that, so I'm delighted to learn that there are existing R packages that will help me with a few things. 

I'm just testing a few things out here.

## Reading in MIDI with pyramidi

It looks like I can read in MIDI data to a dataframe with [pyramidi](https://urswilke.github.io/pyramidi/).

Requires some python stuff, but it is working.

```{r}
library(pyramidi)
library(dplyr)
library(tidyr)
library(purrr)

midifile <- MidiFramer$new("Top Gun Theme.mid")

knitr::kable(midifile$df_notes_wide[1:10,])
```



## Writing Midi

Haven't tried this yet.

```{r}

```


## Synthesizing midi to wav and mp3

[raudiomate](https://urswilke.github.io/raudiomate/)

also need [fluid synth](https://www.fluidsynth.org)

And, apparently fluid synth needs sound fonts. Got this one <https://member.keymusician.com/Member/FluidR3_GM/index.html>

I could not get raudiomate to work. The `processx:run` command kept putting quotes where they didn't belong. 

Used the `av` package to turn the wav into an mp3

```{r, eval = F}

system("fluidsynth -F out.wav ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 'Top Gun Theme.mid'")

av::av_audio_convert("out.wav","out.mp3")
```
`r html_tag_audio("out.mp3", type = "wav")`

This all took way longer than I expected. Mostly fiddling with python packages and paths to things. But, I declare victory because it made the Top Gun theme song into an mp3.

-----------------

## Questions

I've only got 30 minutes right now, can I make this thing randomize notes?

answer, close enough for now.

```{r, eval=FALSE}
# trying stuff from the pyramidi docs

# load in a basic midi file
midi_file_string <- system.file("extdata", "test_midi_file.mid", package = "pyramidi")

mfr <- MidiFramer$new(midi_file_string)

# mfr has a bunch of midi dataframes in it
#mfr$df_notes_wide[1:10]

# helper function
beats_to_ticks <- function(notes_wide) {
  notes_wide %>%
    mutate(
      ticks_note_on  = b_note_on  * ticks_per_beat,
      ticks_note_off = b_note_off * ticks_per_beat
    )
}

n_beats <- 16
ticks_per_beat <- 960L

#(b_note_on = (0:(n_beats-1) %/% 4) * 4)

b_note_on <- 0:(n_beats-1)


notes <- tibble(
  i_track = 0,
  meta = FALSE,
  note = sample(c(60, 64, 67, 72), 16, replace=T),
  channel = 1,
  i_note = 1:n_beats,
  velocity_note_on = 100,
  velocity_note_off = 0,
  b_note_on = b_note_on,
  b_note_off = b_note_on + 1,
)

mfr$update_notes_wide(beats_to_ticks(notes))

df_notes_long <- pivot_long_notes(mfr$df_notes_wide)
df_midi_out <- merge_midi_frames(mfr$df_meta, mfr$df_notes_long, mfr$df_not_notes)

dfc2 <- df_midi_out %>%
        miditapyr$nest_midi(repair_reticulate_conversion = TRUE)

miditapyr$write_midi(dfc2, ticks_per_beat, "test.mid")

system("fluidsynth -F test.wav ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 'test.mid'")

av::av_audio_convert("test.wav","test.mp3")
```

`r html_tag_audio("test.mp3", type = "wav")`


