[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Matt Crump is a cognitive psychologist with research interests in learning, memory, attention, skill learning, semantics, and computational modeling. He runs the computational cognition lab at Brooklyn College of CUNY. He also plays music.\nHomophony is a word created by Richard Semon as a part of his theory of memory processes. It refers to the concept that experiences in memory may resonate with one another, like notes on a piano."
  },
  {
    "objectID": "blog/10_12_30_23_Quantum_Freeloader/index.html",
    "href": "blog/10_12_30_23_Quantum_Freeloader/index.html",
    "title": "Quantum Freeloader",
    "section": "",
    "text": "Tape cassette robot shooting lasers of music at a concert with massive speakers and a universe in the sky. - LCM_Dreamshaper_v7\n  \nThis is a mash-up of a few things I’ve been working on and covered in previous posts.\n\nI spent some time learning solo parts from Wynton Kelly’s piano solo on Miles Davis’ Freddie Freeloader\nI did some beat recreation from Quantic’s Life in the Rain. This ended up as a midi track with lots of added notes and modified note probabilities to make the beat have more natural and interesting variation.\n\nThe track is in the style of Freddie Freeloader. The chords and melody are different, but they are still variations of the general idea."
  },
  {
    "objectID": "blog/4_12_24_23_scales_7ths/index.html",
    "href": "blog/4_12_24_23_scales_7ths/index.html",
    "title": "Scales in 7th chords",
    "section": "",
    "text": "“An armadillo piano player practicing playing piano in the middle of a desert with a bright blue sky” - LCM_Dreamshaper_v7\n  \nThis exercise has two parts. Practice around the circle of fifths.\n\nPlay the major scale up and down using 7th chords.\n\nI∆7 II-7 III-7 IV∆7 V7 VI-7 VII-Dim7\n\nPlay each chord as an arpeggio, but start on the 7th and play down to the root.\n\nNotes: very easy to burn through this in C without thinking. G and F are decent, everything else is slow. Need to keep practicing the hard ones."
  },
  {
    "objectID": "blog/20_1_19_24_7th_chords/index.html",
    "href": "blog/20_1_19_24_7th_chords/index.html",
    "title": "Assessing my 7th chords with a reaction time test",
    "section": "",
    "text": "Testing myself on 7th chords using a choice-reaction time procedure\nThis post is about collecting some performance data on my ability to play stuff. I have some longer term plans to improve on these methods for research purposes, but for now, I’m just interested in roughly tracking my own abilities.\nIn my last post I was about try out some different practice schedules, one of which would require me to randomly pick which keys to practice. That kind of stuff is annoying to do without a computer. Plus, if I’m about to collect a bunch of data in a bunch of conditions, I’d rather not code things and do data entry by hand.\nSo, I jumped ship and decided to start building little computerized measurement tools.\nI put together a really simple choice reaction test using jspsych, the same JavaScript library that I use for online cognition experiments.\nTo start, I decided to test myself on basic seventh chords in all keys. There are 12 different keys, and for this test, I used ∆7, 7, and -7 chords, for a total of 36 different chords.\nThe choice reaction time test is very simple. The program takes all of the possible chord names, randomly shuffles them, and then presents each name on the screen one at a time. I put my laptop on top of my piano, waited for the chord name to appear, and then played the chord as fast as I could on the piano. At the same time, I had my left thumb on the spacebar, and I pressed the spacebar roughly at the same time that I played the chord. This allowed me to measure the reaction time for each chord. I also had the program repeat all the chords randomly twice, so that I got two reaction time measures per chord.\nI love jspsych. It took like 15 minutes to make this happen (granted, I have a bunch of prior experience using that library, and I was doing something very simple). Still, I already made the program and ran the test, and collected the data as a JSON variable. Woo hoo!\nAnd, of course, this is really messy data. Ideally, I would have the reaction times for each note, say using MIDI or something. But, that’s for later.\nI’d really like to take a look at my own performance data. I know all of these chords pretty well, but I have never closely compared my ability to produce them on demand. I expect I’ll be pretty good in C, D, F, G, A, and worse in the other keys. But, I don’t really know what the pattern looks like.\nSo, let’s get to the data analysis.\nI loaded in the data, computed the average chord-reaction time for each chord, and put them all in the following table and graph.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeat. Close to what I expected in terms of which chords I think I’m better at. All of em could do with some extra practice. I suppose I should do more practice on the slow ones, because the fast ones don’t really need it.\n\nI was too hasty with my jspsych programming and totally forgot to add a few things that would make data analysis much easier. I’d like to be able to easily replot the data in a few different ways. One way is to group by maj7, min7, and dominant 7th. Another is to arrange the keys in ascending/descending order (rather than alphabetically), or in the order of the circle of fifths. I could add the necessary factor levels to the data file by hand, but I think I’ll add this to the jspsych program so that the data file includes this by default.\nSo, off to fixing the program, and then I’ll collect some more data and try a few different plots.\nAnd, I’m back. I ran through the practice one more time. Now my data file should be easier to plot in different ways.\nHere is the mean chord reaction time for each key, grouped by 7th chords.\n\n\n\n\n\n\n\n\n\nI wanted to order the keys in terms of the circle of fifths, but there was another bug in the code, now fixed. I’ll need another round of data-collection to get that graph.\nIn terms of piano practice, I should get a bunch of these chord reaction times lower. Especially Gb, Ab, Bb, Eb, and Db, and B. I thought Bb would be better, but nope.\nThe program is not currently keeping track of practice attempts per chord, and I should add that.\n\nAfter some javascript nonsense, the program now runs in a loop. Each loop goes through a set of practice material and records the practice attempt number for each chord.\nMy loop of practice material is all the basic 7th chords. So, I’m about to practice them for a while and then plot the data.\nOne detail is whether to do away with the delay between trials. Right now there is a 2 second delay before the next chord. That’s nice and everything, but I feel like chomping through this, so I’ll get rid of that delay.\n\nI practiced all the chords in a random order for about 10 times each. This is a plot of my chord reaction times as a function of practice for all chords. A bit messy to look at.\n\n\n\n\n\n\n\n\n\nIf I average over keys, did I get any faster?\nThere’s a downward trend I guess.\n\n\n\n\n\n\n\n\n\nAveraging over chords and showing the individual keys:\n\n\n\n\n\n\n\n\n\nAnd, one last summary graph. Let’s average over repetitions, and arrange by the circle of fifths.\n\n\n\n\n\n\n\n\n\nI guess I should practice Gb and Db and Bb and flatten out this performance distribution.\n\nAlright, that’s basically all I wanted to accomplish today. Now I have a clunky little tool to relatively quickly assess where I am at on different musical elements. Time to go do something else.\n\nPracticing “Eb”,“Db”,“Gb”,“Ab”,“B” this morning, and plotting the reactions from yesterday (1), and today (2).\n\n\n\n\n\n\n\n\n\nAveraging over individual chords, and looking at mean chord reaction time yesterday and today on these chords.\n\n\n\n\n\n\n\n\n\nMade a change to the program so that when I load it up I can select which keys to practice. I’m assuming that if I select only Gb and Ab (the slowest), I would be able to get them both under 2 seconds. I wonder if I would then maintain that level of performance when increasing the number of other chords in the test. Need to make breakfast first.\nI put Gb and Ab and practiced those 7th chords for a while.\n\n\n\n\n\n\n\n\n\nNice, with 5 minutes or so of banging around those chords, I could get the times way down in the context of this much simpler test (fewer other chords to play, less hand movement between chords etc).\nI’m curious whether this improvement for Gb and Ab would persist into a test using all of the keys.\n\n\n\n\n\n\n\n\n\nWhen I added all of the keys back the RTs for Gb and Ab went up a little bit from the last time, but they are over all way down. Almost all of them are under 1500 ms now.\nThis practice isn’t very musical, but it seems to work out my ability to identify and then play chords on demand. Perhaps speeding up on this test would improve my ability to sight read chord changes."
  },
  {
    "objectID": "blog/12_1_8_24_screaming_modems/index.html",
    "href": "blog/12_1_8_24_screaming_modems/index.html",
    "title": "Screaming modems",
    "section": "",
    "text": "“massive synthesizer, dueling guitar playing giant robots. guitar looks like an old fashioned modem. speakers. stereo. modular synth. eurorack. cartoon, thundercats, woodcut, linocut.” - dreamshaper-xl-turbo\n  \nSometimes I’ll just give up and go with screaming modems dueling in stereo sonic nonsense…that is unnecessarily too long…courtesy of the OB-6. Not much else to say really."
  },
  {
    "objectID": "blog/7_12_27_23_freddie/index.html",
    "href": "blog/7_12_27_23_freddie/index.html",
    "title": "Freddie Freeloader Wynton Kelly solo",
    "section": "",
    "text": "“A small penguin wearing headphones. The penguin listens to the music from the headphones. The penguin sits in front of a piano. The penguin can imagine the musical notes in a tornado above her head.” - LCM_Dreamshaper_v7\nI spent most of yesterday working through Wynton Kelly’s solo on Freddie Freeloader (from Miles Davis’s Kind of Blue). My plan for this post is to talk a little bit about what I did, and what I learned.\nA slight detour though. It’s been very nice to have some clear time to actually practice piano, let alone enough time to break down a solo. When I was a kid the advice was to listen to recordings and learn to play from them by ear. In some cases I was able to do this (with difficulty, because I didn’t spend enough time practicing this way), but for the most part I didn’t have the recordings to listen to. Flash-forward to today and almost everything is available to stream. That’s amazing when you’re trying to learn something by ear. Not only that, but the internet is full of musicians sharing their practice tips. It’s almost overwhelming, but in a good way.\nFor Freddie Freeloader I checked out a couple of YouTube videos before trying the solo myself: a transcription video and an analysis video. Then I played the song a couple times before heading downstairs. Here’s the breakdown process I used this time.\n\nPlay the original recording into a track on Ableton live\nCut the track so that it starts properly on the first bar\nTurn off “warp”, and then adjust the tempo of Ableton until the beats in the track line up properly with the bars in Ableton.\n\n\nThe purple track is the original recording. The selected area is the first two bars, and it is possible to loop the selection and/or slow it down without changing the pitch (by turning warp back on).\nI wasn’t particularly interested in learning the solo to play it note for note straight through. But, I was interested in trying out each little part and thinking about how the notes were used as phrases.\nSo, I worked in sections of two bars, looped each section, then played over top on the Fender Rhodes until I thought I had the line. The blue track are my 2 bar recordings on the Rhodes.\nI didn’t have time to get through the whole solo, just the first 12 bars, and it got a little messy at the end. Oh well. Had lots of fun.\n  \nPractice notes: I ended up practicing the first phrase through the circle of fifths this morning."
  },
  {
    "objectID": "blog/13_1_9_24_turtle_ep133/index.html",
    "href": "blog/13_1_9_24_turtle_ep133/index.html",
    "title": "Turtle TRS-80 variations",
    "section": "",
    "text": "“Can’t remember exactly. TRS computers, turtle connecting to the universe with a modem. Thundercats. woodcut cartoon vibes.” - dreamshaper-xl-turbo\n\nI did my piano practice in A major today following the 7th chord themes from previous posts.\nThen I accidentally spent too much time generating turtle cartoons with dreamshaper. So, I made a beat with the EP-133 and set them to music.\nSo many variations of the same thing."
  },
  {
    "objectID": "blog/template/index.html",
    "href": "blog/template/index.html",
    "title": "blog template",
    "section": "",
    "text": "“fragments of a dream woven together with little puzzle pieces showing a universe of possibility” - LCM_Dreamshaper_v7\n  \nNothing to see here. This is the template file I use to start posts.\nPost goes here."
  },
  {
    "objectID": "blog/8_12_27_23_quantic_rain/index.html",
    "href": "blog/8_12_27_23_quantic_rain/index.html",
    "title": "Recreating a beat from Quantic’s Life in the Rain",
    "section": "",
    "text": "Album cover - The 5th Exotic by Quantic\n\nRecord this thing into Ableton and grab the beat.\n\nSidenote. Attempting this on a new mac. I was going to record from Apple Music into Ableton, but I wasn’t setup for that. I used to use soundflower to route audio, and now I’m going to try loopback.\nReally annoying to install on my new mac because of the process of allowing kernel extensions. ugh. Will it work? OK, it worked.\n\nHere’s 8 bars from Quantic. The first 4 are a bit of cymabals, and the beat comes in on the last 4. It’s the last 4 that I’m interested.\n\n  \n\nSet up some drums in Ableton and MIDIfy this beat.\n\n\nThis took a little while. Not a huge fan of penciling in midi markers on my computer to make beats.\nSet up a basic drum rack, found some close enough sounds\nUsed the EP-133 as a midi controller to pound in the beats, that was more fun\nNudged things around etc.\ngot close\n\nI ended up with this version of the main beat:\n  \nThen, what’s fun with having everything as a midi note is to play around with note probability, and with adding hits that weren’t there, but now have a small probability of being there. Here’s an example of setting most of the sounds to a 50% probability.\n  \nIt’s more airy, but still carries the groove.\nPractice notes: More or less accomplished the goal for this drum recreation exercise. Did not mix the drum voices to taste.\nTo Do:\n\n[] Use this somehow.\n[] Learn a little bit more about saving midi clips/audio etc. in ableton so that it is easy for me to re-use this later.\n[] See about programming this beat into the Varigate 8+, and or Hermod, so I have a modular version"
  },
  {
    "objectID": "blog/21_1_21_24_chord_reaction_time/index.html",
    "href": "blog/21_1_21_24_chord_reaction_time/index.html",
    "title": "Expanding the chord reaction time test",
    "section": "",
    "text": "cartoon thundercat scientist building a synthesizer. laboratory. science. 80s cartoon. space music.\nI’ve been tinkering with a web-based “Chord Reaction Time” test to measure how fast I can play various chords on demand. The first version of the task had all the keys and three basic 7th chords (7, -7, ∆7). The newer version adds a selection screen, and more chords to practice. I don’t have a demo to share yet, but here’s a figure of what happens:\n\nAt the moment, whenever I play a chord in response to a name, I have to simultaneously press the space bar on a bluetooth keyboard. That keypress records my reaction time to the chord and triggers the next trial. This coming week I’ll look into using the laptop microphone to record reaction time by analyzing amplitude changes (and then later something similar using MIDI).\nYesterday afternoon I tried my hand at all the chords in all the keys. I haven’t looked at the data yet, but I’m sure it will tell me that I’m slow at the chords that I already know I’m slow on.\nThis graph averages over keys and shows mean chord reaction time for each type of tested chord form.\n\n\n\n\n\n\n\n\n\nI should dedicate practice to all of the chords above the 2 second range.\nOne question I have is whether the performance on this kind of test would meaningfully predict my ability to fluently play through chord changes on lead sheets.\nMy experience right now is that I can often get through chord changes on a new song without too much trouble, but I’ll get tripped on the altered chords with flats and sharps. More like fall all over the place. I can’t dead reckon those chords. Instead, I’ll sit there and take 5 seconds or more to puzzle out the chord, which ruins the flow. If I’m feeling diligent I might work out a fingering pattern that makes it easy to play in that context. However, I don’t appear to retain any of that chord knowledge in a general way.\nI assume that if I was able to play all my chords really well in the context of playing lead sheets then my reaction times in the above graph would be faster and more uniform. I could practice this test over and over, and I’m pretty confident that I would get faster on the slower chords. However, one possibility is that that this learning would be somewhat specific to the peculiar demands of this very non-musical chord production task. So, it’s not clear to me that improvements derived from practicing chording in this task would fully generalize to helping me play lead sheets. At the same time, If I can get all the chords down to under 2 seconds in all the keys, that’s gotta help.\nBreakfast time.\n\nI did practice some of the extended chords yesterday. This morning I did a round of practice/testing on the 7b9 chords.\nYesterday, 7b9 were initially excruciatingly slow. I couldn’t play any of them and had to construct them on the fly. As the graph below shows, I took over 12 seconds to produce some of the chords.\nPractice felt very slow and I didn’t feel like I was locking in new patterns at a fingering level. For example, I can visualize a C7 chord immediately, and still can’t visualize a C7b9 (even after all this practice).\nNevertheless, the graph shows some substantial gains in chord production time. There were a few stages in learning to play this chord. Stage 1 was having no clue and working out each chord anew. In stage 2, I settled on a method to get to the 7b9: play the 7th chord in root position, go up 1 inversion, but instead of playing the root at the top, play 1 semitone above the root. Stage 2 was pretty slow also. At some point I realized that I already knew all of these chords as o7 chords. For example C7b9 without the root has E-G-Bb-Db, which are all the notes in Eo7, Go7, Bbo7, and Dbo7. In stage 3, I switched to translating the chord name, “C7b9”, and thinking play the o7 on the third (or, in this case Eo7). That helped a little bit in terms of speed, and I think it makes musical sense to build mental associations between the 7b9 and o7 anyway.\n\n\n\n\n\n\n\n\n\n```"
  },
  {
    "objectID": "blog/3_12_23_23_cycling/index.html",
    "href": "blog/3_12_23_23_cycling/index.html",
    "title": "Cycling through Major 7ths",
    "section": "",
    "text": "The basic task is to play Major 7th chords through the circle of fifths.\n\n\nCircle of fifths\nGoing clockwise the next note is a perfect fifth from the previous note.\n\nMajor 7th chords have a I-III-V-maj7 structure. These are the first, third, fifth, and major 7th notes in the major scale of the root key. For example, the key of C has 7 notes in the major scale: C D E F G A B\nThe chord C Maj7 (also notated as C∆7) is C-E-G-B.\nWhere C is the root or first note, E is the III, G is the V, and B is the major VII. The same pattern is applied to make Maj7 chords in all other keys.\nAt the beginning of this practice track I play chords going anti-clockwise through the circle of fifths:\nC∆7 - F∆7 - Bb∆7 - Eb∆7 - Ab∆7 - Db∆7 - Gb∆7 - B∆7 - E∆7 - A∆7 - D∆7 - G∆7 - C∆7\nI can probably play through this a bit faster, but not without making mistakes. One goal is to get the speed up both in both directions (anti-clockwise and clockwise).\nNext, there is a short riff that I got from Nahre Sol’s youtube channel, who has some other great recommendations for playing through maj7th chords.\nThen, I end with playing ∆7th chords in my left hand and messing around with playing through major scales in the right hand. For example, if I’m playing a C∆7th, then I’m playing through the C major scale and so on. I thought I would have a better grasp on all of the major scales, but some of them are way out of practice. Yeesh. Baby steps.\nA random observation about musical relationships that this exercise drew out. When playing anti-clockwise, the next chord remains ambiguous with respect to the tonal center: it could indicate a switch of the root, but maybe not. There isn’t enough evidence to go on based on the notes in the major 7th chords.\nFor example, going from C∆7 to F∆7 involves CEGB and FACE, all of these notes are in the C Major scale. So, the F∆7 chord doesn’t add any new notes, which in my mind allows some ambiguity about whether we are in C major still or have switched to F major. The F major scale has a Bb, which is the one note that makes F major different from C major, but this note is not voiced in the F∆7 chord. However, this “new note”, which would provide evidence that the root had switched from C to F is voiced by the next chord in the circle of fifths, Bb. So, in some sense moving anti-clockwise by one chord sets up a question—has the root changed?—and moving one more chord answers the question—yes—, but also asks the same question again, in a circle of fifths."
  },
  {
    "objectID": "blog/18_1_18_24_Relearning/index.html",
    "href": "blog/18_1_18_24_Relearning/index.html",
    "title": "Savings in relearning",
    "section": "",
    "text": "Yesterday I spent some time practicing the maj6 diminished scale across all keys. I also posted about practice ideas in relation to cognitive psychology phenomena.\nFor reference, I can play a C major scale in 7th chords in just under 3 seconds without any warm-up. I’ve had a couple days to put some practice into the C major 6th diminished scale, and I times myself at 12 seconds to play through the chords there.\nI timed myself going through several keys for the first time, and B was the slowest at 104 seconds. Then, just for B maj 6 diminished, I did a focused spurt of practice for 10 minutes on that scale. This involved playing the scale ascending in chords over and over again, and not doing anything else. If I made a mistake I would correct it, and if I couldn’t correct it I would start again. I used a stopwatch on my phone for the timer, and pressed the lap button every time I finished the scale.\nI was interested in measuring a learning curve to track my progress with this scale. The data looked like this:\nThe graph shows that I reduced my time to complete the scale by quite a bit in just 10 minutes of practice, from 104 seconds to 21 seconds.\nThat was yesterday."
  },
  {
    "objectID": "blog/18_1_18_24_Relearning/index.html#retentionsavings-in-re-learning",
    "href": "blog/18_1_18_24_Relearning/index.html#retentionsavings-in-re-learning",
    "title": "Savings in relearning",
    "section": "Retention…Savings in re-learning?",
    "text": "Retention…Savings in re-learning?\nMy question this morning is how much did I retain? I haven’t practiced the B maj6 diminished scaled since yesterday. My plan is to go sit down and play it ascending for the next 10 minutes, record my playing times, and then come back here and graph it.\nOne possibility is that I’ll start off around 21 seconds where I left off, and then maybe get a faster. That would indicate I retained almost all of whatever I learned yesterday. Another possibility is that I start back at the beginning, perhaps having retained not very much at all. Still, if that happened, it’s possible that I would re-learn faster the second time through.\nLet’s find out.\nI just put in 10 minutes of repetitive practice, and here are the results.\n\n\n\n\n\n\n\n\n\nWhat happened? I ended up at about 20 seconds per run at the end of yesterday (last red dot), and today I started way back at about 60 seconds per run. That might look like almost no retention of whatever I “learned” yesterday. Yes, my first data point today was almost twice as fast as the first one from yesterday, but that could be an outlier. I remember making loads of mistakes the first time yesterday and restarting the scale several times.\nAlthough my starting points are similar, it appears there was a decent amount of savings in relearning. Savings in relearning is a concept from Hermann Ebbinghaus and his research on forgetting. Ebbinghaus spent time learning lists of random syllables until he could recite them, and then he waited different amounts of time before trying to recall them from memory. Not surprisingly, he forgot items over time. However, he would also relearn lists a second time, after the delay, and often found savings in his relearning attempt. Even though he had forgotten items from the list, when he tried to relearn the list, it took less time (than the initial learning attempt) before he could recite it perfectly again.\nSomething similar is going on here. One possibility is that I forgot everything I learned since yesterday, and that’s why I was back at the beginning. However, even though the data show I was basically back at the beginning, it seems my practice from yesterday helped me relearn faster than before. The green dots go down faster over practice attempts than the red dots. Yesterday I only got 15 scales completed in 10 minutes, but this time I got through 30 attempts. I also got my time down to 8 or 9 seconds by the end."
  },
  {
    "objectID": "blog/18_1_18_24_Relearning/index.html#rd-attempt",
    "href": "blog/18_1_18_24_Relearning/index.html#rd-attempt",
    "title": "Savings in relearning",
    "section": "3rd attempt",
    "text": "3rd attempt\nFor now I’m not being systematic about any of this. It’s been a few hours since my second attempt at practicing the B maj6 diminished scale. What would happen if tried again, instead of waiting for a longer delay?\nSo, I went and tried again for 10 minutes. Here’s the data:\n\n\n\n\n\n\n\n\n\nOverall, the blue dors are pretty similar to the green dots from this morning. Even though I got down to about 8 seconds this morning, I didn’t start from there when I went back to practice. My first run on the third attempt was a big improvement in some sense, but the second run was way worse.\nI’ll sit back an do a bit of introspective psychology along with some hand-wavy data description about what seems to be happening. For convenience, I’ll split this into three phases.\nPhase 1. Puzzling it out. There is a lot action going on in the first 10 practice attempts. I start out really slow, and then make substantial speed gains across the first few attempts. Introspectively, this a very new scale to me. I can’t readily mentally visualize yet. By comparison say to a C major scale, the elements of which I can very easily bring to mind in terms of visual mental imagery or a mental sense of fingering chord shapes. Sitting here right now, I can think of a B maj6 chord now, but I’m drawing a blank when I think about the next chord. OK, it took at least 15 seconds to reconstruct the Dbdim7 chord in my mind. So, at this point in my B maj6 dim scale practice, when I go sit down to try playing it, I don’t have a great mental visualization of it, and I’m puzzling out each chord as I go through the scale.\nIf I zoom into the production of the scale it goes something like this. - OK, play the first chord…and, I’m practicing the B scale. - Look at B, but thumb on B (think that I should’ve practice B more as a kid). - I’m not slamming that B major triad…a little confused on whether to play E or Eb, or F or Gb. If things get really bad, then I can go back to first principles and work out the intervals in semitones, but I rarely need to do that for basic chords. However, this kind of mental puzzle talk is happening at the same time as important memories might come back. For example, I’m starting to get a nice mental image about the shape Bmaj6, with the B Eb Gb Ab. It’s a palpable kind of embodied knowledge or sense of the chord that is hard to describe, but has a lot to do with how my fingers and keyboard look and feel as they spread across and make contact with those notes. The B is the only white note, and the other three notes are pretty far away. I think the B-Gb perfect fifth might be physically the furthest on the piano. In any case, whatever this chord sense is, if I’m lucky enough for it to be “retrieved”, voila, I can use that memory and then play it (sidenote, this is basically the instance theory of automatization idea from an instrospective perspective). - Waiting for either the puzzling out process to find a solution, or for a memory of B maj6 to pop-up, is followed by playing the chord, ya I got it down. - At this point, I’m on the chord I was trying to play, and having trouble thinking up the next chord. - Another puzzling out process/waiting for a useful memory of the next chord ensues. Sometimes I just go for it and land the next chord not really knowing if it was going to work out. - I usually want to have some confidence that I will play the next chord correctly. So, I will sit on the current chord, stare at my fingers, and try to invoke the next chord. - Sometimes, I’m not particularly verbalizing or instructing myself about what the next chord is. Or, I’m changing the instruction. For example, if I’m on Bmaj6 and trying to play the next chord, I could be saying “play the next chord”, or “play the ii”, or “the next one is going to be the diminished chord, but what diminished chord is it?”, or “play the Db”, or “what is the Dbdim7”, and so on. Othertimes, there seems to be less inner voice, and more a sense of fingering patterns that need to change. - More recently, I’ve been trying to mentally visualize the next chord before I play it. Once I lock in the finger sense of what that chord will be, then I go to it. I find this usually helps with accuracy.\nTaking all of this together I think provides some descriptive context to account for the lengthy inter-chord production times that appear in phase 1 (the first 10 practice attempts). I haven’t automatized these chords yet, and lot’s of different kinds of cognitive processing from active puzzling out a solution, to side conversations not super relevant to my current task, to losing track of what I’m doing, to finding that I do remember a chord— these activities all take some amount of time before the next chord is produced. And, those times can account for some of the time in-between each chord.\nIn my first few practice attempts, I find that some chords come back from memory, and others take some puzzling. And, that these can switch around sometimes, especially after going through the scale. There’s also room for a bunch of interference from other chord knowledge. For example, yesterday I did some focused practice on playing fully diminished 7th chords in every key. Now, this morning when I’m going through the Bmaj6 scale, even though I know that I should be alternating maj6 and dim7 chords across each note, sometimes I’ll be trying to play an Ebmaj6, but the Ebdim7 comes out. So, as I get into practicing the scale, achieving fluency also involves managing interference from other chords.\nPhase 2. Almost getting it. I would say phase 2 is what is happening in between practice attempts 10 and 30. I’m hitting a floor here of about 10-12 seconds. This is not very fast or fluid, but I’m able to consistently play through the scale without making too many errors. Introspectively, there is still a lot of thinking going on before many chords. Sometimes I get a nice fast run of three or four chords in a row, other times I might completely fall apart on a chord and have a very long pause before being able to play it.\nPhase 3. Whispers of fluency. In these very early practice attempts, by the time I get past 30 runs I’m starting to almost feel comfortable. I can begin to sense how the chords would be played in a fluent manner. If I go for it I can play it fast with lots of errors, but with the general feeling close to right.\nAchieving actual fluency. As a piano player, I’m just going to keep practicing this scale, mix it up, and have fun with it. Fluency will come with practice, and I’m not going to sweat about it, or really care too much about optimizing the process.\nAs a cognitive psychologist, I am nevertheless wondering about various practice schedules or approaches that could influence getting an unfamiliar musical concept into series of fluent actions that I don’t have to think about. There are lots of manipulations that could be interesting to try.\nOne approach is for me to go all out and just practice ascending chords in the Bmaj6 scale, as I’ve been doing, but go all day long until I reach some kind of fluency criterion that’s good enough for now. If I could play that scale in 3 seconds I would be very happy with that level of fluency. Could I get to 3 seconds by focused repetitive practice, drilling that puppy until I get there or go to sleep trying, or before piano mush brain sets in? Maybe? If I did get to that criterion, what kind of retention would I have tomorrow?\nIt’s possible that large swathes of the above practice schedule would be practically useless. I could get tired and sloppy. Maybe I need to have a sleep and rely on the magic of memory consolidation. Or perhaps spread out the practice with lots of other intervening stuff.\nIn the first scenario I might actually get to a reasonable level of fluency in this scale sometime today. In the second scenario, it could take days or months depending on how many things I did in between.\nI’m realizing I have a bunch more to write about this, but not in this post. I think it’s time for some cross-posting between my main research blog and this here music blog."
  },
  {
    "objectID": "blog/1_12_27_23_blog/index.html",
    "href": "blog/1_12_27_23_blog/index.html",
    "title": "Blogging for music motivation",
    "section": "",
    "text": "I had some time at the end of my semester to play piano more than usual. This semester that meant I actually played the piano, instead of not playing the piano.\nClearly I need to make time for more piano. I’m hoping this blog will keep me somewhat organized and motivated to stay on target.\nThe blog is built with quarto, and this post is just changing the structure a little bit so that each post is it’s own post, rather than a bunch of posts on a single page."
  },
  {
    "objectID": "blog/9_12_29_23_morethan4/index.html",
    "href": "blog/9_12_29_23_morethan4/index.html",
    "title": "Breaking out of the 4 bar drum loop",
    "section": "",
    "text": "A tape cassette robot playing a drum kit. Massive speakers in the background. Super funky beats - LCM_Dreamshaper_v7\nI have a really lazy habit with drum beats that I need to fix. It goes something like this: make a 4 bar beat, loop it, jam endlessly inside a 4 bar beat that gets boring and has no fills.\nYesterday I had some fun recreating a 4 bar beat from Quantic’s “Life in the Rain”. Today, the plan is to make this into a 12-bar beat with a reasonable amount of variation and some simple fills.\n…Does some things in Ableton that didn’t take that long…Should do this kind of thing more often. Now I have a 12-bar drum part with a decent amount of variation. But, it’s on my upstairs computer and I need to get this into a new project on my downstairs computer. I’ve been using Ableton since forever and I don’t think I’ve ever tried to copy or import a midi track from one project to another. It must be possible. Going to play some music over these drums."
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Wall of Notes",
    "section": "",
    "text": "Testing out {fluidsynth} for R!\n\n\n\n\n\n\nmidiblender\n\n\nmidi\n\n\nfluidsynth\n\n\n\nHurrah, fluidsynth made easy for R!\n\n\n\n\n\nFeb 15, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nPlaying a MIDI file on a Quarto blog\n\n\n\n\n\n\nmidiblender\n\n\nmidi\n\n\n\nQuick test to see if a MIDI player will work on this page. And it does!\n\n\n\n\n\nFeb 13, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nMIDI composition with {dplyr}, {midiblender}, and {pyramidi}\n\n\n\n\n\n\nmidiblender\n\n\npyramidi\n\n\nrstats\n\n\ndplyr\n\n\nmidi composition\n\n\n\nTrying some dplyr composition patterns for creating midi files\n\n\n\n\n\nFeb 12, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nEurorack notes to self\n\n\n\n\n\n\nmidiblender\n\n\neurorack\n\n\nprogrammable eurorack\n\n\ncomposition\n\n\n\nKeeping track of some eurorack directions, notes to self.\n\n\n\n\n\nFeb 11, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nEuclidean Rhythms and circulating sequences with information theory concepts\n\n\n\n\n\n\nmidiblender\n\n\neuclidean rhythms\n\n\ninformation theory\n\n\n\nProbably biting off more than I can chew here. A saturday experiment in MIDI mangling.\n\n\n\n\n\nFeb 10, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nTurning up the randomness to 11 for MAXIMUM MIDI MADNESS\n\n\n\n\n\n\nmidiblender\n\n\nfun\n\n\n\nMangling with all the polyphony, will it blend?\n\n\n\n\n\nFeb 9, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing and filtering note occurence by point estimation\n\n\n\n\n\n\nmidiblender\n\n\nprobabilistic filtering\n\n\nrstatsmusic\n\n\nmidi\n\n\nrstats\n\n\n\nUsing matrix representations of MIDI files to mangle them based on point estimates, like a low pass filter cutoff, but for probabilities.\n\n\n\n\n\nFeb 8, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\n{midiblender} is alive!\n\n\n\n\n\n\nmidiblender\n\n\ngenerativemusic\n\n\nrstatsmusic\n\n\nmidi\n\n\nrstats\n\n\n\n{midiblender} is an R package for experimental genRative MIDI mangling\n\n\n\n\n\nFeb 8, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nMidi blending Canon in D probabilistically\n\n\n\n\n\n\nmidiblender\n\n\ngenerativemusic\n\n\nrstatsmusic\n\n\nmidi\n\n\nrstats\n\n\n\nMore work in progress with midiblender. It seems to be blending.\n\n\n\n\n\nFeb 7, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nWIP: Endless probabilistically generated mario music with midiblender\n\n\n\n\n\n\nmidiblender\n\n\ngenerativemusic\n\n\nrstatsmusic\n\n\nmidi\n\n\nrstats\n\n\n\nStill a work in progress, getting closer to victory\n\n\n\n\n\nFeb 7, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nmidiblender: working on an experimental R package\n\n\n\n\n\n\nmidiblender\n\n\ngenerativemusic\n\n\nrstatsmusix\n\n\nmidi\n\n\nrstats\n\n\n\nA scratchpad post to help me with package development\n\n\n\n\n\nFeb 6, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nMIDI analysis, bags of notes, and probabilistic generation\n\n\n\n\n\n\nmidi\n\n\nnlp\n\n\nmatrix\n\n\nrstats\n\n\n\nStill in an exploratory phase. Comments on matrix representation for MIDI, and opportunities for probabilistic playback.\n\n\n\n\n\nFeb 5, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nMessing around with heart and soul\n\n\n\n\n\n\nmidi\n\n\ngenerative\n\n\nmatrix\n\n\nrstats\n\n\n\nA sunday synthesizer exploration.\n\n\n\n\n\nFeb 4, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nMidi to matrix representation and probabilistic super mario music with R\n\n\n\n\n\n\nmidi\n\n\ngenerative\n\n\nmario\n\n\nmatrix\n\n\nrstats\n\n\n\nConverting midi to feature vectors and back, with some probabilistic versions of the super mario overworld music.\n\n\n\n\n\nFeb 3, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nSystematically randomizing Super Mario brothers with R\n\n\n\n\n\n\nmidi\n\n\nfluidsynth\n\n\nrstats\n\n\n\nAchieving my glorious mission.\n\n\n\n\n\nFeb 1, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nCharting out chordal spin\n\n\n\n\n\n\nchord similarity\n\n\ndatavis\n\n\ncircle of fifths\n\n\nmusic theory\n\n\nrstats\n\n\n\nUsing ggplot2 to visualize how chords spin around the circle of fifths\n\n\n\n\n\nJan 30, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nMidi and synthesis in R\n\n\n\n\n\n\nmidi\n\n\nfluidsynth\n\n\nrstats\n\n\n\nTrying out a few R packages to handle MIDI data in dataframes, and play it with fluid synth.\n\n\n\n\n\nJan 30, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nA sphere of fifths\n\n\n\n\n\n\nchord similarity\n\n\ndatavis\n\n\ncircle of fifths\n\n\nmusic theory\n\n\nrstats\n\n\n\n3d plot of chord space\n\n\n\n\n\nJan 30, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nC major scale analysis\n\n\n\n\n\n\nchord similarity\n\n\ndatavis\n\n\ncircle of fifths\n\n\nmusic theory\n\n\nrstats\n\n\nC major scale\n\n\nmovement\n\n\n\nLooking at the C major scale from the geometry of vector space.\n\n\n\n\n\nJan 29, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nWeek of 1/28 - 2/3 daily practice post\n\n\n\n\n\n\npiano practice\n\n\n\nKeeping track of small practice exercises\n\n\n\n\n\nJan 28, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nSymmetry, high-dimensional chord space, the circle of fifths, and Sunday morning\n\n\n\n\n\n\nchord similarity\n\n\ndatavis\n\n\ncircle of fifths\n\n\nmusic theory\n\n\n\nMusing about some chord and scale implications from visualizations of chord vector space\n\n\n\n\n\nJan 28, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nSteps toward visualizing chord spaces\n\n\n\n\n\n\nchord similarity\n\n\ndatavis\n\n\n\nAttempts to visualize version of a chord similarity space\n\n\n\n\n\nJan 26, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nListening to complex tones using sine waves and toneR\n\n\n\n\n\n\ncomputer music\n\n\ntuneR\n\n\nR music\n\n\n\nAn experiment in listening to similar chord patterns as complex tones, and side adventures.\n\n\n\n\n\nJan 24, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nWeek of 1/21 - 1/27 daily practice post\n\n\n\n\n\n\npiano practice\n\n\n\nKeeping track of small practice exercises\n\n\n\n\n\nJan 24, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nMinerva and expectancy from a chord vector space\n\n\n\n\n\n\nchord similarity\n\n\nMINERVA\n\n\n\nScratchpad for messing around with a MINERVA II model in a musical chord context. Notes to self\n\n\n\n\n\nJan 23, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nChord Similarity analysis table\n\n\n\n\n\n\nchord similarity\n\n\n\nUsing a vector space to analyse similarity between chords in an interactive table\n\n\n\n\n\nJan 22, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nExpanding the chord reaction time test\n\n\n\n\n\n\npractice\n\n\nchord reaction time\n\n\n\nUsing a cog psych task to measure how fast I can play various chords on demand.\n\n\n\n\n\nJan 21, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating practice time needed, and a bit on contextual interference\n\n\n\n\n\n\npractice\n\n\ncogsci\n\n\ncontextual interference\n\n\nestimating practice time\n\n\npractice schedules\n\n\n\nEstimating how long I need to practice a scale with my own learning curve, and questioning how much to mix up practice.\n\n\n\n\n\nJan 19, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nAssessing my 7th chords with a reaction time test\n\n\n\n\n\n\npractice\n\n\n7th chords\n\n\njspsych\n\n\nassessment\n\n\n\nI made a jspsych task to test how fast I can play 7th chords.\n\n\n\n\n\nJan 19, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nSavings in relearning\n\n\n\n\n\n\nrelearning\n\n\npractice\n\n\nscales\n\n\n\nTracking performance improvements over time in learning the maj6 diminished scale\n\n\n\n\n\nJan 18, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nPracticing some Barry Harris methods\n\n\n\n\n\n\nBarry Harris\n\n\npractice\n\n\n\nJust learning about Barry Harris’ system ideas and making notes about what I’m practicing.\n\n\n\n\n\nJan 17, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nTransitions in wonderland\n\n\n\n\n\n\nscales\n\n\nmusic theory\n\n\nmovement\n\n\ndiminished chords\n\n\n\nMusic theory ramblings, diminished chords, movement, Barry Harris, rabbit holes, going from here to there.\n\n\n\n\n\nJan 16, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nModes…sigh\n\n\n\n\n\n\nmodes\n\n\nscales\n\n\nmusic theory\n\n\npractice\n\n\n\nWorking on modes…again\n\n\n\n\n\nJan 13, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nThe VERY Expressive Osmose\n\n\n\n\n\n\ngear\n\n\nosmose\n\n\nMPE\n\n\n\nIt is so fun, and possibly a post that will get longer as I learn more about the osmose.\n\n\n\n\n\nJan 12, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nTurtle TRS-80 variations\n\n\n\n\n\n\ntrack\n\n\n\nMessing around with the EP-133 and Dreamshaper to make a series of cartoons with turtles and TRS-80 computers\n\n\n\n\n\nJan 9, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nD major practice\n\n\n\n\n\n\npractice\n\n\n\npractice notes\n\n\n\n\n\nJan 8, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nScreaming modems\n\n\n\n\n\n\ntrack\n\n\n\nChill vibe and pointless melody gets interrupted by dueling screaming modem solo battle that goes on for too long.\n\n\n\n\n\nJan 8, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nBack in the saddle\n\n\n\n\n\n\npractice\n\n\n\npractice notes\n\n\n\n\n\nJan 6, 2024\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nQuantum Freeloader\n\n\n\n\n\n\npractice\n\n\n\nQuantic/Miles Davis inspired Freddie Freeloader reversion\n\n\n\n\n\nDec 30, 2023\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nBreaking out of the 4 bar drum loop\n\n\n\n\n\n\npractice\n\n\n\nForcing myself to program some drum fills.\n\n\n\n\n\nDec 29, 2023\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating a beat from Quantic’s Life in the Rain\n\n\n\n\n\n\npractice\n\n\n\nAn exercise in beat recreation.\n\n\n\n\n\nDec 28, 2023\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nBlogging for music motivation\n\n\n\n\n\n\nquarto\n\n\n\nSetting up a quarto blog for taking music notes.\n\n\n\n\n\nDec 27, 2023\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nFreddie Freeloader Wynton Kelly solo\n\n\n\n\n\n\npractice\n\n\n\nNotes on learning phrasing from the solo\n\n\n\n\n\nDec 27, 2023\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nRootless 9th chords\n\n\n\n\n\n\npractice\n\n\n\nRunning I∆7add9 chords through the circle of fifths.\n\n\n\n\n\nDec 26, 2023\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\n7th first II-V-I\n\n\n\n\n\n\npractice\n\n\n\nA piano exercise\n\n\n\n\n\nDec 25, 2023\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nScales in 7th chords\n\n\n\n\n\n\npractice\n\n\n\nA piano exercise\n\n\n\n\n\nDec 24, 2023\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nCycling through Major 7ths\n\n\n\n\n\n\npractice exercise\n\n\ncircle of fifths\n\n\n\nNotes on a piano exercise.\n\n\n\n\n\nDec 23, 2023\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n\n\n\nmotivation\n\n\ncircle of fifths\n\n\n\nStarting up a blog about practicing and trying to practice more piano.\n\n\n\n\n\nDec 23, 2023\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nDolphin Dance\n\n\n\n\n\n\nquarto\n\n\n\nA synthesized version of Herbie Hancock’s Dolphin Dance.\n\n\n\n\n\nDec 20, 2023\n\n\nMatt Crump\n\n\n\n\n\n\n\n\n\n\n\n\nblog template\n\n\n\n\n\n\npractice\n\n\n\ntemplate files\n\n\n\n\n\nDec 19, 2023\n\n\nMatt Crump\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "music/Basement_Tapes.html",
    "href": "music/Basement_Tapes.html",
    "title": "Basement Tapes",
    "section": "",
    "text": "Some newer tracks, sharing them on soundcloud."
  },
  {
    "objectID": "music/Basement_Tapes.html#concept",
    "href": "music/Basement_Tapes.html#concept",
    "title": "Basement Tapes",
    "section": "",
    "text": "Some newer tracks, sharing them on soundcloud."
  },
  {
    "objectID": "music/Basement_Tapes.html#production-notes",
    "href": "music/Basement_Tapes.html#production-notes",
    "title": "Basement Tapes",
    "section": "Production Notes",
    "text": "Production Notes\nI promise to say more about this, maybe, at some point."
  },
  {
    "objectID": "music/RS.html",
    "href": "music/RS.html",
    "title": "RS",
    "section": "",
    "text": "RS is an album of music. Inspired by synthpop. With much less vocoder, and mostly instrumental."
  },
  {
    "objectID": "music/RS.html#concept",
    "href": "music/RS.html#concept",
    "title": "RS",
    "section": "",
    "text": "RS is an album of music. Inspired by synthpop. With much less vocoder, and mostly instrumental."
  },
  {
    "objectID": "music/RS.html#production-notes",
    "href": "music/RS.html#production-notes",
    "title": "RS",
    "section": "Production Notes",
    "text": "Production Notes\nI promise to say more about this, maybe, at some point."
  },
  {
    "objectID": "music/VO.html",
    "href": "music/VO.html",
    "title": "VO",
    "section": "",
    "text": "VO is an album of music. Inspired by synthpop. Occasionally heavy use of vocoder."
  },
  {
    "objectID": "music/VO.html#concept",
    "href": "music/VO.html#concept",
    "title": "VO",
    "section": "",
    "text": "VO is an album of music. Inspired by synthpop. Occasionally heavy use of vocoder."
  },
  {
    "objectID": "music/VO.html#production-notes",
    "href": "music/VO.html#production-notes",
    "title": "VO",
    "section": "Production Notes",
    "text": "Production Notes\nI promise to say more about this, maybe, at some point."
  },
  {
    "objectID": "music/New.html",
    "href": "music/New.html",
    "title": "New stuff",
    "section": "",
    "text": "title\ntrack\n\n\n\n\n240108 - Screaming Modems\n  \n\n\n231230 - Quantum Freeloader\n  \n\n\n231809 - Dolphin Dance\n  \n\n\n231209 - Ambient droney drums reverb crush"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Homophony",
    "section": "",
    "text": "New stuff\n\n\nSome newer tracks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasement Tapes\n\n\nsynth explorations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRS\n\n\n2nd album, Fall 2020. Tasting notes: Synthpop, less vocoder\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVO\n\n\n1st album, Fall 2020. Tasting notes: Synthpop, lots of vocoder\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/16_1_16_24_transitions/index.html",
    "href": "blog/16_1_16_24_transitions/index.html",
    "title": "Transitions in wonderland",
    "section": "",
    "text": "Not sure where today’s post will take me, exciting times! The plan is to write a little bit in the morning, go play, write a little bit more, and so on. I’m hoping the writing will help focus my playing goals. Plus, I need to get back in the habit of writing everyday as the new semester approaches. So, win win."
  },
  {
    "objectID": "blog/16_1_16_24_transitions/index.html#explanatory-and-descriptive-theory",
    "href": "blog/16_1_16_24_transitions/index.html#explanatory-and-descriptive-theory",
    "title": "Transitions in wonderland",
    "section": "Explanatory and descriptive theory",
    "text": "Explanatory and descriptive theory\nThe title of this post is transitions. Mostly, I want to focus this on changes from one musical element to another. Before I head in that direction, I’ve been mulling over a more theoretical transition that has been helpful. My own mental transition from thinking about music theory as explanatory versus descriptive.\nWhen I was first learning music theory (Royal Conservatory of Music, Canadian styles) I was a kid and didn’t have a wide exposure to different ways of thinking about music or theory in general. I remember working through music theory exercise books as homework. I don’t remember what was in those books necessarily, but probably they gave me a lot of basic names and definitions for musical elements.\nIn high school and early undergrad years I was still playing music when I could, and was getting much more heavily into jazz and improvising. I was also learning more about science, philosophy of science, and cognitive psychology, which shaped a general interest in theory. My day job is now as a cognitive psychology prof, and after many years of that job, the topic of theory still holds my interest. For example, in cognitive psychology a common goal is to develop theories that “explain” some phenomena of interest. Given the complexity of human/animal cognition, explaining how something works is usually a very tall order. This also raises the question of what gets to count as a successful explanation. Mechanistic theories are often held in high regard, as they would describe how individual components of some system work together to produce the phenomena of interest. Here, a phenomena is “explained” in terms of the way that it’s component pieces/processes work together.\nWhen I became interested in learning “jazz” chords I was also scrambling to connect what I was learning back to music theory concepts I had been taught growing up. I can’t remember what my piano teachers had told me about what kind of “theory” music theory was supposed to be…I’m not sure that issue ever came up. Looking back, there were definitely a few years where I thought that music theory was somehow “explanatory”. For example, I would be analyzing a lead sheet trying to use music theory concepts to “understand” and “explain” why the melody went from here to there, or why the chord changed from this to that. Although sizeable chunks of a given song could be readily described using music theory concepts I had learned, there would be lots of changes that didn’t make sense to me. Sometimes a particularly inexplicable change would lead to hours of pondering possible contortions of music theory that might explain why something sounded good or made sense. I’ll give an example soon, but this is all to say that I was frustrated by looking toward music theory for explanation.\nAt that time I think I was missing the concept of “descriptive theory”. In cognitive science there are lots of examples of researchers using descriptive theories. Many of theses example derive from a logical positivist approach to science. Here, a primary goal is to predict and control a phenomena of interest. A common strategy is to create a descriptive system that can provide names for elements of a system, and mathematical functions to relate how elements move together or interact with one another. If a system can be well described using mathematical functions then it can be predicted and controlled. In this framework, the correspondence between reality and the descriptive system matters less than the amount of prediction and control achieved by the system. I’m getting too caught up here, and need to get back on track so I can practice more piano.\nHere’s a point I have been slowly internalizing. Music theory provides a systematic way of describing some musical elements. But, it’s a descriptive theory, not an explanatory theory. Music theory doesn’t explain many important aspects of music. For example, it doesn’t explain how feelings of tension and resolution and created by musical elements. Music theory is not very generative. I may understand the C major scale, and the modes, and the chords that come from the C major scale, but this understanding does not show me how to compose any given song with C as a tonal center.\nAt least in my experience (and I don’t have an advanced degree in music theory), music theory is less of a theory and more of a naming convention system, rather than a fully worked out descriptive theory that a logical positivist would be proud of. Potato, tomato.\n\n\n\nWhereas earlier I would go down “beautiful mind” rabbit holes trying to “explain” a lead sheet from a music theory point of view, now I’m much more content for music theory to be the exceptional descriptive system that it is. Exceptional in the sense that exceptions are used whenever they are needed. And, that’s just the way it is.\n\nAlice in wonderland\nHere’s an example of the rabbit holes I go down from “Alice in Wonderland”. And, maybe it would all “make sense” if only I knew more music theory 🤷.\n\nThe first seven chords follow a similar pattern, and there is more than one way to describe it. I used the blue pen to show the pattern in terms of leading 7ths being resolved to thirds in the next chord. For example, the first chord change is from D-7 to G7. In D-7, C is the 7th note, which moves down one semitone to a B, or major third in G7. In G7, F is the 7th note, which moves down one semitone to an E, or major third of C∆7.\nAspects of this pattern repeat until it starts breaking. The transition between B-7b5 and E7 is a little bit exceptional. Up until this point all of the notes are in the C major scale. If I was focusing on music theory as explanation I would ask, “what is G# doing in there?”. It sort of fits the pattern in that the 7th of of B-7b5 (A) is resolved to the third of E7 (G#). But, the pattern really breaks when going from A-7 to Eb7. Now, the 7th of A-7 (G) is repeated as the third in Eb7 (G), and all of the other notes in Eb7 (Eb Bb Db) are not in C major. There is also some interesting symmetry in the exceptions. The -iii7 chord in C is E-7, or EGBD. The E7 chord is close, and it sharpens the third EG#BD. Similarly, the Eb7 chord is close, but it keeps the G, and flattens the other notes, EbGBbDb. A kind of chordal call and response in terms of playing with exceptions to the C major scale.\nIt’s also possible to describe the chord changes from other perspectives. For example, instead of 7ths to 3rds, the pattern is also 1sts to 5ths. The root in D-7 becomes the fifth in G7. The root in G7 becomes the fifth in C∆7, and so on. This keeps repeating until the root in A-7 becomes a Bb in Eb7, which I guess is close enough.\nMy younger, and even older self on occasion, would get flustered by the Eb7, and consider questions like, “what music theory concepts make sense of these exceptions?”. It’s a fun question kind of question to ponder, but I’m more aware now of not getting too lost in pondering for too long.\nNow it’s flavor over explanation every time.\n\n\nCocktail music\nMusic is more like mixing cocktails. It should taste good and be interesting. I’m no mixologist, but I’ll mix up a Negroni once in a while with 1 part gin, 1 part vermouth, and 1 part campari. This cocktail is amenable to lots of variations. Switch the gin to bourbon for a boulevardier. Add a twist. Play with the ratios. Add little bits of more stuff for extra fun.\nWith the cocktail metaphor we can have an almost useless generic descriptive theory of cocktails. Cocktails are composed of multiple component parts, usually liquids, but not always, in particular ratios to each other. This kind of theory isn’t useful for making any cocktails. It can be useful to describe a cocktail using the system and then notice elements that might be analogized to other cocktails. For example, a negroni is 3 equal parts with lots of options for substitutions, implying a whole family of negroni-like drinks. So, the system could be used to explore negroni-style drinks by switching out pieces, trying different amaros instead of campari, etc. And, by giving names to components of cocktails, similarities may appear to arise between cocktails based on the collections of components. For example, the Manhattan is 2 parts bourbon 1 part red vermouth. It’s like a boulevardier, but substituting campari for more whiskey. In any case, I don’t need cocktails to make sense, they just need to taste good. And, they often taste exceptional when they contain some random stuff from left field that I never expected.\nSo, with music, the mood is to trod around in places that seem to have nice regular structure, and then throw in the good tasty flavors as much as possible.\nAnd, now to the thing I actually wanted to write about."
  },
  {
    "objectID": "blog/16_1_16_24_transitions/index.html#transitions-and-movement",
    "href": "blog/16_1_16_24_transitions/index.html#transitions-and-movement",
    "title": "Transitions in wonderland",
    "section": "Transitions and movement",
    "text": "Transitions and movement\nThis should probably be a second post, but the above is a preface on transitions. I actually failed to state one of the main points I intended to make in the above discussion—perspective does make things seem different. Even though music theory is more of a descriptive than explanatory theory, the descriptions change perspective, even if they are describing the same thing. For example, a D-7 chord invokes certain concepts for me. It’s the -ii in C major. It’s D Dorian. It sounds a certain way, etc. That same chord in the 1st inversion is FACD, which is also an Fmaj6 chord that invites a different set of musical ideas. As a sidenote, it would be interesting to discuss these perspective shifts a bit further from a cognitive psychology perspective (e.g., in terms of the associative fan effect, and other phenomena).\nNow, onto the main post.\nOn review, much of the exercises I have been working on lately are “noun” focused. I’ve been working on chords and modes mostly in the major scale, and trying to get all of that stuff more accessible to my fingers when I am playing. I could also call this “object” focused, where I’m trying to increase the fluency in my object production, where the objects are chords or scales. I’m getting better, so that’s good.\nBut, a question is looming that I have not been very actively addressing. These are questions about movement and transition. If I am here…where do I go from here. Where is there?\nI need to start shifting my exercise into a verb focus, where I’m taking actions with musical elements, rather than just being a finger factory that produces musical doo-dads with high fluency for no reason.\nEnter dominant chords, piano YouTube, and Barry Harris.\nI’m not sure how organized the next stuff will be. Mostly a scratch pad."
  },
  {
    "objectID": "blog/16_1_16_24_transitions/index.html#fully-diminished-chords-and-more",
    "href": "blog/16_1_16_24_transitions/index.html#fully-diminished-chords-and-more",
    "title": "Transitions in wonderland",
    "section": "Fully diminished chords and more",
    "text": "Fully diminished chords and more\nI feel confident that I learned important things about fully diminished 7th chords when I was younger. But, I completely forget about them until yesterday when I started watching Shan Verma discussing Alice in Wonderland on youtube.\nThis quickly led to a series of rabbit holes about fully diminished 7th chords, Barry Harris and his system, and the major 6th diminished scale.\nBut, before I get to that. I need to go make lunch.\n\nLunch break is over. Now, my goal is to put down some breadcrumbs and then get back to the piano.\n\nBreadcrumb 1 - Fully Diminished 7th chords\nThese are composed of entirely of minor 3rds. So, a fully diminished Cbb7 is CEbGbA.\nRandomly came across this series of observations about using these chords for transitions\nThere are only three different versions of this chord–C, Db and D–as the inversions cover all of the roots.\nThey are very useful for transitions.\nMoving any note down a semitone turns the chord into a dominant 7th chord. Very useful for modulation.\n\n\nBreadcrumb 2 - major 6th diminished scale\nThis scale has 8 notes, adding a flat 6 to the major scale\nC D E F G A Ab B\n\n\nBreadcrumb 3 - Barry Harris’s system\nI haven’t had enough to time to learn what this whole system is. So far, I have just watched one youtube video from Shan Verma on this.\nThe trail of breadcrumbs started at the beginning of his Alice in Wonderland analysis video. In that video he played alternating inversions of maj6 and fully diminished chords, and that immediately clicked something for me.\nThere are four inversions of Cmaj6 (CEGA), starting on each note in chord. These are also four of the notes in the major 6th diminished scale.\nThere are four inversions of Ddim7 (DFAbB), starting on each note in the chord. These are also the four remaining notes in the major 6th diminished scale.\nIt is possible to play each note of that scale as a series of maj6 and dim7 chords.\n\n\n\nC\nD\nE\nF\nG\nA\nAb\nB\n\n\n\n\n1st\n1st\n2nd\n2nd\n3rd\n3rd\n4th\n4th\n\n\nCMaj6\nDdim7\nCMaj6\nDdim7\nCMaj6\nDdim7\nCMaj6\nDdim7\n\n\nCEGA\nDFAbB\nEGAC\nFAbBD\nGACE\nACEG\nAbBDF\nBDFAb\n\n\n\nThis is basically where I am at right. I spent the morning locking my fingers into these inversions so I could play through the scale in chords. Loving the sound and the possibilities. Giving me lots to think about.\nNext up is to practice through the other diminished chords.\nDid a bit of that. Got through C, F, and Bb. Time to make another table for my reference.\nThis shows the notes in every the major 6th diminished scale. I think I fixed all the typos. The scales are ordered in terms of the circle of fifths, going anti-clockwise. The bolded notes are fully diminished 7ths."
  },
  {
    "objectID": "blog/16_1_16_24_transitions/index.html#tables-major-6th-diminished-scale",
    "href": "blog/16_1_16_24_transitions/index.html#tables-major-6th-diminished-scale",
    "title": "Transitions in wonderland",
    "section": "Tables: Major 6th diminished scale",
    "text": "Tables: Major 6th diminished scale\n\n\n\ni\nii\niii\niv\nv\nvi\nvii\nviii\n\n\n\n\nC\nD\nE\nF\nG\nAb\nA\nB\n\n\nF\nG\nA\nBb\nC\nDb\nD\nE\n\n\nBb\nC\nD\nEb\nF\nGb\nG\nA\n\n\nEb\nF\nG\nAb\nBb\nB\nC\nD\n\n\nAb\nBb\nC\nDb\nEb\nE\nF\nG\n\n\nDb\nEb\nF\nGb\nAb\nA\nBb\nC\n\n\nGb\nAb\nBb\nB\nDb\nD\nEb\nF\n\n\nB\nDb\nEb\nE\nGb\nG\nAb\nBb\n\n\nE\nGb\nAb\nA\nB\nC\nDb\nEb\n\n\nA\nB\nDb\nD\nE\nF\nGb\nAb\n\n\nD\nE\nGb\nG\nA\nBb\nB\nDb\n\n\nG\nA\nB\nC\nD\nEb\nE\nGb\n\n\n\nThis is the same table, but the scales are grouped in terms whether they share the same diminished chord.\n\n\n\ni\nii\niii\niv\nv\nvi\nvii\nviii\n\n\n\n\nC\nD\nE\nF\nG\nAb\nA\nB\n\n\nEb\nF\nG\nAb\nBb\nB\nC\nD\n\n\nGb\nAb\nBb\nB\nDb\nD\nEb\nF\n\n\nA\nB\nDb\nD\nE\nF\nGb\nAb\n\n\n–\n–\n–\n–\n–\n–\n–\n–\n\n\nF\nG\nA\nBb\nC\nDb\nD\nE\n\n\nAb\nBb\nC\nDb\nEb\nE\nF\nG\n\n\nB\nDb\nEb\nE\nGb\nG\nAb\nBb\n\n\nD\nE\nGb\nG\nA\nBb\nB\nDb\n\n\n–\n–\n–\n–\n–\n–\n–\n–\n\n\nBb\nC\nD\nEb\nF\nGb\nG\nA\n\n\nDb\nEb\nF\nGb\nAb\nA\nBb\nC\n\n\nE\nGb\nAb\nA\nB\nC\nDb\nEb\n\n\nG\nA\nB\nC\nD\nEb\nE\nGb\n\n\n\nSame table again with some extra stuff. I started playing through these scales and was trying to figure out what aspects would be new to me. For each grouping of four, it is nice that that the diminished chord is the same, it is just expressed as a different inversion. Within each scale, the major chord is also the same, just expressed as a different inversion.\nThe new column simply points out the maj6/min7 relations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmaj6 = -7\ni\nii\niii\niv\nv\nvi\nvii\nviii\n\n\n\n\nC6 = A-7\nC\nD\nE\nF\nG\nAb\nA\nB\n\n\nEb6 = C-7\nEb\nF\nG\nAb\nBb\nB\nC\nD\n\n\nGb6 = Eb-7\nGb\nAb\nBb\nB\nDb\nD\nEb\nF\n\n\nA6 = Gb-7\nA\nB\nDb\nD\nE\nF\nGb\nAb\n\n\n\n–\n–\n–\n–\n–\n–\n–\n–\n\n\nF6 = D-7\nF\nG\nA\nBb\nC\nDb\nD\nE\n\n\nAb6 = F-7\nAb\nBb\nC\nDb\nEb\nE\nF\nG\n\n\nB6 = Ab-7\nB\nDb\nEb\nE\nGb\nG\nAb\nBb\n\n\nD6 = B-7\nD\nE\nGb\nG\nA\nBb\nB\nDb\n\n\n\n–\n–\n–\n–\n–\n–\n–\n–\n\n\nBb6 = G-7\nBb\nC\nD\nEb\nF\nGb\nG\nA\n\n\nDb6 = Bb-7\nDb\nEb\nF\nGb\nAb\nA\nBb\nC\n\n\nE6 = Db-7\nE\nGb\nAb\nA\nB\nC\nDb\nEb\n\n\nG6 = E-7\nG\nA\nB\nC\nD\nEb\nE\nGb\n\n\n\nThis table could be useful for tapping into things I already know. I can play 6th chords fairly well, but I already know my min7 chords very well. Sometimes thinking about a chord in terms of another helps me play it, and it encourages shifting perspectives on the same thing. The Eb scale got easier to play once I realized the Eb6 was just a 1st inversion C-7 chord waiting to happen…and then I’m like, OMG C-7 is an Eb major 6th 🤯."
  },
  {
    "objectID": "blog/11_1_7_24_saddle/index.html",
    "href": "blog/11_1_7_24_saddle/index.html",
    "title": "Back in the saddle",
    "section": "",
    "text": "“a piano riding a horse. piano rides horse. piano riding a horse. piano keys. synthesizer keyboard.” - LCM_Dreamshaper_v7\nVacation was nice. Easing back into practice.\n\nPlayed through each scale using 7ths chords going anti-clockwise through the circle of fifths.\n\nI’ve overplayed the C major scale for decades and should probably just stop practicing in C. At the same time, I can rip through the C major scale using 7th chords very quickly. In addition to practice, it seems easier to “see” the 7th chords in C major– they all have the same shape and use only white keys. It would be nice to have that immediacy for all of the other scales, and this is something I’m slowly working towards.\n\nDid a little bit of focused practice in G. Played through the G Major scale using 7th chords to build up speed. Played up and down using each inversion of each 7th chord."
  },
  {
    "objectID": "blog/1_12_20_23_dolphin_dance/index.html",
    "href": "blog/1_12_20_23_dolphin_dance/index.html",
    "title": "Dolphin Dance",
    "section": "",
    "text": "” Herbie Hancock’s dolphin dance with a bunch of synthesizers” - LCM_Dreamshaper_v7\n  \nHave been messing about with Herbie Hancock’s “Dolphin Dance” on the piano for a while.\nHad a clear day to try some mixer mimicking. Plan was to develop my own voicings for everything, but mix them similar to the Maiden Voyage recording…and then jam it.\nWhat happened:\n\nRecorded the Hancock version into Ableton\nLocated BPM\nVoices\n\nRhodes\nMoog matriarch bass lines\nMPC drums\nOB-6 horns\n\nFor each voice, play with the original record. Keep recording in a loop using Ableton comping lanes\nFix messes by sampling in comping lanes/ explore have fun with fixing the mess\nDuplicate the head, then solo overtop for a bit\nMix it down.\nTry some final mixing touches."
  },
  {
    "objectID": "blog/23_1_23_24_minerva/index.html",
    "href": "blog/23_1_23_24_minerva/index.html",
    "title": "Minerva and expectancy from a chord vector space",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"Minerva robot of wisdom. binary codes. thundercat cartoon. music. music theory. conceptual. reverberation. resonance. colorful.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nThis is a scratchpad post with R code to explore some esoteric computational modeling ideas. I want to get coding, but will put a bit of context around this.\nMy plan is to take the chord vector space I made yesterday (see last post), and put it into the memory of a MINERVA-II model. Then, I’m going to probe the model with various input patterns, and see what comes out.\nI wish I had time to review MINERVA-II in more depth here, but I don’t. Very quickly, MINERVA-II is an instance-based model of human memory processes by Douglas Hintzman (Hintzman 1984). This model was inspired by Richard Semon’s memory theory (Semon 1923), which I find very poetic. Semon made up his own terms so that he could more precisely state his theoretical ideas, including words like engram, engraphic, and homophony.\nThe basic idea is that people store the patterns of individual experiences in memory. And, a current pattern can retrieve old memories by similarity. MINERVA-II uses a resonance metaphor. A pattern is presented to a memory system. The pattern activates all of the traces in memory, in proportion to their similarity to the pattern. In this way, memory is call and response process. The pattern of the present moment resonates with the memory system bringing forth a chorus of activated traces. This memory response is called the echo. The resonance between the structure of the pattern in the present moment and similar traces from the past is what Richard Semon called homophony. I have some lecture material on these concepts in my intro to cognition course.\nNow onto the R code. Memory is the chord vector matrix. I can “probe” the model by giving it any feature vector as an input. The input probe activates every chord in memory by it’s similarity (using the vector cosine). The memory responds as a similarity weighted sum. All of traces are multiplied by their similarity, and then summed up into a single feature vector, called the echo."
  },
  {
    "objectID": "blog/23_1_23_24_minerva/index.html#loading-the-chord-vector-space",
    "href": "blog/23_1_23_24_minerva/index.html#loading-the-chord-vector-space",
    "title": "Minerva and expectancy from a chord vector space",
    "section": "Loading the chord vector space",
    "text": "Loading the chord vector space\nThe excel file for the chord vector space is available from the github repo for this blog.\n\n\nShow the code\nlibrary(tidyverse)\n# pre-processing to get the chord vectors\n\n# load chord vectors\nc_chord_excel &lt;- rio::import(\"chord_vectors.xlsx\")\n\n# grab feature vectors\nc_chord_matrix &lt;- as.matrix(c_chord_excel[,4:15])\n\n# assign row names to the third column containing chord names\nrow.names(c_chord_matrix) &lt;- c_chord_excel[,3]\n\n# define all keys\nkeys &lt;- c(\"C\",\"Db\",\"D\",\"Eb\",\"E\",\"F\",\"Gb\",\"G\",\"Ab\",\"A\",\"Bb\",\"B\")\n\n\n# the excel sheet only has chords in C\n# loop through the keys, permute the matrix to get the chords in the next key\n# add the permuted matrix to new rows in the overall chord_matrix\nfor (i in 1:length(keys)) {\n  \n  if (i == 1) {\n    # initialize chord_matrix with C matrix\n    chord_matrix &lt;- c_chord_matrix\n    \n  } else {\n    #permute the matrix as a function of iterator\n    new_matrix &lt;- cbind(c_chord_matrix[, (14-i):12],c_chord_matrix[, 1:(13-i)] )\n    \n    # rename the rows with the new key\n    new_names &lt;- gsub(\"C\", keys[i], c_chord_excel[,3])\n    row.names(new_matrix) &lt;- new_names\n    \n    # append the new_matrix to chord_matrix\n    chord_matrix &lt;- rbind(chord_matrix,new_matrix)\n  \n    }\n}\n\n\nEach chord is represented as a vector with 12 features, corresponding to each of the 12 possible notes. If a note is in a chord, then the note feature gets a 1 in the vector. All other features are set to 0.\nThe first 10 rows look like this:\n\n\nShow the code\nknitr::kable(c_chord_excel[1:10,])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nkey\ntype\nitem\nC\nDb\nD\nEb\nE\nF\nGb\nG\nAb\nA\nBb\nB\n\n\n\n\nC\nkey\nC note\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nC\nscale\nC major scale\n1\n0\n1\n0\n1\n1\n0\n1\n0\n1\n0\n1\n\n\nC\ntriads\nC major triad\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n\n\nC\ntriads\nC minor triad\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nC\ntriads\nC6\n1\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\n\n\nC\ntriads\nCm6\n1\n0\n0\n1\n0\n0\n0\n1\n0\n1\n0\n0\n\n\nC\ntriads\nC (add 9)\n1\n0\n1\n0\n1\n0\n0\n1\n0\n0\n0\n0\n\n\nC\ntriads\nCm (add 9)\n1\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nC\ndominant 7th\nC7\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n\n\nC\ndominant 7th\nC9\n1\n0\n1\n0\n1\n0\n0\n1\n0\n0\n1\n0\n\n\n\n\n\nThe vector space includes one feature vector for all of the following chords and scales:\nC note, C major scale, C major triad, C minor triad, C6, Cm6, C (add 9), Cm (add 9), C7, C9, C9(#11), C9 (13), C13 (#11), C∆7, C∆9, C∆9(#11), C∆9(13), C∆7(#5), C∆7(b5), Cm7, Cm9, Cm11, Cm7(11), Cm13, Cm13(#11), Cm7(b5), Cm9(b5), Cm11(b5), Csus, C7sus, C9sus, C13sus, C7susb9, C13susb9, C7(b5), C7(#5), C7(b9), C7#9b5, C7#9#5, C7b9b5, C7b9#5, Cm∆7, Cm∆9, Cdim, Co7, Cdim(∆7), C7#11#9, C chromatic, C whole-tone, C major pentatonic, C minor pentatonic, C Ionian, C Dorian, C Phrygian, C Lydian, C Mixolydian, C Aeolian, C Locrian, C maj 6th diminished, C melodic minor, C half-step/whole-step, C whole-step/half-step, C Blues\nThe above shows everything in the key of C. The matrix contains all of the above in all of the keys. For a total of 756 patterns to be stored in the memory matrix.\nIn the next sections I’ll be giving this model an input pattern as a “reminder cue”, and then computing what the model “remembers” based on the cue. This is a way of asking about associations or expectations between one pattern and a history of other patterns. The answers the model gives back are entirely dependent on the nature of the memory traces.\nThe current set of chord vectors is very unlike my own musical experience. If I tried to capture my own musical experience as a series of individual traces, I would be inputting one feature vector for every chord, note, scale, or let’s say short phrase, that I have ever played in my entire life. That collection of traces would be severely biased in terms key, as I way over played things in CFGDA in my life.\nThe chord vector space I’m using here is more like an uniform agent who played every chord and scale equally frequently in all keys. So, the expectations returned by the model are in relation to that kind of unbiased musical history."
  },
  {
    "objectID": "blog/23_1_23_24_minerva/index.html#minerva-ii-modeling",
    "href": "blog/23_1_23_24_minerva/index.html#minerva-ii-modeling",
    "title": "Minerva and expectancy from a chord vector space",
    "section": "MINERVA II modeling",
    "text": "MINERVA II modeling\n\nProbe with a C note\nThe following code shows the basic steps in probing the memory with a cue pattern. I used a C note, which is coded as a single 1, followed by 11 zeros.\nThe cosine similarity between the probe and all patterns in memory is computed. There is a possibility of “tuning” the similarities by raising them to an exponent, but I’ll talk about that later.\nThe individual patterns in memory are multiplied by their similarity to the probe. This allows the cue to selectively retrieve memories that contain features in the cue. For example, traces that have 0 similarity to the cue will be multiplied by 0, and thus eliminated from the echo. The echo is produced by summing the similarity weighted traces.\nThe values in the echo are additive and can get very large. In the last step I divide all of the values in the echo by the maximum value to squish them between 0 and 1.\n\n\nShow the code\n# Try minerva\nmemory &lt;- chord_matrix\n\n# probe with a C\n# Each of the 12 spots is a note, starting on C\nprobe &lt;- c(1,0,0,0,0,0,0,0,0,0,0,0)\n\n# compute similarities between probe and all traces\nsimilarities &lt;- RsemanticLibrarian::cosine_x_to_m(probe,memory)\n\n# tuning function: raise similarities to a power \nactivations &lt;- similarities^1\n\necho &lt;- colSums((memory*c(activations)))\necho &lt;- echo/max(abs(echo))\n\necho\n\n\n        C        Db         D        Eb         E         F        Gb         G \n1.0000000 0.2163667 0.4790949 0.4772587 0.4200609 0.5390773 0.3656121 0.5390773 \n       Ab         A        Bb         B \n0.4200609 0.4772587 0.4790949 0.2163667 \n\n\nI like to think of the echo as the reminder values. Given the model hears a C, it is reminded of things that have a C in them. The echo is a similarity weighted sum of all of those things.\n\n\nFull cacophony\nA couple short detours before going in a more musical direction with this.\nThe echo in MINERVA-II is the concept that memory retrieval acts like a chorus of singers, where each singer is an individual memory trace.\nConsider what would happen if memory was totally unselective and everything was retrieved all at once.\nIn the model, this would be like hearing all of the chords in the memory all at once. This can be represented by summing every trace together into one echo. This is the same as summing down the columns of the matrix like so:\n\n\nShow the code\ncolSums(memory)\n\n\n  C  Db   D  Eb   E   F  Gb   G  Ab   A  Bb   B \n338 338 338 338 338 338 338 338 338 338 338 338 \n\n\nEach note appears 338 times across all of the chords in memory. If they were all played at once, the echo would sound like every note played simultaneously with a loudness of 338. If I normalize the echo by dividing by 338, I’d get all 1s, which would be saying play all the notes at 100% amplitude. In other words, if memory reminded you of everything, everywhere, all at once, it sounds like full cacophony.\n\n\nThe sound of unbiased memory from C\nMINERVA-II allows for selective retrieval of prior memories. The primary mechanism is that a probe pattern activates memories by similarity.\nIn this example, I apply the ceiling() function to the similarities and transform any positive value to 1, and leave the 0s at 0.\nI’m using the C probe, so any chord pattern that has a C element in it will get a 1, and any chord pattern that does not have a C in it will get a 0.\nI calculate both the echo and the normalized echo.\n\n\nShow the code\n# Try minerva\nmemory &lt;- chord_matrix\n\n# probe with a C\n# Each of the 12 spots is a note, starting on C\nprobe &lt;- c(1,0,0,0,0,0,0,0,0,0,0,0)\n\n# compute similarities between probe and all traces\nsimilarities &lt;- RsemanticLibrarian::cosine_x_to_m(probe,memory)\n\n# force similarities to 0 or 1\nactivations &lt;- ceiling(similarities)\n\necho &lt;- colSums((memory*c(activations)))\necho\n\n\n  C  Db   D  Eb   E   F  Gb   G  Ab   A  Bb   B \n338  81 172 169 148 191 130 191 148 169 172  81 \n\n\nShow the code\necho &lt;- echo/max(abs(echo))\necho\n\n\n        C        Db         D        Eb         E         F        Gb         G \n1.0000000 0.2396450 0.5088757 0.5000000 0.4378698 0.5650888 0.3846154 0.5650888 \n       Ab         A        Bb         B \n0.4378698 0.5000000 0.5088757 0.2396450 \n\n\nThe first echo is basically co-occurrence context vector containing the co-occurrence frequency between C and the other notes, counting across all of the patterns in memory. The second echo is the same information, just in terms of proportion to the largest value.\nC always co-occurs with itself. C co-occurs next most often with G and F, and then Bb and D etc.\nThis echo is not as cacophonous as hearing every single chord in memory played at the same time. However, i’m guessing this would still sound pretty cacophonous, as it is the sound of about 338 patterns that all contain a C played at the same time.\nAt some point, hopefully today, I’d like to synthesize tones using these echo values for note amplitude and hear what they sound like.\n\n\nIncreasingly selective echoes of C\nMINERVA-II has a few options for controlling how many memories get added into the echo. After computing similarities between the probe and memory traces, the similarities can be raised to a power before weighting the traces. As the exponent increases, smaller similarity values get squashed into 0 and become effectively 0. Larger similarity values remain proportionally larger. Perfectly similar traces remain at 1 regardless of the exponent.\nThe bottom line is that as the power is raised, fewer traces (only the most similar) are allowed to contribute to the echo. As a result, the echo becomes much less cacophonous.\nThe code below shows what happens when the probe is a C, and the exponent is raised to 1, 3, 11, and 51.\nWhen the exponent is small, C is the loudest feature in the echo, but many other notes have some loudness too.\nWhen the exponent is increased, the C remains the loudest, but the other notes get softer.\nIn the case of this vector space, driving up the exponent really, really high, essentially causes only the identical patterns in memory to be retrieved. In the extreme, the C retrieves itself, and there are not other sounds of co-occurrence.\n\n\nShow the code\n# Try minerva\nmemory &lt;- chord_matrix\n\n# probe with a C\n# Each of the 12 spots is a note, starting on C\nprobe &lt;- c(1,0,0,0,0,0,0,0,0,0,0,0)\n\n# compute similarities between probe and all traces\nsimilarities &lt;- RsemanticLibrarian::cosine_x_to_m(probe,memory)\n\necho &lt;- colSums((memory*c(similarities^1)))\necho/max(abs(echo))\n\n\n        C        Db         D        Eb         E         F        Gb         G \n1.0000000 0.2163667 0.4790949 0.4772587 0.4200609 0.5390773 0.3656121 0.5390773 \n       Ab         A        Bb         B \n0.4200609 0.4772587 0.4790949 0.2163667 \n\n\nShow the code\necho &lt;- colSums((memory*c(similarities^3)))\necho/max(abs(echo))\n\n\n        C        Db         D        Eb         E         F        Gb         G \n1.0000000 0.1731107 0.4114787 0.4288597 0.3797730 0.4798933 0.3271873 0.4798933 \n       Ab         A        Bb         B \n0.3797730 0.4288597 0.4114787 0.1731107 \n\n\nShow the code\necho &lt;- colSums((memory*c(similarities^11)))\necho/max(abs(echo))\n\n\n          C          Db           D          Eb           E           F \n1.000000000 0.005599951 0.016650339 0.025743877 0.020721170 0.026529044 \n         Gb           G          Ab           A          Bb           B \n0.018535963 0.026529044 0.020721170 0.025743877 0.016650339 0.005599951 \n\n\nShow the code\necho &lt;- colSums((memory*c(similarities^51)))\nround(echo/max(abs(echo)))\n\n\n C Db  D Eb  E  F Gb  G Ab  A Bb  B \n 1  0  0  0  0  0  0  0  0  0  0  0"
  },
  {
    "objectID": "blog/23_1_23_24_minerva/index.html#messing-around",
    "href": "blog/23_1_23_24_minerva/index.html#messing-around",
    "title": "Minerva and expectancy from a chord vector space",
    "section": "Messing around",
    "text": "Messing around\nI’m using this code block to try different probe patterns and see what happens. In general, the echo contains the elements of the probe, and then partial activation of other elements in approximate orders that seem to make musical sense.\n\n\nShow the code\n# Try minerva\nmemory &lt;- chord_matrix\n\n# probe using row names\nprobe &lt;- chord_matrix['C major triad',]\n\n# compute similarities between probe and all traces\nsimilarities &lt;- RsemanticLibrarian::cosine_x_to_m(probe,memory)\n\n# tuning function: raise similarities to a power \nactivations &lt;- similarities^3\n\necho &lt;- colSums((memory*c(activations)))\necho &lt;- echo/max(abs(echo))\n\nsort(echo,decreasing = TRUE)\n\n\n        G         C         E         A         D        Bb         F         B \n1.0000000 0.9648918 0.9283912 0.6326961 0.6308542 0.4828259 0.4436583 0.4141189 \n       Gb        Eb        Db        Ab \n0.3390815 0.3329919 0.3127329 0.3109608 \n\n\n\nAdding probes together\nLet’s say one is playing a Dm7 chord in the left hand, and a G as part of a melody line. A probe could be constructed by adding together the vector for Dm7 and G. I’m also sorting the echo by feature intensity. I wonder if the order of notes in the echo could work for figuring out which scales to play over what chords and situations.\n\n\nShow the code\n# Try minerva\nmemory &lt;- chord_matrix\n\n# probe using row names\nprobe &lt;- colSums(chord_matrix[c('Dm7','G note'),])\n\n# compute similarities between probe and all traces\nsimilarities &lt;- RsemanticLibrarian::cosine_x_to_m(probe,memory)\n\n# tuning function: raise similarities to a power \nactivations &lt;- similarities^3\n\necho &lt;- colSums((memory*c(activations)))\necho &lt;- echo/max(abs(echo))\n\nsort(echo,decreasing = TRUE)\n\n\n        C         D         G         F         A         E        Bb        Eb \n1.0000000 0.9970230 0.9948008 0.9326157 0.9326157 0.5419088 0.5419088 0.4465277 \n        B        Ab        Gb        Db \n0.4378429 0.3103659 0.2854669 0.2686588 \n\n\n\n\nDiscrepancy\nThe echo contains partial activations of non-probe features. These in some sense represent an expectation about what elements usually co-occur with the probe features in the stored memory traces.\nIt may be interesting to compute a discrepancy vector, which is a difference between the pattern in the probe and the echo.\nThese differences in expectation might be interesting to think about in terms of musical tension and resolution.\nrandom notes:\n\nsubtraction introduces negative values and negative similarity\nwhich way to subtract? probe-echo or echo-probe\n\n\n\nShow the code\n# Try minerva\nmemory &lt;- chord_matrix\n\n# probe using row names\nprobe &lt;- chord_matrix['C note',]\n\n# compute similarities between probe and all traces\nsimilarities &lt;- RsemanticLibrarian::cosine_x_to_m(probe,memory)\n\n# tuning function: raise similarities to a power \nactivations &lt;- similarities^1\n\necho &lt;- colSums((memory*c(activations)))\necho &lt;- echo/max(abs(echo))\necho\n\n\n        C        Db         D        Eb         E         F        Gb         G \n1.0000000 0.2163667 0.4790949 0.4772587 0.4200609 0.5390773 0.3656121 0.5390773 \n       Ab         A        Bb         B \n0.4200609 0.4772587 0.4790949 0.2163667 \n\n\nShow the code\ndiscrepancy &lt;- echo-probe\ndiscrepancy\n\n\n        C        Db         D        Eb         E         F        Gb         G \n0.0000000 0.2163667 0.4790949 0.4772587 0.4200609 0.5390773 0.3656121 0.5390773 \n       Ab         A        Bb         B \n0.4200609 0.4772587 0.4790949 0.2163667 \n\n\nIn this case the discrepancy vector has activation across all notes except C. Although the activation is not uniform, this discrepancy vector is similar to the chromatic scale, which is all of the notes.\nSubmitting the discrepancy vector as a probe to memory, and then listing the top 10 most similar traces in memory as a way to interpret the vector in terms of the chord patterns.\n\n\nShow the code\nprobe &lt;- discrepancy\n\n# compute similarities between probe and all traces\nsimilarities &lt;- RsemanticLibrarian::cosine_x_to_m(probe,memory)\n\ndf &lt;- data.frame(chords = row.names(similarities),\n                 similarities = similarities) %&gt;%\n  arrange(desc(similarities))\ndf[1:10,]\n\n\n                   chords similarities\nC chromatic   C chromatic    0.9274934\nDb chromatic Db chromatic    0.9274934\nD chromatic   D chromatic    0.9274934\nEb chromatic Eb chromatic    0.9274934\nE chromatic   E chromatic    0.9274934\nF chromatic   F chromatic    0.9274934\nGb chromatic Gb chromatic    0.9274934\nG chromatic   G chromatic    0.9274934\nAb chromatic Ab chromatic    0.9274934\nA chromatic   A chromatic    0.9274934\n\n\nAdding the echo to probe. After hearing a note the model retrieves the echo as a response. In this new moment the original note and the retrieved chorus are a new combined probe.\n\n\nShow the code\n# Try minerva\nmemory &lt;- chord_matrix\n\n# probe using row names\nprobe &lt;- chord_matrix['C note',]\n\n# compute similarities between probe and all traces\nsimilarities &lt;- RsemanticLibrarian::cosine_x_to_m(probe,memory)\n\n# tuning function: raise similarities to a power \nactivations &lt;- similarities^1\n\necho &lt;- colSums((memory*c(activations)))\necho &lt;- echo/max(abs(echo))\necho\n\n\n        C        Db         D        Eb         E         F        Gb         G \n1.0000000 0.2163667 0.4790949 0.4772587 0.4200609 0.5390773 0.3656121 0.5390773 \n       Ab         A        Bb         B \n0.4200609 0.4772587 0.4790949 0.2163667 \n\n\nShow the code\n# add echo to probe\nprobe &lt;- probe+echo\n\n# compute similarities between probe and all traces\nsimilarities &lt;- RsemanticLibrarian::cosine_x_to_m(probe,memory)\n\nactivations &lt;- similarities^1\n\necho &lt;- colSums((memory*c(activations)))\necho &lt;- echo/max(abs(echo))\necho\n\n\n        C        Db         D        Eb         E         F        Gb         G \n1.0000000 0.6284936 0.7864139 0.7701819 0.7288598 0.8174667 0.6928161 0.8174667 \n       Ab         A        Bb         B \n0.7288598 0.7701819 0.7864139 0.6284936 \n\n\nThis is like having steps of iterative retrieval. A variation is to get the retrieved echo and then submit the echo as the probe. What happens is that the echo fills up with more general co-occurrence information.\n\n\nEcho meaning\nThe echo is a feature vector in the same space as the chords. In general, the echo will contain more activation across all elements compared to any individual chord. This is because the echo sums over many chords, and typically sums over enough chords that all notes end up in the sum.\nIn this sense, the fact that an echo usually has partial activation across all notes, makes the pattern in the echo similar to the chromatic scale, which has all notes. This is not a particularly interesting or nuanced meaning of the echo. If the echo was all 1s, then it would be the chromatic scale.\nThe activation values in the echo depend on the activation function that raises similarity to a power. A given echo can be interpreted in terms of the original chord vectors by calculating similarity between the echo and all of the chords, and then looking at the chords that are most similar. When the exponent is small, the most similar chords returned are all the chromatic scales (which are identical), and other chords with lots of notes in them.\nAs the exponent is raised higher, the pattern in the echo grows more similar to the probe pattern (with some extra activations), and the echo becomes similar to different patterns of chords, eventually honing in on the same ordering as the probe pattern would have.\n\n\nShow the code\n# Try minerva\nmemory &lt;- chord_matrix\n\n# probe using row names\nprobe &lt;- chord_matrix['C note',]\n\n# compute similarities between probe and all traces\nsimilarities &lt;- RsemanticLibrarian::cosine_x_to_m(probe,memory)\n\n# tuning function: raise similarities to a power \nactivations &lt;- similarities^9\n\necho &lt;- colSums((memory*c(activations)))\necho &lt;- echo/max(abs(echo))\necho\n\n\n         C         Db          D         Eb          E          F         Gb \n1.00000000 0.02269835 0.06352622 0.08694634 0.07307524 0.09134910 0.06433469 \n         G         Ab          A         Bb          B \n0.09134910 0.07307524 0.08694634 0.06352622 0.02269835 \n\n\nShow the code\necho_similarities &lt;- RsemanticLibrarian::cosine_x_to_m(echo,memory)\ndf &lt;- data.frame(chords = row.names(echo_similarities),\n                 similarities = echo_similarities) %&gt;%\n  arrange(desc(echo_similarities))\ndf[1:20,]\n\n\n                       chords similarities\nC note                 C note    0.9732485\nCsus                     Csus    0.6645644\nC minor triad   C minor triad    0.6620904\nF major triad   F major triad    0.6620904\nAdim                     Adim    0.6596165\nC major triad   C major triad    0.6542962\nF minor triad   F minor triad    0.6542962\nAb major triad Ab major triad    0.6518223\nA minor triad   A minor triad    0.6518223\nFsus                     Fsus    0.6489305\nGsus                     Gsus    0.6489305\nCdim                     Cdim    0.6469109\nGbdim                   Gbdim    0.6469109\nF (add 9)           F (add 9)    0.6178398\nCm6                       Cm6    0.6156973\nF7                         F7    0.6156973\nAm7(b5)               Am7(b5)    0.6156973\nFm (add 9)         Fm (add 9)    0.6110898\nC6                         C6    0.6089473\nF∆7                       F∆7    0.6089473\n\n\nIn this variation the echo is submitted as the probe to generate a second echo. The first echo is already very general because even a single C note is in many chords. The second echo is extremely general because it has positive similarity to all chords. The values in the second echo can be thought of as reflecting very general expectations about note co-occurrence. The first echo also has some of these very general expectations.\nWhat happens here is some proportion of the second echo, which represents super general features, is subtracted from the first echo. This allows the first echo to reflect more nuanced and specific expectations given the probe.\nIt seems necessary to turn this into a shiny app or something, where the parameters can be wiggled around as a way to explore whether there are interesting things going on.\n\n\nShow the code\n# Try minerva\nmemory &lt;- chord_matrix\n\n# probe using row names\nprobe &lt;- chord_matrix['Dm7',]\n\n# compute similarities between probe and all traces\nsimilarities &lt;- RsemanticLibrarian::cosine_x_to_m(probe,memory)\n\n# tuning function: raise similarities to a power \nactivations &lt;- similarities^3\n\necho &lt;- colSums((memory*c(activations)))\necho &lt;- echo/max(abs(echo))\necho\n\n\n        C        Db         D        Eb         E         F        Gb         G \n0.9918638 0.2479850 1.0000000 0.4178599 0.5037347 0.9766951 0.3491312 0.7242753 \n       Ab         A        Bb         B \n0.3603990 0.9657370 0.4859291 0.4261934 \n\n\nShow the code\n# add echo to probe\nprobe &lt;- echo\n\n# compute similarities between probe and all traces\nsimilarities &lt;- RsemanticLibrarian::cosine_x_to_m(probe,memory)\n\nactivations &lt;- similarities^3\n\nsecond_echo &lt;- colSums((memory*c(activations)))\nsecond_echo &lt;- second_echo/max(abs(second_echo))\nsecond_echo\n\n\n        C        Db         D        Eb         E         F        Gb         G \n0.9946912 0.5014757 1.0000000 0.6381784 0.7237607 0.9389013 0.5636836 0.9371725 \n       Ab         A        Bb         B \n0.5704482 0.9385442 0.7156892 0.6432196 \n\n\nShow the code\n# subtract a weighted portion of second echo\nmore_specific_echo &lt;- echo-(.8*second_echo)\n\necho_similarities &lt;- RsemanticLibrarian::cosine_x_to_m(more_specific_echo,memory)\ndf &lt;- data.frame(chords = row.names(echo_similarities),\n                 similarities = echo_similarities) %&gt;%\n  arrange(desc(echo_similarities))\ndf[1:20,]\n\n\n                               chords similarities\nDm7                               Dm7    0.8386204\nF6                                 F6    0.8386204\nD minor triad           D minor triad    0.7413557\nF major triad           F major triad    0.7368539\nDm7(11)                       Dm7(11)    0.7272550\nD minor pentatonic D minor pentatonic    0.7272550\nF major pentatonic F major pentatonic    0.7272550\nG9sus                           G9sus    0.7272550\nDm9                               Dm9    0.6825942\nBb∆9                             Bb∆9    0.6724193\nD7#11#9                       D7#11#9    0.6640474\nF (add 9)                   F (add 9)    0.6126096\nDm11                             Dm11    0.6022796\nG13sus                         G13sus    0.6022796\nG7sus                           G7sus    0.5976717\nC13sus                         C13sus    0.5929913\nGm11                             Gm11    0.5929913\nF9 (13)                       F9 (13)    0.5880307\nD7sus                           D7sus    0.5869733\nD Blues                       D Blues    0.5853489\n\n\nLots of breadcrumbs here to follow up on later. Ultimately, I didn’t get close to what I was hoping to accomplish. Making a note here that it would be interesting if this type of analysis could provide insight into chord movement from one to another.\n\nSome code for listening to echoes in terms of complex tones made from sine waves, with sine wave amplitude for each note set by the echo intensity for each note.\nneed to explore this\n\n\nShow the code\nlibrary(tuneR)\n# Function to generate a complex tone\ngenerate_complex_tone &lt;- function(duration, sampling_rate, frequencies, amplitudes) {\n  time_points &lt;- seq(0, duration, 1/sampling_rate)\n  complex_tone &lt;- sapply(seq_along(frequencies), function(i) {\n    amplitudes[i] * sin(2 * pi * frequencies[i] * time_points)\n  })\n  \n  return(rowSums(complex_tone))\n}\n\n# Set parameters\nduration &lt;- 5  # seconds\nsampling_rate &lt;- 44100  # Hz (standard audio sampling rate)\nfrequencies &lt;- c(261.63,\n277.18,\n293.66,\n311.13,\n329.63,\n349.23,\n369.99,\n392,\n415.3,\n440,\n466.16,\n493.88)  # frequencies of sine waves in Hz\n\n# Try minerva\nmemory &lt;- chord_matrix\n\n# probe with a C\n# Each of the 12 spots is a note, starting on C\nprobe &lt;- chord_matrix['C note',]\n\n# compute similarities between probe and all traces\nsimilarities &lt;- RsemanticLibrarian::cosine_x_to_m(probe,memory)\n\n# tuning function: raise similarities to a power \nactivations &lt;- similarities^5\n\necho &lt;- colSums((memory*c(activations)))\necho &lt;- echo/max(abs(echo))\n\namplitudes &lt;- echo   # amplitudes of sine waves\n\n# Generate complex tone\ncomplex_tone &lt;- generate_complex_tone(duration, sampling_rate, frequencies, amplitudes)\ncomplex_tone &lt;- complex_tone/max(abs(complex_tone))\ncomplex_tone &lt;- complex_tone*32767\n\n\nwave &lt;- Wave(left = complex_tone, \n             right = complex_tone,\n             samp.rate = sampling_rate, bit = 16)\n#writeWave(wave,\"test.wav\")\n\n\n\nexperiment graveyard of fun ideas\nextruding a subtracted echo from the middle out\n\n\nShow the code\n# Try minerva\nmemory &lt;- chord_matrix\n\n# probe using row names\nprobe &lt;- chord_matrix['C7',]\n\nfor(i in 1:10){\n\n# compute similarities between probe and all traces\nsimilarities &lt;- RsemanticLibrarian::cosine_x_to_m(probe,memory)\n\n# tuning function: raise similarities to a power \nactivations &lt;- similarities^3\n\necho &lt;- colSums((memory*c(activations)))\necho &lt;- echo/max(abs(echo))\n#echo\n\n# add echo to probe\nsubtracted_echo &lt;- echo-probe + rnorm(12,0,.05)\nsubtracted_echo &lt;- subtracted_echo/max(abs(subtracted_echo))\nsubtracted_echo &lt;- subtracted_echo^5\n\necho_similarities &lt;- RsemanticLibrarian::cosine_x_to_m(subtracted_echo, memory)\n\ndf &lt;- data.frame(chords = row.names(echo_similarities),\n                 similarities = echo_similarities) %&gt;%\n  arrange(desc(echo_similarities))\n\nnext_chord &lt;- df$chord[sample(1,1)]\n#probe &lt;- chord_matrix[next_chord,]\nprobe &lt;- subtracted_echo\n\nprint(next_chord)\n\n}\n\n\n[1] \"D note\"\n[1] \"C∆7\"\n[1] \"D note\"\n[1] \"Am9\"\n[1] \"D note\"\n[1] \"B7(#5)\"\n[1] \"Asus\"\n[1] \"A note\"\n[1] \"E note\"\n[1] \"A note\""
  },
  {
    "objectID": "blog/12_1_8_24_Dmajor/index.html",
    "href": "blog/12_1_8_24_Dmajor/index.html",
    "title": "D major practice",
    "section": "",
    "text": "“D major scale. Piano. Circle of fifths. D major 7th chords played on an infinite universe piano. synthesizer keyboard universe.” - dreamshaper-xl-turbo\nContinuing from yesterday’s G major practice I moved onto D major.\nPlayed the D major scale up and down using 7th chords. Did this multiple times with each inversion.\nNoted that I liked the sound of the 3rd inversion the best. Ran up and down enough times that I could play fairly fluently, almost into melody territory. Will likely continue this sort of practice across other keys.\n\nAside…I’ve been messing around with huggingface models as a possible way to generate visual stimuli for some experiments this semester. The class of latent consistency models can generate decent images fairly quickly. This morning I downloaded the “dreamshaper-xl-turbo” model to test it out. Today’s blog picture was generated from this model."
  },
  {
    "objectID": "blog/2_12_23_23_practice/index.html",
    "href": "blog/2_12_23_23_practice/index.html",
    "title": "Practice",
    "section": "",
    "text": "“A piano keyboard shaped like a perfect circle. Very bright and colorful. Lightning bolts in the sky.” - LCM_Dreamshaper_v7\nBack into practicing the piano and working on my chops. It’s been a little bit painful because I am so out of practice. Perhaps there is a new year’s resolution here to practice more. But, what to practice?\nMy recent routine is working through the circle of fifths and reclaiming some basics.\n\nPlaying Maj7 chords anti-clockwise and then clockwise through the circle of fifths\nPlaying II-V-I through the circle of fifths\nPlaying V7-V7 through the circle of fifths\nPlaying I-VI-II-V through the circle of fifths. This is the same progression as in “Heart and Soul”, a duet we’d play as kids. Currently, I’m asking the question whether I can play this song effortlessly (chords in the left hand, melody in the right) in a key as I go around the circle, and if the answer is yes, I shouldn’t practice this key, and if the answer is no, then I should.\n\nStarting to get some fluency back. I was never that diligent at practicing through all of the keys, and some of them need a lot of work.\nIdeally, I’d like to keep a little journal here of practice strategies, reporting on: what I’m doing, what I’m hoping to get out of it (or did get out of it), and other observations that could relate to musical connections and/or connections to the cognitive science of skill learning."
  },
  {
    "objectID": "blog/22_1_22_24_chord_similarity/index.html",
    "href": "blog/22_1_22_24_chord_similarity/index.html",
    "title": "Chord Similarity analysis table",
    "section": "",
    "text": "This is a quick post with some tools for looking at chord similarity.\nI took all of the chords on this music theory page, and represented them in a chord vector space using one-hot codes.\nEach chord is represented as a vector with 12 features, corresponding to each of the 12 possible notes. If a note is in a chord, then the note feature get’s a 1 in the vector. All other features are set to 0.\nHere’s a few examples of what the vectors look like.\nkey\ntype\nitem\nC\nDb\nD\nEb\nE\nF\nGb\nG\nAb\nA\nBb\nB\n\n\n\n\nC\nkey\nC note\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nC\nscale\nC major scale\n1\n0\n1\n0\n1\n1\n0\n1\n0\n1\n0\n1\n\n\nC\ntriads\nC major triad\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n\n\nC\ntriads\nC minor triad\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nC\ntriads\nC6\n1\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\nI created vectors for 47 chords across all keys. This results in a matrix with 564 rows (for each chord vector) and 12 columns for each note feature.\nNext, I used the vector cosine to compute the similarity between each chord and every other chords. That results in a 564x564 chord-similarity matrix.\nIn the matrix, similarity ranges from 0 (no similarity) to 1 (perfect similarity). The similarity function basically tracks how many overlapping features there are between two chords. If each chord has zero shared notes, then similarity will be 0. As a chord increases in the number of shared notes, then similarity increases.\nIf I have extra time I may play around with some additional ways to visualize this matrix. For now, I’m using the datatable library to load the matrix into this webpage. It’s a bit clunky and very large.\nThe table should scroll left and right. The columns can be sorted ascending or descending by clicking the little arrows beside a column name. Sorting from the largest value to the smallest value will re-arrange the row-names on the left side. This produces an ordered list of chords from most to least similar to the column name that was clicked."
  },
  {
    "objectID": "blog/22_1_22_24_chord_similarity/index.html#shiny-app",
    "href": "blog/22_1_22_24_chord_similarity/index.html#shiny-app",
    "title": "Chord Similarity analysis table",
    "section": "Shiny app",
    "text": "Shiny app\nupdate\nThere were originally tables in this post, but they were basically unusable. Ideally, I’d like to search the tables on my ipad while it is sitting on my piano.\nI tried to make a shiny app using webr (that would run without a server), but I couldn’t make it work.\nSo, here is a shiny app that does work:\nhttps://crumplab.shinyapps.io/webr-chord-similarity/"
  },
  {
    "objectID": "blog/22_1_22_24_chord_similarity/index.html#first-order-chord-similarity",
    "href": "blog/22_1_22_24_chord_similarity/index.html#first-order-chord-similarity",
    "title": "Chord Similarity analysis table",
    "section": "First order chord similarity",
    "text": "First order chord similarity\n\n# import excel sheet\nc_chord_excel &lt;- rio::import(\"chord_vectors.xlsx\")\n\n# grab feature vectors\nc_chord_matrix &lt;- as.matrix(c_chord_excel[,4:15])\n\n# assign row names to the third column containing chord names\nrow.names(c_chord_matrix) &lt;- c_chord_excel[,3]\n\n# define all keys\nkeys &lt;- c(\"C\",\"Db\",\"D\",\"Eb\",\"E\",\"F\",\"Gb\",\"G\",\"Ab\",\"A\",\"Bb\",\"B\")\n\n\n# the excel sheet only has chords in C\n# loop through the keys, permute the matrix to get the chords in the next key\n# add the permuted matrix to new rows in the overall chord_matrix\nfor (i in 1:length(keys)) {\n  \n  if (i == 1) {\n    # initialize chord_matrix with C matrix\n    chord_matrix &lt;- c_chord_matrix\n    \n  } else {\n    #permute the matrix as a function of iterator\n    new_matrix &lt;- cbind(c_chord_matrix[, (14-i):12],c_chord_matrix[, 1:(13-i)] )\n    \n    # rename the rows with the new key\n    new_names &lt;- gsub(\"C\", keys[i], c_chord_excel[,3])\n    row.names(new_matrix) &lt;- new_names\n    \n    # append the new_matrix to chord_matrix\n    chord_matrix &lt;- rbind(chord_matrix,new_matrix)\n  \n    }\n}\n\n# calculate similarities between keys\nkey_similarities &lt;- lsa::cosine(chord_matrix)\n\n# calculate similarities between keys\nchord_similarities &lt;- lsa::cosine(t(chord_matrix))\n\nchord_properties &lt;- tibble(\n  type = rep(c_chord_excel$type,length(keys)),\n  key = rep(keys, each = dim(c_chord_matrix)[1]),\n  chord_names  = row.names(chord_similarities)\n)\n\nchord_similarities &lt;- cbind(chord_properties,\n                            chord_similarities)\nrow.names(chord_similarities) &lt;- NULL\n\n# print table\n# try datatable options...didn't work as I wanted\n# DT::datatable(\n#   chord_similarities[1:50,1:50],\n#   extensions = c(\"FixedColumns\",\"SearchBuilder\",\"Buttons\"),\n#   options = list(\n#     paging = TRUE,\n#     pageLength =  25,\n#     fixedColumns = list(leftColumns = 4),\n#     dom = \"BQlfrtip\",\n#     searchBuilder = TRUE,\n#     search = list(return = TRUE),\n#     buttons = 'colvis'\n#   )\n# )\n\nDT::datatable(\n  chord_similarities,\n  extensions = \"FixedColumns\",\n  options = list(\n    paging = TRUE,\n    pageLength =  25,\n    fixedColumns = list(leftColumns = 4)\n  )\n)"
  },
  {
    "objectID": "blog/22_1_22_24_chord_similarity/index.html#second-order-similarities",
    "href": "blog/22_1_22_24_chord_similarity/index.html#second-order-similarities",
    "title": "Chord Similarity analysis table",
    "section": "Second-order similarities",
    "text": "Second-order similarities\nupdate: this table has been moved to the shiny app\nThis table uses the above cosine similarity vectors for each chord as the basis vectors, and then computes another similarity matrix. The approach has interesting properties in other domains like computational modeling of word semantics. Just curious to look at it in this context.\nOne basic observation is that in the first-order similarity space (above), there are many 0 similarities between pairs. For example, the single note C has 0 similarity to every other single note, because each note only has one feature, and the features never overlap. However, in the second-order matrix, individual notes now have some positive similarity to each other. This is because the whole vector of similarities for C (from above), now has many shared elements with the vectors for the other chords, and a similarity can be computed.\nI suspect this table will have some more musically interesting things going on.\n\nfirst_order &lt;- lsa::cosine(t(chord_matrix))\nsecond_order &lt;- lsa::cosine(first_order)\n\nsecond_order_similarities &lt;- cbind(chord_properties,\n                                   second_order)\nrow.names(second_order_similarities) &lt;- NULL\n\nDT::datatable(\n  second_order_similarities,\n  extensions = \"FixedColumns\",\n  options = list(\n    paging = TRUE,\n    pageLength =  25,\n    fixedColumns = list(leftColumns = 4)\n  )\n)"
  },
  {
    "objectID": "blog/5_12_25_23_7thFirst/index.html",
    "href": "blog/5_12_25_23_7thFirst/index.html",
    "title": "7th first II-V-I",
    "section": "",
    "text": "I’ve been taking practice inspiration from Mark Levine’s Jazz Theory Book. This exercise wasn’t in there specifically, but it’s in the spirit of root-bias deprogramming.\n\n\n\nLevine has these fun quips throughout that I’m trying to follow through on:\n\nRemember, practice all scales and patterns both ascending and descending; on the major, melodic minor, diminished, and whole-tone scales and in all keys\n\n\nYou’ve now learned about all four of the scales you’ll need under your fingers to play over chord changes. The next question is how to practice them. And you don’t just want to practice them, but to internalize them to the point where they become an available pool of notes, on which to improvise.\n\n\n…the goal is to deprogram yourself from years of root-bias conditioning\n\nI learned piano in the classical music tradition and practiced all my scales and chords in a massively root-biased way when I was a kid. So, one of my goals right now is to think about chords from non-root perspectives.\nFor example, in this exercise my goal is to practice leading with the 7th note. This is related to the exercise from yesterday, but I did few things differently. Here’s the idea:\n\n\nPlay II-V-I chords through the circle of fifths\nFor each chord, lead with the 7th note, then bounce between the 7th note and each inversion of the 7th chord.\n\nIn the first bar I have a D-7 chord in the key of C major. The minor 7th note is a C, and instead of thinking C = C major, I’m trying to also think C = D-7 . And, by doing this around the circle of fifths, you end up practicing every kind of 7th chord (maj, dom, min) for every key.\nThis exercise implies some related ones that I’ll get to some other day, such as leading with the 5th, or leading with the 3rd…and then leading with the weirder stuff (sus, b5, b9, #11)."
  },
  {
    "objectID": "blog/19_1_19_24_Contextual_Interference/index.html",
    "href": "blog/19_1_19_24_Contextual_Interference/index.html",
    "title": "Estimating practice time needed, and a bit on contextual interference",
    "section": "",
    "text": "I’ve been having so much fun making time for more music. I so needed this.\nAs my next semester at Brooklyn College is about to begin next week, I’m hoping I can keep up some of this momentum. Coincidentally, this semester I am planning to turn some of my research program in cognitive psychology towards aspects of music cognition, and I ended up posting about that a little bit on my main research blog yesterday. The research won’t necessarily mean playing more music, but it should at least keep me on my fingers, so to speak.\nToday’s post continues in the spirit of intermingling my interests in cognition and piano practice."
  },
  {
    "objectID": "blog/19_1_19_24_Contextual_Interference/index.html#recap",
    "href": "blog/19_1_19_24_Contextual_Interference/index.html#recap",
    "title": "Estimating practice time needed, and a bit on contextual interference",
    "section": "Recap",
    "text": "Recap\nOver the past few posts I’ve been focusing on getting command of the maj6 diminished scale in the key of B. I measured my improvement with practice and found a few things were happening. When I sit down to play the scale (I’m playing it as a series of chords ascending), there is a big start-up cost. On the first go round it took me 104 seconds to play the scale. Then a day later only about 60 seconds. Last night I started up at about 20 seconds. For each practice set, I also continued to produce the scale for 10 minutes, by playing it over and over, in what I might term massed repetitive practice. Within each practice set, my scale completion times decreased with each attempt. I usually got down to about 10 seconds to complete the scale. Still pretty slow, but making progress.\nI can play the 7th chords of a regular major scale in C in about 3 seconds, from a cold start. So, I’m using 3 seconds as a rough achievement criterion. If I can get my B maj6 diminished scale to 3 seconds, then I’ll have achieved that somewhat arbitrary criterion. And, then I would literally be confident that I could play it in my sleep. I was trying to play it going to sleep in my head last night, and it is still slow to visualize.\nI am definitely hitting a floor right now at about 10 seconds, and there could be several problems that I need to sort out. I don’t often play in B, and it might be one of my weakest keys over all. Sometimes I get confused about whether I should be playing an E or F (yikes). The maj6 diminished scales adds a b6, and sometimes I get confused about that note (G). I don’t know why I sometimes want to play A. I conceptually know the correct chords and fingerings, but can’t execute everytime, especially with speed. So, in cognitive psychology terms, I am running up against the speed-accuracy tradeoff. I can go slow and accurate, but errors creep in approaching 10 seconds, and then they really popoff if I push past faster.\nIf I limit my musical goals to getting this one scale down, then I have so many questions about how to go about doing that. Overall, it is clear that I need to practice, practice, practice, and I will meet my fluency criterion eventually. But, there are lots of things to practice. I could practice the chords ascending only, which is how I’m measuring my performance. I could practice up and down, or arpeggios, and all sorts of other stuff. I could practice other scales. I could practice in lentghy repetitive sessions, or takes lots of breaks and practice every other day, or longer. Infinite practice variations here.\nAlmost done with the recap. All of the above is about to connect to a phenomena in cognitive psychology called contextual interference. This phenomena shows that repeatedly practicing a motor sequence versus alternating practice between different motor sequences, can have notable consequences for learning, performance, and retention. And, I’d like to explore that with my own practice this morning.\nBut, because I am full of tangents, I need to attempt an answer to a burning question first.\nHow many practice attempts do I need to get my B maj6 diminished scale under 3 seconds?"
  },
  {
    "objectID": "blog/19_1_19_24_Contextual_Interference/index.html#estimating-how-much-practice-i-need-with-my-own-practice-curve",
    "href": "blog/19_1_19_24_Contextual_Interference/index.html#estimating-how-much-practice-i-need-with-my-own-practice-curve",
    "title": "Estimating practice time needed, and a bit on contextual interference",
    "section": "Estimating how much practice I need with my own practice curve",
    "text": "Estimating how much practice I need with my own practice curve\nThis graph shows my learning curves for three different sets of practice attempts. I start producing the scale slowly, and then as I repeatedly attempt the scale, I pick up speed until I get down to about 10 seconds.\n\n\n\n\n\n\n\n\n\nWhat I can do next is take an average of these three learning curves, which looks like this:\n\n\n\n\n\n\n\n\n\nThere is debate in the cognitive science literature about the functional form of this curve. Maybe it’s a power function, maybe it’s an exponential function. They are both pretty similar looking. Here’s what a power function fit looks like:\n\n\n\nCall:\nlm(formula = log(mean_ft) ~ log(sets), data = df_avg)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.46986 -0.09867  0.01724  0.16628  0.31374 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.49031    0.10817   41.51   &lt;2e-16 ***\nlog(sets)   -0.63201    0.03874  -16.32   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1983 on 34 degrees of freedom\nMultiple R-squared:  0.8867,    Adjusted R-squared:  0.8834 \nF-statistic: 266.2 on 1 and 34 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\n\nLooks pretty good to me, the line is going right through dots. So, the line can represent a kind of average practice curve. The neat thing is that I can keep drawing the line out following the power function. This allows me to use my existing data to make a prediction about how many practice attempts I need to get down to 3 seconds.\n\n\n\n\n\n\n\n\n\nSo, according to the limited data that I collected, I should be able to get down to 3 seconds in 219 practice attempts. If I add up all of the scale completion times from 1 to 219 (that are implied by this curve), then it will take 1568 seconds to get there, or 26 minutes of repeated practice.\nWow, that surprises me actually, I thought it would take longer. And, this is only an estimate, it could be way off in either direction. For example, I’ve already practiced a total of 30 minutes, over 3 sets of 10 minutes. Maybe I should have tried 26 minutes the first time and just got it over with!\nTBH I kind of want to go try this for 26 minutes as see what happens. Maybe I will set that aside as something I do later and report back. Instead, I will continue with the goal of this post."
  },
  {
    "objectID": "blog/19_1_19_24_Contextual_Interference/index.html#contextual-interference-and-mixed-vs-random-practice",
    "href": "blog/19_1_19_24_Contextual_Interference/index.html#contextual-interference-and-mixed-vs-random-practice",
    "title": "Estimating practice time needed, and a bit on contextual interference",
    "section": "Contextual interference and mixed vs random practice",
    "text": "Contextual interference and mixed vs random practice\n\nShea, J. B., & Morgan, R. L. (1979). Contextual interference effects on the acquisition, retention, and transfer of a motor skill. Journal of Experimental psychology: Human Learning and memory, 5(2), 179.\n\nThe above paper showed a curious effect possibly relevant to practicing music.\n\n\n\nShea & Morgan had people practice sequences of motor movements on an unfamiliar device. The device is shown in the margin.\nOn each practice trial, one of the stimulus lights would turn on. The light indicated one of three movement sequences to perform. For example, one of the movement sequences was to grab the tennis ball in the middle, then strike down the gates in a particular order:\n\nright rear, left middle, right front\nright front, left middle, right rear\nleft front, right middle, left rear\n\nThis is abstractly similar to learning to play a series of notes on a strange instrument.\nThe important manipulation in their experiment involved whether or not people repeatedly practiced a sequence, or had to randomly alternate between sequences.\nOne group received “blocked” practice training. They got practice on each of the three sequences. Each sequence was performed 18 times in a row. So, each sequence was grouped into a “block” of 18 practice attempts. This group got to practice a sequence repeatedly until they ran out of attempts, and then they moved onto the next sequence and practiced it on repeat.\nThe other group received “random” practice training. They practiced each sequence 18 times overall, but the order was totally randomized by the stimulus lights. This involves many more switches between sequences during practice. Let’s take a look at the data from the study:\n\nThe y-axis shows total time in seconds, which measures how fast people were able to produce a sequence. In the acquisition phase (when they were practicing the sequences), both groups got faster with practice. The random group started out the slowest, and made large gains with practice, but they didn’t end up as fast as the blocked group. The blocked group started out pretty fast, made smaller gains with practice, but ended up the fastest.\n+1 for blocked practice right?\nNot so fast. Shea & Morgan also included a retention test. Some subjects waited 10 minutes after the acquisition before attempting the sequences again, others waited 10 whole days before producing the sequence again.\nThe retention graph has several things going on here. I should also mention that the type retention test was counterbalanced. Some participants who received blocked training also received a blocked version of the retention test, but others received a random version of the retention test; and vice versa for the random group.\nThe basic pattern shown in the retention graph is that participants who received random practice retained the more than the participants who received blocked practice. This seems almost paradoxical. The blocked practice group “learned the most”, or “performed the best” during acquisition, but they also forgot the most when trying to redo the sequences later on. The random group appeared to have inferior learning, but they retained what they learned much better than the blocked group, and they even performed better on the retention test than the best scores from the block group.\nAre there implications of this result for music training? A quick search on google scholar shows several papers looking at blocked vs random practice schedules in music learning contexts. I haven’t had time to read them yet, but I assume there are positive findings showing some influence of blocking or randomly varying practice on learning musical skills. In general, this phenomena has been demonstrated across a fairly wide range of tasks, so I would expect it to generalize to motor skills on piano.\n\nI need to interrupt this post. I had ambitions to try blocked vs random practice myself, and I was going to do it. But, then I thought more about how I would do it so that I can measure my performance times, as well as how would I randomly tell myself what scales to play…and, then I realized I had to do something else first.\nBegin the process of building computational assessment methods. Should be fun. This semester I have some plans to get more serious about this using MIDI and jspsych.\nFor now, I’m going to try something as simple as possible. The basic idea is to be able to have my ipad present a target stimulus for practice, like “A major triad”, or “c major scale”, or “C7 chord”, or whatever I want. I’ll set my ipad up on the piano, wait for the stimulus, and the try to play it as fast as possible. I’ll also add a little button on the screen to press when I finish playing the target stimulus. This will be really hacky from a cogsci perspective, but I’ll improve the precision later. For now, this will work to collect some rough personal data on my progress learning the maj6 diminished scale.\nTime to roll up the javascript programming sleeves."
  },
  {
    "objectID": "blog/6_12_26_23_rootless9th/index.html",
    "href": "blog/6_12_26_23_rootless9th/index.html",
    "title": "Rootless 9th chords",
    "section": "",
    "text": "“An underground lair of piano playing rabbits. Above the rabbits are the roots of carrots that have been eaten by rabbits as they play piano.” - LCM_Dreamshaper_v7\nToday’s exercise was running I∆7add9 chords through the circle of fifths.\nThe first row shows the notes of the C major scale with corresponding numbers. The 9th note is the same as the second note in the scale.\nThe second row shows the concept of constructing chords from thirds. From this perspective, you get all of the notes in the scale going up by thirds, but the 9th, 11th, and 13th note are out of their normal order.\nThis exercise focuses on the “rootless” 9th chord on the bottom row. That one has a major 3rd, a perfect fifth, a major 7th, and a 9th.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n\nC\nD\nE\nF\nG\nA\nB\nC\nD\nE\nF\nG\nA\nB\n\n\nC\n\nE\n\nG\n\nB\n\nD\n\nF\n\nA\n\n\n\n\n\nE\n\nG\n\nB\n\nD\n\n\n\n\n\n\n\n\nIn the exercise I played these chords clockwise and anti-clockwise through the circle of fifths. I usually used my left hand for the bass note and to remind myself where I was in the circle. Then I played versions of the 9th chord. They all have a smashing pumpkins kind of vibe.\nOne reason I chose this exercise is to press harder on deprogramming my root conditioning. These chords don’t have the root in them, so they must be good for that (e.g., thinking about doing something in C, but not playing C, or only playing around C).\nAnother musical connection is that these 9th chords are the same as a III-7 chord. So, an E minor 7th is a C major 7th add 9.\nCognitive science connections: As a sidenote, my day job is a cognitive psychology professor who studies skill-learning. I’ve been meaning to do some piano skill research and I even have the keyboards in my lab to do it. So, as I go through these exercises it’s been fun thinking about issues like how does training on some musical ideas transfer to others.\nFor example, I’m already fairly fluid at playing the circle of fifths in different ways. If I’m playing single notes, I can go through the circle very quickly. It’s slower for chords, but still fairly quick. I’ve been practicing my 7th chords through the circle, so I can already go through the circle playing minor 7th chords fairly quickly. However, it turns out I can not do these 9th chords quickly. There should be pretty good transfer here because the 9th chords are exactly the same as the minor 7th chords that I have already practiced.\nIn terms of minor 7th chords I had been practicing these chords, with the root in the bass.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\nE-7\nB-7\nGb-7\nDb-7\nAb-7\nEb-7\nBb-7\nF-7\nC-7\nG-7\nD-7\nA-7\n\n\nE\nB\nGb\nDb\nAb\nEb\nBb\nF\nC\nG\nD\nA\n\n\n\nBut, I had not been practicing those chords with the following bass notes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\nE-7\nB-7\nGb-7\nDb-7\nAb-7\nEb-7\nBb-7\nF-7\nC-7\nG-7\nD-7\nA-7\n\n\nC\nG\nD\nA\nE\nB\nGb\nDb\nAb\nEb\nBb\nF\n\n\n\nGeneralization is so fickle sometimes.\nAlso, I have some super-biased thirds. What’s the third of C -&gt; E. No problem, that’s fast. A -&gt; C, G -&gt; B. But, B ———&gt; Eb. That one hurts. Eb seems too far away to be a third. In any case. I need to flatten out this distribution at some point."
  },
  {
    "objectID": "blog/14_1_12_24_osmose/index.html",
    "href": "blog/14_1_12_24_osmose/index.html",
    "title": "The VERY Expressive Osmose",
    "section": "",
    "text": "I’ve been really curious about the Expressive Osmose keyboard for a while now. So, I picked one up yesterday.\nBasically all I have to say is: HOLY WOW.\nI might have more to say later, and I may add notes to this post about the osmose as I go through the 7 stages of osmose ownership.\nFor, now I need to get back to pitch bending every note into a tornado of meowing cats.\nEarly afternoon update. I have been noodling away roughly alphabetically through the 500 presets. I’m just starting the ones that begin with C. It’s easy to get lost in fun for hours doing this. Not that I want to take the fun away, but I should start integrating this a little bit more into my setup. Probably the next step will be recording the noodles.\nHere are some notes so far on playing the osmose. These are notes to self to help me progress through this thing."
  },
  {
    "objectID": "blog/14_1_12_24_osmose/index.html#velocity-and-pressure",
    "href": "blog/14_1_12_24_osmose/index.html#velocity-and-pressure",
    "title": "The VERY Expressive Osmose",
    "section": "Velocity and pressure",
    "text": "Velocity and pressure\nThe osmose has several dimensions of expressive control. Like most standard midi keyboards it has note velocity, but it also has pressure, aftertouch, and pitch bend for each key independently, with 24 voice polyphony!\n\n\n\nA digression into my past life of utterly not liking midi keyboards. I grew up playing a Roland 5600 with a MC-300 (or 500?) and MT-32 (where much was had with the orchestra hit sound). I would go to my piano teacher’s houses for lessons on a real piano, and practice on the midi piano at home. It was fun to make different sounds on the roland, but I always wanted a real piano as a kid.\nI still love the action of a real piano. It turns out I also love the action of my rhodes 88 key electric piano. But, I have never loved the action of any midi keyboard that I have ever owned or played. Not the hammer action ones. Not any of them.\nIn 2019 I got to play a CS-80 one time in Melbourne and I could not believe how much a difference polyphonic aftertouch made. It’s so sad that midi keyboards suck so much for so long.\nMaybe I didn’t spend enough time with the other midi keyboards that I didn’t like. Most recently I brought home a Nord 88 key stage piano to use a controller, and it was just ok…one key broke, and there is an uncurable noisy power supply. Plus, these things never feel like an instrument to me, so they just constantly suck joy.\nI have tried to adjust velocity curves and find something that seems better, but never got there. It’s possible I just don’t like how velocity is usually implemented, or that velocity is missing something for me. In any case, when I play on my acoustic or electric piano, even though both keyboards have very different action, they are both very playable and immediately expressive. I don’t have think about it, the expression is just there.\nThis is what I now adore about the osmose. The expression is right there at my finger tips.\nI really like that they have pressure per key too."
  },
  {
    "objectID": "blog/14_1_12_24_osmose/index.html#squishy-goodness",
    "href": "blog/14_1_12_24_osmose/index.html#squishy-goodness",
    "title": "The VERY Expressive Osmose",
    "section": "Squishy goodness",
    "text": "Squishy goodness\nIt’s hard to describe what the action feels like. There is some initial resistance to the keys, but not too much. This is great for light and/or bouncy tapping and slowly applying pressure to the keys. Many of the presets make sound from pressure alone, and sometimes different sounds from pressure or velocity.\nThe aftertouch is the best feeling aftertouch I’ve ever played. There is a slight difference in resistance when you get to aftertouch land, and way more travel down into the aftertouch than on typical midi keyboards. It feels like the key keeps going down into it’s aftertouch home, where it belongs sometimes.\nSimilarly, the key’s pick up side-to-side wiggling throughout the travel of the key and into aftertouch range. This parameter is usually applied to pitch bend. It get’s a bit easier to wiggle when the key is pressed down a bit, but over all it’s very natural."
  },
  {
    "objectID": "blog/14_1_12_24_osmose/index.html#presets",
    "href": "blog/14_1_12_24_osmose/index.html#presets",
    "title": "The VERY Expressive Osmose",
    "section": "Presets",
    "text": "Presets\nLots of juicy stuff in the presets, all powered by EaganMatrix. It’ll be a while before I go into that thing.\nThere are a few onboard effects like a reverb and delays. The reverb can sound pretty good, especially with extend “on”.\nI don’t like menu diving, and it’s pretty fast to make changes to sounds and to sensitivity parameters of the controller. Very happy with that.\nNeed to go play it some more."
  },
  {
    "objectID": "blog/14_1_12_24_osmose/index.html#mpe-controller",
    "href": "blog/14_1_12_24_osmose/index.html#mpe-controller",
    "title": "The VERY Expressive Osmose",
    "section": "MPE controller",
    "text": "MPE controller\nI’ve got it hooked up to the OB-6 and it works very well. Whenever I need the sound to go to 11, I just back it up with some OB-6. The more the scarier."
  },
  {
    "objectID": "blog/14_1_12_24_osmose/index.html#recording-midi-in-ableton-live",
    "href": "blog/14_1_12_24_osmose/index.html#recording-midi-in-ableton-live",
    "title": "The VERY Expressive Osmose",
    "section": "Recording midi in ableton live",
    "text": "Recording midi in ableton live\nI was recording for a bit with one midi track and one audio track. Things seemed to be going fine, but Ableton started freezing up once in a while, and the midi recordings were sometimes really off. The likely culprit is me trying to record MPE midi for the first time.\nIt seems there are some MPE midi settings that are important, like from this video\nStill crashing ableton a bunch. A problem for another day."
  },
  {
    "objectID": "blog/15_1_13_24_modes/index.html",
    "href": "blog/15_1_13_24_modes/index.html",
    "title": "Modes…sigh",
    "section": "",
    "text": "I’m not friends with modes yet. Barely an acquaintance, even after all these years.\nToday’s practice is focusing on modes, yet again. This morning I am thinking of modes like a Donald Rumsfeld quote. Modes for me are known knowns, known unknowns, and unknown unknowns."
  },
  {
    "objectID": "blog/15_1_13_24_modes/index.html#modes-as-known-knowns",
    "href": "blog/15_1_13_24_modes/index.html#modes-as-known-knowns",
    "title": "Modes…sigh",
    "section": "Modes as known knowns",
    "text": "Modes as known knowns\nI’m not sure when I first learned about modes, but probably some time in middle school. Since then I have– in a definitional sense– known what they were.\nThe way I encoded them as a piano player was in terms of the C major scale. There are 7 modes of the major scale, corresponding to each key of the major scale. Each mode involves playing the major scale from a different root note. This got turned into a heuristic like, play all the white keys from C to C…then play them from D to D, E to E, and so on. Those are the modes. That’s it. Also, the major scale modes have these Greek names:\n\n\n\nMode Name\nStarting Note\nNotes in C Major\n\n\n\n\nIonian\nI\nCDEFGABC\n\n\nDorian\nII\nDEFGACBD\n\n\nPhrygian\nIII\nEFGABCDE\n\n\nLydian\nIV\nFGABCDEF\n\n\nMixolydian\nV\nGABCDEFG\n\n\nAeolian\nVI\nABCDEFGA\n\n\nLocrian\nVII\nBCDEFGAB\n\n\n\nThe mode concept is a descriptive one. Other scales have can be described as having modes using the same principles as above.\nThat’s about as far the known knowns go for me. I don’t really think in terms of modes when I play. Also, that’s why I’m working on thinking in terms of modes again. For the extra flavor and different perspective that they bring."
  },
  {
    "objectID": "blog/15_1_13_24_modes/index.html#modes-as-known-unknowns",
    "href": "blog/15_1_13_24_modes/index.html#modes-as-known-unknowns",
    "title": "Modes…sigh",
    "section": "Modes as known unknowns",
    "text": "Modes as known unknowns\nThere’s lots of other stuff that I in some sense “know” about modes. But, in practice, that conceptual knowledge is not well integrated into my playing.\nThe best example is that I never really bothered to automatize the modes across all of the keys. Oops. Here again is a strange failure of generalization.\nOn the one hand, I have practiced all of the modes across all of the keys. For example, over the past month I’ve been doing a lot of major scale work going up and down the major scale in every key, in every mode. I can play them all reasonably quickly now. So, that’s great. I must know my modes right? Not really.\nOn the other hand, I wasn’t thinking about modes, mode names, or modal relationships as I was playing through those scales. So, for example, I knew it would be hard to play C scales in the different modes. Of course I know C ionian. But, what is C dorian? Mind goes blank. Then slowly, “yes, C is the II of Bb”, so then C dorian is, “play a Bb major scale starting from C.” Then, what is the third mode even? Can’t remember…oh yes, Phrygian. What is C Phrygian? ummm….blah.\nOf course, I can play an Ab major scale, but thinking of C Phygrian as Ab is one of those known unknowns. It’s the kind of missing connections I know are there, but haven’t bothered to glue together.\nMore generally, I’m hoping that working through modes will help me firm up these kinds of relations between notes so that they are within striking distance of my finger tips. For example:\n\nC as a I (Ionian) in C\nC as a II (dorian) in Bb\nC as a III (phrygian) in Ab\nC as a IV (Lydian) in G\nC as a V (Mixolydian) in F\nC as a VI (Aeolian) in Eb\nC as a VII (Locrian) in Db\n\nAnd, then all of those same relationships for every note."
  },
  {
    "objectID": "blog/15_1_13_24_modes/index.html#modes-and-unknown-unknowns",
    "href": "blog/15_1_13_24_modes/index.html#modes-and-unknown-unknowns",
    "title": "Modes…sigh",
    "section": "Modes and unknown unknowns",
    "text": "Modes and unknown unknowns\nI suppose this is the stuff I’ll discover later on when I know my modes better.\nOne example from this morning was noticing patterns of harmonic ambiguity that are mildly interesting. For example, let’s say you play a C and G, which could be described as a perfect fifth in the key of C. What else could it be? Those notes are in the C major scale, so they could imply C Ionian. Those notes are also in the Bb major scale (the II and VI), so they could imply Bb as the tonal center, and in that case playing notes from C dorian would help fill in that implication. The same goes for all of the other modes of C (except for locrian where the G is diminished). And this is a fun little toy to play with, like a jack-in-the-box. Playing Cs and Gs is like turning the gear of the box, building expectations about what kind of jack will pop up, and then OMG, it’s a locrian scale."
  },
  {
    "objectID": "blog/15_1_13_24_modes/index.html#modes-by-accident",
    "href": "blog/15_1_13_24_modes/index.html#modes-by-accident",
    "title": "Modes…sigh",
    "section": "Modes by accident",
    "text": "Modes by accident\nI still can’t remember the names of modes. And, I haven’t encoded them in terms of their accidental values. So, that’s why I am making this table. These are the different modes of C.\n\n\n\n\n\n\n\n\n\nMode Name\nAccidental\nMajor scale\nC scale\n\n\n\n\nIonian I\nnone\nC\nC D E F G A B C\n\n\nDorian II\nbIII, bVII\nBb (C is -ii)\nC D Eb F G A Bb C\n\n\nPhrygian III\nbII, bIII, bVI, bVII\nAb (C is -iii)\nC Db Eb F G Ab Bb C\n\n\nLydian IV\n#IV\nG (C is iv)\nC D E F# G A B C\n\n\nMixolydian V\nbVII\nF (C is v)\nC D E F G A Bb C\n\n\nAeolian VI\nbIII, bVI, bVII\nEb (C is -vi)\nC D Eb F G Ab Bb C\n\n\nLocrian VII\nbII, bIII, bV, bVI, bVII\nDb (C is vii)\nC Db Eb F Gb Ab Bb C\n\n\n\nSo, a new goal will be to play the modes in each key from this point of view.\nThe I (ionian), IV (lydian), and V (mixolydian) modes are “major” flavor, and the II (dorian), III (phrygian), IV (aeolian), and VII (locrian) modes are “minor” flavor.\nI’m going to re-order the modes in terms of the circle of fifths.\n\n\n\n\n\n\n\n\n\nMode Name\nAccidental\nMajor scale\nC scale\n\n\n\n\nLydian IV\n#IV\nG (C is iv)\nC D E F# G A B C\n\n\nIonian I\nnone\nC\nC D E F G A B C\n\n\nMixolydian V\nbVII\nF (C is v)\nC D E F G A Bb C\n\n\nDorian II\nbIII, bVII\nBb (C is -ii)\nC D Eb F G A Bb C\n\n\nAeolian VI\nbIII, bVI, bVII\nEb (C is -vi)\nC D Eb F G Ab Bb C\n\n\nPhrygian III\nbII, bIII, bVI, bVII\nAb (C is -iii)\nC Db Eb F G Ab Bb C\n\n\nLocrian VII\nbII, bIII, bV, bVI, bVII\nDb (C is vii)\nC Db Eb F Gb Ab Bb C\n\n\n\nFrom this point of view there are five increasingly “minor” modes, going from Mixolydian to Locrian. Thinking in terms of what spices are added to the C major, the mixolydian adds a Bb. If you play a C and the dominant 7th (Bb), then there is a kind of question…how many more minor spices are there going to be? When I’m cooking a minor chord, there’s lots of spices to be added. There’s less major spice options, but throw in the tri-tone once in a while I guess.\nMore notes…The same table as above, using arabic numerals, and re-ordering in terms how the flats are added in the circle of fifths.\n\n\n\n\n\n\n\n\n\nMode Name\nAccidental\nMajor scale\nC scale\n\n\n\n\nLydian IV\n#4\nG (C is iv)\nC D E F# G A B C\n\n\nIonian I\nnone\nC\nC D E F G A B C\n\n\nMixolydian V\nb7\nF (C is v)\nC D E F G A Bb C\n\n\nDorian II\nb7, b3\nBb (C is -ii)\nC D Eb F G A Bb C\n\n\nAeolian VI\nb7, b3, b6\nEb (C is -vi)\nC D Eb F G Ab Bb C\n\n\nPhrygian III\nb7, b3, b6, b2\nAb (C is -iii)\nC Db Eb F G Ab Bb C\n\n\nLocrian VII\nb7, b3, b6, b2, b5\nDb (C is vii)\nC Db Eb F Gb Ab Bb C"
  },
  {
    "objectID": "blog/15_1_13_24_modes/index.html#modes-as-slashpolychords",
    "href": "blog/15_1_13_24_modes/index.html#modes-as-slashpolychords",
    "title": "Modes…sigh",
    "section": "Modes as slash/polychords",
    "text": "Modes as slash/polychords\nI’ve seen different definitions of slash and polychords. One definition of a slash chord (say D/F#) is to play the numerator triad (D) with the denominator bass note (F#). And, a polychord would be playing one triad with the right hand (D) and another with the left hand (F#).\nAnother way to think of playing modes is in terms of slash chords or poly chords; or, I guess slash scales or poly scales. For example, consider a G/C. That could indicate a G major scale over a C bass note. Playing the G major scale starting and ending on C is the same as C lydian.\nIn the next table the note in the denominator is the root. The note in the numerator is the major scale to be played over the root (starting and ending on the root). The roman numerals show interval relations between the root and scale. For example, in the lydian column, the root is the fourth note in the corresponding scale (C is the fourth of G, G is the fourth of C). Modifying the major scale in the denominator by the accidental yields the major scale in the numerator (For Bb/C, flattening the 37 in C is B-&gt; Bb and E -&gt; Eb, which then turns the C scale into a Bb scale). The table is arranged in terms of the circle of fifths.\n\n\n\n\n\n\n\n\n\n\n\n\nLydian\nIonian\nMixolydian\nDorian\nAeolian\nPhrygian\nLocrian\n\n\n\n\nMajory\nMajor\nMajorish\nMinor\neven\nmore\nMINOR\n\n\nIV\nI\nV\nII\nVI\nIII\nVII\n\n\n#4\n–\nb7\nb73\nb736\nb7362\nb73625\n\n\nG/C\nC/C\nF/C\nBb/C\nEb/C\nAb/C\nDb/C\n\n\nC/F\nF/F\nBb/F\nEb/F\nAb/F\nDb/F\nGb/F\n\n\nF/Bb\nBb/Bb\nEb/Bb\nAb/Bb\nDb/Bb\nGb/Bb\nB/Bb\n\n\nBb/Eb\nEb/Eb\nAb/Eb\nDb/Eb\nGb/Eb\nB/Eb\nE/Eb\n\n\nEb/Ab\nAb/Ab\nDb/Ab\nGb/Ab\nB/Ab\nE/Ab\nA/Ab\n\n\nAb/Db\nDb/Db\nGb/Db\nB/Db\nE/Db\nA/Db\nD/Db\n\n\nDb/Gb\nGb/Gb\nB/Gb\nE/Gb\nA/Gb\nD/Gb\nG/Gb\n\n\nGb/B\nB/B\nE/B\nA/B\nD/B\nG/B\nC/B\n\n\nB/E\nE/E\nA/E\nD/E\nG/E\nC/E\nF/E\n\n\nE/A\nA/A\nD/A\nG/A\nC/A\nF/A\nBb/A\n\n\nA/D\nD/D\nG/D\nC/D\nF/D\nBb/D\nEb/D\n\n\nD/G\nG/G\nC/G\nF/G\nBb/G\nEb/G\nAb/G\n\n\n\nThinking in terms of the circle of fifths. Playing a slash scale one fifth above the root is Lydian. Playing a scale in the root is Ionian. Playing slash scales down the circle of fifths (anti-clockwise) is Mixolydian, Dorian, Aeolian, Phrygian, Locrian."
  },
  {
    "objectID": "blog/15_1_13_24_modes/index.html#playing-the-sound-of-the-modes",
    "href": "blog/15_1_13_24_modes/index.html#playing-the-sound-of-the-modes",
    "title": "Modes…sigh",
    "section": "Playing the sound of the modes",
    "text": "Playing the sound of the modes\nNote to self to just get into it."
  },
  {
    "objectID": "blog/17_1_17_24_BarryHarris/index.html",
    "href": "blog/17_1_17_24_BarryHarris/index.html",
    "title": "Practicing some Barry Harris methods",
    "section": "",
    "text": "A couple of days ago I stumbled across the Barry Harris system as taught by one of his students Shan Verma, who also runs https://www.jazzskills.com.\nI’ve been randomly watching a few videos from his youtube channel and trying a few things out.\nAll of the practice is around learning the major 6th diminished scale, which has some very interesting properties, and sounds great.\nThese two videos show the two primary ideas for today’s practice."
  },
  {
    "objectID": "blog/17_1_17_24_BarryHarris/index.html#what-i-did-so-far",
    "href": "blog/17_1_17_24_BarryHarris/index.html#what-i-did-so-far",
    "title": "Practicing some Barry Harris methods",
    "section": "What I did so far",
    "text": "What I did so far\nWoke up and played the scale using alternating maj6 and dim7 chords. Played through C, F, and Bb. Also, ran through the building up notes exercise in each key, starting from each note in the scale for each key."
  },
  {
    "objectID": "blog/17_1_17_24_BarryHarris/index.html#practice-notes",
    "href": "blog/17_1_17_24_BarryHarris/index.html#practice-notes",
    "title": "Practicing some Barry Harris methods",
    "section": "Practice notes",
    "text": "Practice notes\nC scale notes. I was playing through this one last night as well. In particular, I was attempting to get the ‘building up notes’ exercise fluid. If I played each step like a new chord it felt chunky. If I played each step like an ascending run in the right hand, and simultaneous descending run in the left hand it was fast. However, when I did those runs I was only playing single notes, and leaving out the internal notes that form the maj6 or dim7 chords. Then, I got into the swing of alternating my fingers properly.\nThe table shows the fingering for thumb to pinky (1-5), for the left and right hands. Start with both thumbs on the same note. Then, the index to pinky (2345) go on every note ascending on the right, and every note descending on the left.\n\n\n\n\n\n\n\n1\n\n1\n\n\n\n\n\n\n\n\n\n2\n\n\n\n2\n\n\n\n\n\n\n\n3\n\n1\n\n1\n\n3\n\n\n\n\n\n4\n\n2\n\n\n\n2\n\n4\n\n\n\n5\n\n3\n\n1\n\n1\n\n3\n\n5\n\n\n\nTook a little while, but once the symmetry hits the fingering locked in pretty easily.\nIn C, starting from the Ab has a particularly nice symmetrical viewpoint.\nF scale notes. Practice went pretty well. This scale uses a different dim7 chord (GBbDdE). Felt like a little bit of interference from C. The Fmaj6 chord has the same shape and fingering as Cmaj6.\nAs I was playing the building up notes exercise through every note, I started to think more about how I could use the exercise. I started asking questions like, where does this movement want to go? For example, starting on the root, and “building up notes” seems to want to resolve on the V.\nBb scale notes. These observations would apply to any of the other scales, but I roughly had these thoughts around this time in practice. So, I’m going to stop organizing notes by scale."
  },
  {
    "objectID": "blog/17_1_17_24_BarryHarris/index.html#scale-practice-checkbox",
    "href": "blog/17_1_17_24_BarryHarris/index.html#scale-practice-checkbox",
    "title": "Practicing some Barry Harris methods",
    "section": "Scale practice checkbox",
    "text": "Scale practice checkbox\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC\nF\nBb\nEb\nAb\nDb\nGb\nB\nE\nA\nD\nG\n\n\n\n\nx\nx\nx\nx"
  },
  {
    "objectID": "blog/17_1_17_24_BarryHarris/index.html#more-practice-notes",
    "href": "blog/17_1_17_24_BarryHarris/index.html#more-practice-notes",
    "title": "Practicing some Barry Harris methods",
    "section": "More practice notes",
    "text": "More practice notes\n\nWhen/Where to start building up?\nNoodling around in CMaj6…lots of CEGA. I started building up on C.\n\nstep 1, playing a C is kind of redundant with the chord, but it sets the location\nstep 2. playing a BD is nice. It’s not part of CEGA, but it adds the maj7 and 9th. Easy to sprinkle in whenever, even if staying on CMaj6. sprinkles of possible movement.\nstep 3. ACE, restating the center, with minor vii feels\nstep 4. AbBDF, dominant chord, lots of possibilities, wants to go G\n\nStill playing over a Cmaj6, I did a little bit of starting the building up notes on different notes, but not too much. Need to do that more, to get a better sense of the directions.\n\n\nWhat a step in the building up exercise provides\nThe building up notes exercise starts by emphasizing some particular note in the scale. This could be where you are at, say in a melody. Then, it alternates between adding the next notes in a maj6 chord, or a dim7 chord. I should sit down and spell this out for myself at some point after I get the fingering down across keys.\n\nin general, the exercise provides a way to start on a note in the scale, transition between restating aspects of the root (through the maj6), or the diminshed 7th chord, with very harmonious sounding transitions. The transitions already have a bit of movement, and they set up off-ramps into other directions. Especially from a dim7th chord, which can easily end up as four different 7th chords.\n\n\n\nCognitive science notes: proactive interference\nProactive interference is a well-known phenomena in the learning and memory literature. The classic finding is from studies of free recall. Participants are given lists of items to study, like words, and then after a delay they are asked to recall as many as possible from the list. Pro-active interference occurs when your memory for items in the list is impaired by previous exposure and/or learning attempts to memorize other lists.\n\nThe figure shows some recall data from Underwood (1957). The x-axis is number of previous lists. Some subjects were new to these kinds of studies, and they had not tried to memorize lists of items before. Their recall was pretty high. Other subjects had participated in these kinds of studies before, and had tried to memorize many different lists (up to 20 different lists). The more lists one had previously tried to learn, the worse participants did when trying to learn and recall a new list.\nAs I try to learn the maj6th diminished scale across all of the keys I’m wondering about the role of pre-active interference, as well as other kinds of interference. If learning scales was like learning random lists of words, then it is possible that every time I learn one scale, the next scale becomes harder to learn because of this so-called pro-active interference. My general sense is that this isn’t how learning scales works in practice, and that prior experience generally helps with learning new things. Not to say that there isn’t sometimes interference.\nEb practice…got through it, thought a little more about pro-active interference, and “task-switching” or key-switching.\nIt’s not clear to me that findings showing pro-active interference in free recall should neatly generalize to learning scales. One reason is that the free-recall studies usually used arbitrary lists of items with no particular relationships to one another. Part of the pro-active interference is that one accidentally recalls words from a previous list, and those get counted as errors. Also, learning additional lists of random items is like trying to grow a vocabulary of random words with no particular meaning.\nIn contrast to random lists of words, musical scales have many shared elements across keys. There is lots of opportunity for facilitation and interference. For example, learning to play the dim7 in the C major 6th diminished scale could facilitate learning the Eb, Gb, and A versions of that scale, because they use the same dim7 chord. At the same time, there could be associative interference at play based on prior learning attempts. For example, learning to interleave CEGA with DFAbB(dim7) could reinforce those interchord assocations, such as C6-Ddim7-E6. Then, consider moving to learn Eb major 6th diminshed. There is a C6-Ddim7-Eb6, and perhaps here the E6 will interfere with the Eb6? I wonder what a MINERVA process would do here…\n\n\nCogsci notes: Key-switching costs\nAnother classic finding in cognition is called the task-switching cost. This has been demonstrated in diverse task situations. The basic finding is that there are usually costs, in terms of time and accuracy, when switching from one task to another.\nI’ve been practicing the build up exercise in C, F, Bb, and Eb. The fingerings are all different, but similar enough that my fluency is getting better, even as I switch to new keys.\nThese four keys share some notes. For example, C appears in all of C, F, Bb, and Eb scales. I did practice the build up for all four, across all notes of the scale, which means that I did start on C and do the build up for each key. However, I practiced the build up ascending and descending within each scale.\nI am 100% experiencing pretty strong key-switching costs if I start on C alone, and try to then produce a build in the key of F, Bb, or Eb. It’s much easier to start on C major. Need to break out of the switch cost by practicing these more.\nA musical benefit is that a build-up exercise starting say on C, can take you in lots of places, by building up into different keys from that one note. Opening so much opportunity for movement. Wow.\n\n\nCogsci notes: Spacing and repetition during practice\nAnother well-known set of phenomena in learning and memory are spacing and repetition effects. I have been thinking about these phenomena as I’ve been practicing, and I was probably thinking more about these than what I was trying to accomplish in Ab as I was learning the maj6 diminished scale in that key.\nSpacing effects seem to be a common export of cognitive psychology, especially in various apps for ‘optimizing’ learning. For example, flashcard apps and language learning apps will build in a spacing algorithm. The purpose of the algorithm is to space out repetitions during practice, which can facilitate learning and memory (whether or not spacing effects have positive or negative benefits, or no effect can depend).\nRepetition effects are another commonly studied phenomena. In general, repeatedly practicing an action, movement pattern, or series of movements, increases the fluency and “automaticity” of the action. Musician’s know this very well. If you haven’t practiced a Gb scale before, or in a while, your performance will be slow and error-prone. If you repeatedly play the scale up and down and up and down, over and over, it get’s easier, and your performance will become faster and more accurate. Repetitions can also make performance worse sometimes (but I’ll skip over those cogsci things for now).\nStudies of practicing motor skills, like learning scales, typically show that performance gains follow a power or exponential functions (both are very similar looking), like the one below.\n\nThe general outcome is that measures of fluency, like scale speed, increase with more practice. However, the gains that are made with practice decrease. The biggest gains in performance are seen early on in practice, afterwards, it takes a lot of practice to make very tiny gains. There are lots of reasons for this basic curve. On a piano, one issue is that people can only move their fingers up to some limit, so there are hard limits to the floor of the function. And, it may take loads of practice to approach those near floor limits.\nA related concept with repetitive practice is the idea of diminished returns. Repetitive practice will increase fluency for sure, but at some point in practice one becomes fluent enough, and further practice isn’t going to do much. So, the question becomes when to stop repeating and move onto something else? Which leads back to the spacing of repetitions question.\nLet’s say you are trying to practice major scales in all of the keys. What kind of practice schedule should one adopt? There are lots of options, and depending on how you arrange things, the repeated attempts can be massed together or spread out. I’ve got more to say about this kind of stuff, but will get back to practicing Gb. Although, it would be possible to get fancy and use concepts from cognition, I’m just going through the circle of fifths. This has musical benefits, and moving in that direction involves going from one key to another that share many notes. As a result, I think I get a little bit of facilitation from learning one scale to another, and also a little of bit of a discriminative challenge to make sure I’m not playing the previous one. Seems like a productive level of difficulty.\nLunch break\n\n\nTurning up the interference\nAll the above being said, over lunch I was wondering about maximum interference practice schedules. For example, consider this build-up note exercise. I could write a computer program to tell me what note to start on, and what key to build on, and have this be totally random. That would be a great little practice tool (note to self to make some midi javascript stuff later). But, it would be hard. Especially for me right now because I don’t know the build-up patterns in most keys yet.\nSomething more approachable, but still hard, would be for me to do much less repetition as I practice the remaining keys (I think I have about 8 left). Up until this point, I have been doing a lot of varied practice within each key. Something like this for each key:\n\nplay the all the notes of the scale up and down, and all around\nplay them all at the same time to “see the notes all together”\nplay them in sets of 4 or 5, starting on different notes\nplay them up and down starting on different notes\nplay the maj6 inversions as chords, then as arpeggios\nplay the dim7 inversions as chords, then as arpeggios\nput the alternating chords together, play through the scale up and down in chords\nplay through the scale up an down in arpeggios\nStart playing the build-up exercise on each note of the scale. Do this up and down and all around\n\nThat’s a lot of different kinds of repetition, and it seems to work pretty well. My fingers get used to the pattern of notes in the key, and then I’m on my way. Still, playing the scale in terms of chords is slow at first, and I’m not getting very fast, but it does become comfortably slow fairly quickly.\nA different sort of practice schedule would be more painful. For example, I could go immediately to playing the chords of the maj6 diminished scaled in Db, which is the next key I need to practice. This will be new for me, and I know it will take sometimes a few seconds or more of staring and thinking, and getting a bit lost, to press the first chord, and then the second and so on. I will be working it out on the fly. After going up the scale one time, I could decide to stop Db, and switch to Gb (the next one in my anti-clockwise circle of fifths practice cycle). That would be new and somewhat painful too. In other words, the harder, less repetitive, more spaced out practice schedule could be: once up then switch to new key.\nMaybe I will try this next I’m not sure if I have the patience for it…as I like to feel the progress. However, I can imagine some potentially interesting side effects (benefits? or at least potential consequences) of this switchy schedule. One possibility is that this schedule may increase associative linkages between a current musical goal and an intended outcome. One reason this might happen is that when attempting new patterns, the goal may be clear (e.g., play the third chord of the maj6 dim scale in Gb), but the action and corresponding pattern may not be clear. So, one is keeping the goal in mind as an instruction, and trying to work out the pattern, and then execute. As a result, this constitutes a processing episode where the goal was in mind at the same time as a puzzling out process, as well as an action being executed. According some theories of learning and memory the things that happen during an episode of learning are preserved in memory, and later retrieved when similar circumstance are re-encountered. Not sure what the practical consequences would be for learning scales in this manner…maybe I should go try this.\nAlright…not as bad as I thought that would feel. Here’s a practice table with some scale completion time data:\n\n\n\nKey\nFinishing time (minutes)\n\n\n\n\nDb\n1:23\n\n\nGb\n1:00\n\n\nB\n1:44\n\n\nE\n1:30\n\n\nA\n:49\n\n\nD\n1:17\n\n\nG\n:56\n\n\nWell-practiced (C)\n:12\n\n\n\nThis was pretty slow going, and fairly cognitively taxing. Was constantly reminding myself what I was trying to do, while puzzling out what to play. Not too bad though. Perhaps this caused me to make some more deliberate connections than I otherwise would have, not sure. My times were generally getting faster as I moved anti-clockwise, but that could reflect my overall fluency in those keys in general. I hadn’t tried the major 6 dim scale in any of these keys before and all the completion times (to play the chord for each note ascending) are about a minute or longer. By comparison, I went back to the piano and played through the C scale in 12 seconds…still pretty slow. For another comparison, I went back and played a regular C major scale in 7th chords in less than 3 seconds. Presumably with enough practice I get the above scales down under 3 seconds too. In C, I’ve already got down to 12 seconds. Not bad.\nI had the worst time on B. I’m also curious on what kind of fluency gains I would make by just repeating this over and over. I think I should practice this a bunch of times in a row and see what the learning curve looks like. I’m using anything fancy, just the stopwatch on my iphone. I will play the scale in chords ascending, and press the lap button every time I’m finished. Let’s put in 10 minutes of repetitive practice and see where that gets me.\nDone. And, time plot the data in R!\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\n\nIt’s not a pretty power curve, but at least it is generally decreasing. I was hovering around a minute per ascending attempt for the first 7 attempts. Then, a few things started to get easier, and I started speeding up. I cut my time by more than half, yah! Still a long way to go to get faster than 3 seconds. And, I suppose this gives me some perspective on what I got out 10 minutes of this kind of practice.\nI’m going to try one more 10 minute burst for fun. This time I’ll do the scale in E, which was my second slowest time. However, instead of playing the scale ascending over and over for 10 minutes. I’m going to do whatever I want for 10 minutes, and see what happens. More specifically, I’ll take the list of things I mentioned earlier that I was doing, that helped me get my fingers into the key, and all that kind of stuff. Then, in the 9th minute, I’ll start timing my ascending runs. Maybe my more varied practice version of 10 minutes would get that scale into shape faster than just focusing on ascending runs…Or maybe not?\nDid that. At the end I did a run of chords in E in about 20 seconds. But, my second attempt ended around 30 seconds with lots of errors. I was probably getting a bit better at all of the other things I was practicing, rather than being able to specifically do the ascending chords.\nAlso, starting to experience brain melt from too much piano practice. Maybe I should call it a day :)"
  },
  {
    "objectID": "blog/24_1_24_24_weekly_practice/index.html",
    "href": "blog/24_1_24_24_weekly_practice/index.html",
    "title": "Week of 1/21 - 1/27 daily practice post",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"musical pattern. music wallpaper. intense music trip. music everywhere.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nTrying out a single post to record daily practice exercises that I will continue to update throughout a week. It’s 1/24 and I will have to reconstruct what might have happened earlier this week."
  },
  {
    "objectID": "blog/24_1_24_24_weekly_practice/index.html#section",
    "href": "blog/24_1_24_24_weekly_practice/index.html#section",
    "title": "Week of 1/21 - 1/27 daily practice post",
    "section": "1/21",
    "text": "1/21\n\ndid a couple of chord reaction time tests\ngot into the 7b5 chords a little bit, playing them in all keys very slowly"
  },
  {
    "objectID": "blog/24_1_24_24_weekly_practice/index.html#section-1",
    "href": "blog/24_1_24_24_weekly_practice/index.html#section-1",
    "title": "Week of 1/21 - 1/27 daily practice post",
    "section": "1/22",
    "text": "1/22\n\nDrilled 7b5 chords a bunch"
  },
  {
    "objectID": "blog/24_1_24_24_weekly_practice/index.html#section-2",
    "href": "blog/24_1_24_24_weekly_practice/index.html#section-2",
    "title": "Week of 1/21 - 1/27 daily practice post",
    "section": "1/23",
    "text": "1/23\n\nmissed a day really oops\nplayed “all of me” a couple times"
  },
  {
    "objectID": "blog/24_1_24_24_weekly_practice/index.html#section-3",
    "href": "blog/24_1_24_24_weekly_practice/index.html#section-3",
    "title": "Week of 1/21 - 1/27 daily practice post",
    "section": "1/24",
    "text": "1/24\nMajor triads\nWent through all of the below in all keys, anti-clockwise through circle of fifths\n\nplay them once\nplayed the triad in right hand, with different bass notes from the triad\nplayed the triad up through keyboard through the inversions\nplayed the triad down through the inversions\nplayed the triad once, on the first inversion\nplayed the triad once, on the second inversion\n\nI didn’t make too many mistakes, but I was more rusty at this then I thought I would be. I’m still slow on Db and Ab.\nStarting on the different inversions is interesting. When I’m really into a jam, I don’t think about what inversions to play and they just happen (on a good day). But, in this case, I’m not used to deliberately trying to play a specific triad in a specific inversion. Playing the second inversion was hardest for some reason."
  },
  {
    "objectID": "blog/25_1_25_24_r_sounds/index.html",
    "href": "blog/25_1_25_24_r_sounds/index.html",
    "title": "Listening to complex tones using sine waves and toneR",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"sine waves. nature. brilliant. sunset. ocean. frequency. complex\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nIn previous posts I’ve been messing around with analyzing chord similarity using a vector space. My plan for this post is render chords as complex tones using R and the {toneR} package, and then listen to things.\nThe code for this is taking me back to grad school. I used to run experiments involving complex tones with missing fundamentals, and I used Matlab to create those tones from sine waves.\nHere’s some R code will generate a complex tone as a sum of a series of sine waves, each at different frequencies and amplitudes.\nThe result is a C7 chord, rendered as a complex tone, with the frequency for each note getting equal amplitude in the summed sine wave.\nShow the code\nlibrary(tuneR)\n\n\n# Function to generate a complex tone\ngenerate_complex_tone &lt;-\n  function(duration,\n           sampling_rate,\n           frequencies,\n           amplitudes) {\n    time_points &lt;- seq(0, duration, 1 / sampling_rate)\n    complex_tone &lt;- sapply(seq_along(frequencies), \n                           function(i) {\n      amplitudes[i] * sin(2 * pi * frequencies[i] * time_points)\n                             }\n      )\n    return(rowSums(complex_tone))\n  }\n\n# Set parameters\nduration &lt;- 5  # seconds\nsampling_rate &lt;- 44100  # Hz (standard audio sampling rate)\nfrequencies &lt;- c(261.63, # Middle C, frequencies of sine waves in Hz\n                 277.18, # D\n                 293.66,\n                 311.13,\n                 329.63,\n                 349.23,\n                 369.99,\n                 392,\n                 415.3,\n                 440,\n                 466.16,\n                 493.88)\n\n# set amplitudes\n# 12 amplitudes, one for each note from C to B\n# this makes a C7 chord\namplitudes &lt;- c(1,0,0,0,1,0,0,1,0,0,1,0)\n\n# Generate complex tone\ncomplex_tone &lt;- generate_complex_tone(duration, \n                                      sampling_rate, \n                                      frequencies, \n                                      amplitudes)\n# normalize for 16 bit\ncomplex_tone &lt;- complex_tone / max(abs(complex_tone))\ncomplex_tone &lt;- complex_tone * 32767\n\n# use tuneR to convert to wave\nwave &lt;- Wave(\n  left = complex_tone,\n  right = complex_tone,\n  samp.rate = sampling_rate,\n  bit = 16\n)\n\nwriteWave(wave,\"C7.wav\")"
  },
  {
    "objectID": "blog/25_1_25_24_r_sounds/index.html#a-series-of-similar-chords",
    "href": "blog/25_1_25_24_r_sounds/index.html#a-series-of-similar-chords",
    "title": "Listening to complex tones using sine waves and toneR",
    "section": "A series of similar chords",
    "text": "A series of similar chords\nI’m pretty sure what I’m about to make is going to sound janky, oh well.\nStep 1: load in the chord vector space I’ve been using to compute similarities between chord patterns.\n\n\nShow the code\nlibrary(tidyverse)\n# pre-processing to get the chord vectors\n\n# load chord vectors\nc_chord_excel &lt;- rio::import(\"chord_vectors.xlsx\")\n\n# grab feature vectors\nc_chord_matrix &lt;- as.matrix(c_chord_excel[,4:15])\n\n# assign row names to the third column containing chord names\nrow.names(c_chord_matrix) &lt;- c_chord_excel[,3]\n\n# define all keys\nkeys &lt;- c(\"C\",\"Db\",\"D\",\"Eb\",\"E\",\"F\",\"Gb\",\"G\",\"Ab\",\"A\",\"Bb\",\"B\")\n\n\n# the excel sheet only has chords in C\n# loop through the keys, permute the matrix to get the chords in the next key\n# add the permuted matrix to new rows in the overall chord_matrix\nfor (i in 1:length(keys)) {\n  \n  if (i == 1) {\n    # initialize chord_matrix with C matrix\n    chord_matrix &lt;- c_chord_matrix\n    \n  } else {\n    #permute the matrix as a function of iterator\n    new_matrix &lt;- cbind(c_chord_matrix[, (14-i):12],c_chord_matrix[, 1:(13-i)] )\n    \n    # rename the rows with the new key\n    new_names &lt;- gsub(\"C\", keys[i], c_chord_excel[,3])\n    row.names(new_matrix) &lt;- new_names\n    \n    # append the new_matrix to chord_matrix\n    chord_matrix &lt;- rbind(chord_matrix,new_matrix)\n  \n    }\n}\n\n\nStep 2: compute the cosine similarities between all chords\n\n\nShow the code\nfirst_order &lt;- lsa::cosine(t(chord_matrix))\nsecond_order &lt;- lsa::cosine(first_order)\n\n\nStep 3: Pick a starting chord and get the list of all other chords, ordered in terms of decreasing similarity. I’ll start with C7. This also prints the first 10, but I there are hundreds more in a row. Also using second-order similarity here for fun.\n\n\nShow the code\nchord_similarities &lt;- sort(second_order['C7',],decreasing=T)\nprint(chord_similarities[1:10])\n\n\n           C7            C9        C7(b9)       Gb7b9b5          Edim \n    1.0000000     0.9763873     0.9701562     0.9691460     0.9633453 \nC major triad       C9(#11)       C9 (13)       Em7(b5)           Gm6 \n    0.9597563     0.9528986     0.9527819     0.9401221     0.9401221 \n\n\nStep 4: This is what I’ve been waiting for. Time for some teeny tiny steps. Basically, the plan is to generate a complex tone for each chord, in order of decreasing similarity, then stitch them together and then listen to them.\nI’m going to experiment with some of the parameters, like duration, and see what happens. Let’s also try the first 10 chords.\nIssue tracker: - try shorter duration per chord - clicks between tones need smoothing - add amplitude envelopes at beginning and end - learned about the {av} package! looks like a wrapper for ffmpeg, can use this to convert wavs to mp3\n\n\nShow the code\n# envelope generator function\nvector_envelope &lt;- function(input_vector,proportion){\n  window &lt;- round(length(input_vector)*proportion)\n  up &lt;- seq(0,1,length.out = window)\n  down &lt;- seq(1,0,length.out = window)\n  input_vector[1:window] &lt;- input_vector[1:window]*up\n  input_vector[(length(input_vector)-(window-1)):length(input_vector)] &lt;- input_vector[(length(input_vector)-(window-1)):length(input_vector)]*down\n  return(input_vector)\n}\n\n\n\n\nShow the code\n# Set parameters\nduration &lt;- 1  # seconds\nsampling_rate &lt;- 44100  # Hz (standard audio sampling rate)\nfrequencies &lt;- c(261.63, # Middle C, frequencies of sine waves in Hz\n                 277.18, # D\n                 293.66,\n                 311.13,\n                 329.63,\n                 349.23,\n                 369.99,\n                 392,\n                 415.3,\n                 440,\n                 466.16,\n                 493.88)\n\n# loop through chords\n\nfor(i in 1:20) {\n  amplitudes &lt;- chord_matrix[names(chord_similarities[i]),]\n  \n  # Generate complex tone\n  complex_tone &lt;- generate_complex_tone(duration,\n                                        sampling_rate,\n                                        frequencies,\n                                        amplitudes)\n  # normalize for 16 bit\n  complex_tone &lt;- complex_tone / max(abs(complex_tone))\n  complex_tone &lt;- complex_tone * 32767\n  \n  complex_tone &lt;- vector_envelope(complex_tone,.5)\n  \n  if(i == 1){\n    tone_series &lt;- complex_tone\n  } else {\n    ##smoothing\n    smooth &lt;- round(length(complex_tone)*.1)\n    \n    tone_series[(length(tone_series)-(smooth-1)):length(tone_series)] &lt;- (tone_series[(length(tone_series)-(smooth-1)):length(tone_series)] + complex_tone[1:smooth])\n    \n    tone_series &lt;- c(tone_series, complex_tone[(smooth+1):length(complex_tone)])\n    \n    #tone_series &lt;- c(tone_series,complex_tone)\n  }\n  \n}\n\n\n# use tuneR to convert to wave\nwave &lt;- Wave(\n  left = tone_series,\n  right = tone_series,\n  samp.rate = sampling_rate,\n  bit = 16\n)\n\nsound_name &lt;- \"C7_series_10\"\nwriteWave(wave,paste0(sound_name,\".wav\"))\nav::av_audio_convert(paste0(sound_name,\".wav\"),paste0(sound_name,\".mp3\"), verbose=FALSE)\nfile.remove(paste0(sound_name,\".wav\"))\n\n\n  \nThis one runs through the first 20 most similar chords to C7. The chords don’transition super smoothly (no portamento), but I got rid of then clicks using envelopes. It’s been a while since I did the math for audio signal processing. The last 10 or so chords all sound the same because the database has lots of chord identities (same note pattern under a different name)."
  },
  {
    "objectID": "blog/25_1_25_24_r_sounds/index.html#more-experiments",
    "href": "blog/25_1_25_24_r_sounds/index.html#more-experiments",
    "title": "Listening to complex tones using sine waves and toneR",
    "section": "More experiments",
    "text": "More experiments\nRemove identical chord patterns\nI botched this so many times…kept coming back to fix little problems.\n\n\nShow the code\nchord_properties &lt;- tibble(\n  type = rep(c_chord_excel$type,length(keys)),\n  key = rep(keys, each = dim(c_chord_matrix)[1]),\n  chord_names  = row.names(chord_matrix),\n  synonyms = list(NA)\n)\n\n# find repeats and build synonym list\nrepeat_indices &lt;- c()\nchord_names &lt;- list()\n\nfirst_occurrence &lt;- c()\n\nfor(i in 1:dim(first_order)[1]){\n  # get the current row\n  evaluate_row &lt;- first_order[i,]\n  \n  # don't count the current item as a repeat\n  evaluate_row[i] &lt;- 0\n  \n  # repeats are the ids for any other 1s found\n  repeats &lt;- which(evaluate_row == 1 )\n  \n  if(length(repeats) == 0){\n  }\n  \n  if(length(repeats) &gt; 0){\n    #add to list of repeat items\n    repeat_indices &lt;- c(repeat_indices,repeats)\n    \n    # add synonyms\n    chord_properties$synonyms[i] &lt;- list(synonyms = row.names(first_order)[repeats])\n  }\n  \n   if(i %in% first_occurrence == FALSE){\n     if(i %in% repeat_indices == FALSE){\n       first_occurrence &lt;- c(first_occurrence,i)\n     }\n  }\n}\n\n# remove repeats, recompute similarities\nchord_matrix_no_repeats &lt;- chord_matrix[first_occurrence,]\n\nfirst_order_no_repeats &lt;- lsa::cosine(t(chord_matrix_no_repeats))\nsecond_order_no_repeats &lt;- lsa::cosine(first_order_no_repeats)\n\n\nListen to C7 in descending similarity with no repeat chords.\n\n\nShow the code\nchord_similarities &lt;- sort(second_order_no_repeats['C7',],decreasing=T)\n\n# Set parameters\nduration &lt;- 1  # seconds\nsampling_rate &lt;- 44100  # Hz (standard audio sampling rate)\nfrequencies &lt;- c(261.63, # Middle C, frequencies of sine waves in Hz\n                 277.18, # D\n                 293.66,\n                 311.13,\n                 329.63,\n                 349.23,\n                 369.99,\n                 392,\n                 415.3,\n                 440,\n                 466.16,\n                 493.88)\n\n# loop through chords\n\nfor(i in 1:30) {\n  amplitudes &lt;- chord_matrix[names(chord_similarities[i]),]\n  \n  # Generate complex tone\n  complex_tone &lt;- generate_complex_tone(duration,\n                                        sampling_rate,\n                                        frequencies,\n                                        amplitudes)\n  # normalize for 16 bit\n  complex_tone &lt;- complex_tone / max(abs(complex_tone))\n  complex_tone &lt;- complex_tone * 32767\n  \n  complex_tone &lt;- vector_envelope(complex_tone,.5)\n  \n  if(i == 1){\n    tone_series &lt;- complex_tone\n  } else {\n    ##smoothing\n    smooth &lt;- round(length(complex_tone)*.1)\n    \n    tone_series[(length(tone_series)-(smooth-1)):length(tone_series)] &lt;- (tone_series[(length(tone_series)-(smooth-1)):length(tone_series)] + complex_tone[1:smooth])\n    \n    tone_series &lt;- c(tone_series, complex_tone[(smooth+1):length(complex_tone)])\n    \n    #tone_series &lt;- c(tone_series,complex_tone)\n  }\n  \n}\n\n\n# use tuneR to convert to wave\nwave &lt;- Wave(\n  left = tone_series,\n  right = tone_series,\n  samp.rate = sampling_rate,\n  bit = 16\n)\n\nsound_name &lt;- \"C7_series_30\"\nwriteWave(wave,paste0(sound_name,\".wav\"))\nav::av_audio_convert(paste0(sound_name,\".wav\"),paste0(sound_name,\".mp3\"), verbose=FALSE)\nfile.remove(paste0(sound_name,\".wav\"))\n\n\n  \nTaking bigger steps (10) between each chord in terms of similarity.\n\n\nShow the code\nchord_similarities &lt;- sort(second_order_no_repeats['C7',],decreasing=T)\n\n# Set parameters\nduration &lt;- 1  # seconds\nsampling_rate &lt;- 44100  # Hz (standard audio sampling rate)\nfrequencies &lt;- c(261.63, # Middle C, frequencies of sine waves in Hz\n                 277.18, # D\n                 293.66,\n                 311.13,\n                 329.63,\n                 349.23,\n                 369.99,\n                 392,\n                 415.3,\n                 440,\n                 466.16,\n                 493.88)\n\n# loop through chords\n\nfor(i in seq(1,300,10)) {\n  amplitudes &lt;- chord_matrix[names(chord_similarities[i]),]\n  \n  # Generate complex tone\n  complex_tone &lt;- generate_complex_tone(duration,\n                                        sampling_rate,\n                                        frequencies,\n                                        amplitudes)\n  # normalize for 16 bit\n  complex_tone &lt;- complex_tone / max(abs(complex_tone))\n  complex_tone &lt;- complex_tone * 32767\n  \n  complex_tone &lt;- vector_envelope(complex_tone,.5)\n  \n  if(i == 1){\n    tone_series &lt;- complex_tone\n  } else {\n    ##smoothing\n    smooth &lt;- round(length(complex_tone)*.1)\n    \n    tone_series[(length(tone_series)-(smooth-1)):length(tone_series)] &lt;- (tone_series[(length(tone_series)-(smooth-1)):length(tone_series)] + complex_tone[1:smooth])\n    \n    tone_series &lt;- c(tone_series, complex_tone[(smooth+1):length(complex_tone)])\n    \n    #tone_series &lt;- c(tone_series,complex_tone)\n  }\n  \n}\n\n\n# use tuneR to convert to wave\nwave &lt;- Wave(\n  left = tone_series,\n  right = tone_series,\n  samp.rate = sampling_rate,\n  bit = 16\n)\n\nsound_name &lt;- \"C7_series_30b\"\nwriteWave(wave,paste0(sound_name,\".wav\"))\nav::av_audio_convert(paste0(sound_name,\".wav\"),paste0(sound_name,\".mp3\"), verbose=FALSE)\nfile.remove(paste0(sound_name,\".wav\"))\n\n\n  \nMildly interesting. These sine waves are tough on the ears though.\n\nThe chord vectors only have 12 features, so they abstract over note positions, chord inversions, voicings, octaves, etc. These experiments might sound more interesting if I randomly assigned frequencies to different octaves to spread out the sound a little bit.\n\n\nShow the code\nchord_similarities &lt;- sort(second_order_no_repeats['C7',],decreasing=T)\n\n# Set parameters\nduration &lt;- 1  # seconds\nsampling_rate &lt;- 44100  # Hz (standard audio sampling rate)\nfrequencies &lt;- c(261.63, # Middle C, frequencies of sine waves in Hz\n                 277.18, # D\n                 293.66,\n                 311.13,\n                 329.63,\n                 349.23,\n                 369.99,\n                 392,\n                 415.3,\n                 440,\n                 466.16,\n                 493.88)\n\n# loop through chords\n\nfor(i in seq(1,300,10)) {\n  amplitudes &lt;- chord_matrix[names(chord_similarities[i]),]\n  \n  random_frequencies &lt;- sample(c(.5,1,2),12,replace=TRUE) *frequencies\n  # Generate complex tone\n  complex_tone &lt;- generate_complex_tone(duration,\n                                        sampling_rate,\n                                        random_frequencies,\n                                        amplitudes)\n  # normalize for 16 bit\n  complex_tone &lt;- complex_tone / max(abs(complex_tone))\n  complex_tone &lt;- complex_tone * 32767\n  \n  complex_tone &lt;- vector_envelope(complex_tone,.5)\n  \n  if(i == 1){\n    tone_series &lt;- complex_tone\n  } else {\n    ##smoothing\n    smooth &lt;- round(length(complex_tone)*.1)\n    \n    tone_series[(length(tone_series)-(smooth-1)):length(tone_series)] &lt;- (tone_series[(length(tone_series)-(smooth-1)):length(tone_series)] + complex_tone[1:smooth])\n    \n    tone_series &lt;- c(tone_series, complex_tone[(smooth+1):length(complex_tone)])\n    \n    #tone_series &lt;- c(tone_series,complex_tone)\n  }\n  \n}\n\n\n# use tuneR to convert to wave\nwave &lt;- Wave(\n  left = tone_series,\n  right = tone_series,\n  samp.rate = sampling_rate,\n  bit = 16\n)\n\nsound_name &lt;- \"C7_series_30c\"\nwriteWave(wave,paste0(sound_name,\".wav\"))\nav::av_audio_convert(paste0(sound_name,\".wav\"),paste0(sound_name,\".mp3\"), verbose=FALSE)\nfile.remove(paste0(sound_name,\".wav\"))\n\n\n  \nsounds more interesting. need to refactor the code a bit so I delete the wav file and save the mp3\n\nLet’s start somewhere out there, and then move toward C7.\n\n\nShow the code\nchord_similarities &lt;- sort(second_order_no_repeats['C7',],decreasing=T)\n\n# Set parameters\nduration &lt;- 1  # seconds\nsampling_rate &lt;- 44100  # Hz (standard audio sampling rate)\nfrequencies &lt;- c(261.63, # Middle C, frequencies of sine waves in Hz\n                 277.18, # D\n                 293.66,\n                 311.13,\n                 329.63,\n                 349.23,\n                 369.99,\n                 392,\n                 415.3,\n                 440,\n                 466.16,\n                 493.88)\n\n# loop through chords\ncounter &lt;- 0\nfor(i in seq(301,1,-5)) {\n  counter &lt;- counter+1\n  amplitudes &lt;- chord_matrix[names(chord_similarities[i]),]\n  \n  random_frequencies &lt;- sample(c(.5,1,2),12,replace=TRUE) *frequencies\n  # Generate complex tone\n  complex_tone &lt;- generate_complex_tone(duration,\n                                        sampling_rate,\n                                        random_frequencies,\n                                        amplitudes)\n  # normalize for 16 bit\n  complex_tone &lt;- complex_tone / max(abs(complex_tone))\n  complex_tone &lt;- complex_tone * 32767\n  \n  complex_tone &lt;- vector_envelope(complex_tone,.5)\n  \n  if(counter == 1){\n    tone_series &lt;- complex_tone\n  } else {\n    ##smoothing\n    smooth &lt;- round(length(complex_tone)*.1)\n    \n    tone_series[(length(tone_series)-(smooth-1)):length(tone_series)] &lt;- (tone_series[(length(tone_series)-(smooth-1)):length(tone_series)] + complex_tone[1:smooth])\n    \n    tone_series &lt;- c(tone_series, complex_tone[(smooth+1):length(complex_tone)])\n    \n    #tone_series &lt;- c(tone_series,complex_tone)\n  }\n  \n}\n\n\n# use tuneR to convert to wave\nwave &lt;- Wave(\n  left = tone_series,\n  right = tone_series,\n  samp.rate = sampling_rate,\n  bit = 16\n)\n\nsound_name &lt;- \"C7_series_30d\"\nwriteWave(wave,paste0(sound_name,\".wav\"))\nav::av_audio_convert(paste0(sound_name,\".wav\"),paste0(sound_name,\".mp3\"), verbose=FALSE)\nfile.remove(paste0(sound_name,\".wav\"))\n\n\n  \nThat goes for almost a minute, not sure If I got the feeling I was headed toward C7."
  },
  {
    "objectID": "blog/25_1_25_24_r_sounds/index.html#stepping-distances-between-chords",
    "href": "blog/25_1_25_24_r_sounds/index.html#stepping-distances-between-chords",
    "title": "Listening to complex tones using sine waves and toneR",
    "section": "Stepping distances between chords",
    "text": "Stepping distances between chords\nNOTE: This section is rough because I keep going back and fixing code, and the answers change.\nI was going to start working on an endless chord progression algorithm, but I realized I needed to pause and figure this out instead.\nThe issue is to consider movement from one chord to another in the vector space. In the above examples I was moving away from C7 or toward C7, simply on the basis of the similarity scores from other chords.\nWhat if I wanted to move between two different chords, say a C major triad, and a G major triad.\nIt’s still the morning, so my vector space math isn’t totally working.\nI’m at C major triad in the multi-dimensional vector space. I see G major triad over there as another point in vector space. I could go straight there. Or, I could go anywhere else from C before going to G. With those options out of the way, let’s do equal-interval steps.\nStarting simple. Choose the C major chord, sort it by decreasing similarity. Find, the G major triad in the list. Print the chord that is halfway between the two.\n\n\nShow the code\nstarting_chord_similarities &lt;- sort(second_order_no_repeats[\"C major triad\",],decreasing = T)\n\ntarget_id &lt;- which(names(starting_chord_similarities) == 'G major triad')\n\nmiddle_chord &lt;- starting_chord_similarities[round(target_id/2)]\nprint(middle_chord)\n\n\n     Cm13 \n0.8095027 \n\n\nThis one works fine between C and G\nanother mistake:I had made a mistake earlier in the code that removed repeats, I previously got a D 9 (13), which sounded good. The Dm13(#11) is harded on the ears.\nFrom before the fix: I played C major triad, D 9 (13), and G major triad on a piano and it sounded great! Neat.\nI’m curious what some of the other chords near this spot are like.\n\n\nShow the code\nstarting_chord_similarities[(round(target_id/2)-3):(round(target_id/2)+3)]\n\n\n      Ab∆9(#11)       Gm13(#11)       Cm13(#11)            Cm13         F9(#11) \n      0.8113597       0.8111463       0.8101535       0.8095027       0.8088325 \nC melodic minor      Em (add 9) \n      0.8085356       0.8084651 \n\n\nAfter another fix, mostly look fine. Good that there are good options!\nAfter fix: The Ab∆9 sounds great in between C and G. The Eb7(b9) works very well. I was almost worried this would start breaking, but still sounding good.\nFrom before the fix: Nice, all of those work pretty well. I might need to switch gears today and make a tool to compute these while I’m at the piano.\nBut let’s persist a little longer.\nWhat if we want to get there in more equal interval steps.\n\n\nShow the code\nstarting_chord_similarities &lt;- sort(second_order_no_repeats[\"C major triad\",],decreasing = T)\n\ntarget_id &lt;- which(names(starting_chord_similarities) == 'G major triad')\n\nnum_steps &lt;- 3\n\nprogression &lt;- starting_chord_similarities[floor(seq(1,target_id,length.out=2+num_steps ))]\nprint(progression)\n\n\nC major triad      Edim(∆7)          Cm13        C note G major triad \n    1.0000000     0.8436963     0.8095027     0.7804147     0.7302693 \n\n\nThis is pretty cool, damn. Sometimes I’m getting back scales instead of chords, so I’d need to clean this up a little bit.\nI’m using existing chords in the space as the locations for intermediate chords. It would be interesting to add a little noise into the process so that some the chords are not quite “right”. Also, interesting to explore unequal steps.\nThe method I’m using is not necessarily finding the exact middle between two chords in chord space, but that’s OK these ones sound good.\nAlso, not necessarily getting symmetrical answers. fun.\n\n\nShow the code\nstarting_chord_similarities &lt;- sort(second_order_no_repeats[\"G major triad\",],decreasing = T)\n\ntarget_id &lt;- which(names(starting_chord_similarities) == 'C major triad')\n\nnum_steps &lt;- 3\n\nprogression &lt;- starting_chord_similarities[floor(seq(1,target_id,length.out=2+num_steps ))]\nprint(progression)\n\n\nG major triad      Bdim(∆7)  C Mixolydian        G note C major triad \n    1.0000000     0.8436963     0.8095027     0.7804147     0.7302693 \n\n\n\nusing this a chord progression calculator. Had to switch to the similarity matrix with lots of repeat chords.\nOverall, I’d like to refine this chord vector space “intermediate” chord finder. It has potential. But, lots of little issues right now.\n\n\nShow the code\nstart_chord &lt;- \"D7\"\nend_chord &lt;- \"G7\"\n\nstarting_chord_similarities &lt;- sort(second_order[start_chord,],decreasing = T)\n\ntarget_id &lt;- which(names(starting_chord_similarities) == end_chord)\n\nnum_steps &lt;- 1\n\nprogression &lt;- starting_chord_similarities[floor(seq(1,target_id,length.out=2+num_steps ))]\nprint(progression)\n\n\n       D7   D∆9(13)        G7 \n1.0000000 0.8595150 0.7792604"
  },
  {
    "objectID": "blog/25_1_25_24_r_sounds/index.html#improving-an-intermediate-chord-finder-function",
    "href": "blog/25_1_25_24_r_sounds/index.html#improving-an-intermediate-chord-finder-function",
    "title": "Listening to complex tones using sine waves and toneR",
    "section": "Improving an intermediate chord finder function",
    "text": "Improving an intermediate chord finder function\nI need to clean up a bunch of things. Ideally, I want a function that takes a chord similarity matrix, a start and end chord, and computes intermediate chords and returns them.\n\ndelete non-unique chord, lots of bugs but seems fine now\nmade synonym list for deleted chords\nnot yet implemented, search by synonym\n\n\n\nShow the code\nlibrary(tidyverse)\n# pre-processing to get the chord vectors\n\n# load chord vectors\nc_chord_excel &lt;- rio::import(\"chord_vectors.xlsx\")\n\n# grab feature vectors\nc_chord_matrix &lt;- as.matrix(c_chord_excel[,4:15])\n\n# assign row names to the third column containing chord names\nrow.names(c_chord_matrix) &lt;- c_chord_excel[,3]\n\n# define all keys\nkeys &lt;- c(\"C\",\"Db\",\"D\",\"Eb\",\"E\",\"F\",\"Gb\",\"G\",\"Ab\",\"A\",\"Bb\",\"B\")\n\n\n# the excel sheet only has chords in C\n# loop through the keys, permute the matrix to get the chords in the next key\n# add the permuted matrix to new rows in the overall chord_matrix\nfor (i in 1:length(keys)) {\n\n  if (i == 1) {\n    # initialize chord_matrix with C matrix\n    chord_matrix &lt;- c_chord_matrix\n\n  } else {\n    #permute the matrix as a function of iterator\n    new_matrix &lt;- cbind(c_chord_matrix[, (14-i):12],c_chord_matrix[, 1:(13-i)] )\n\n    # rename the rows with the new key\n    new_names &lt;- gsub(\"C\", keys[i], c_chord_excel[,3])\n    row.names(new_matrix) &lt;- new_names\n\n    # append the new_matrix to chord_matrix\n    chord_matrix &lt;- rbind(chord_matrix,new_matrix)\n\n  }\n}\n\nchord_properties &lt;- tibble(\n  type = rep(c_chord_excel$type,length(keys)),\n  key = rep(keys, each = dim(c_chord_matrix)[1]),\n  chord_names  = row.names(chord_matrix),\n  synonyms = list(NA),\n  database_chord = FALSE\n)\n\nfirst_order &lt;- lsa::cosine(t(chord_matrix))\n\n# find repeats and build synonym list\nrepeat_indices &lt;- c()\n\nfirst_occurrence &lt;- c()\n\nfor(i in 1:dim(chord_matrix)[1]){\n  # get the current row\n  evaluate_row &lt;- first_order[i,]\n\n  # don't count the current item as a repeat\n  evaluate_row[i] &lt;- 0\n\n  # repeats are the ids for any other 1s found\n  repeats &lt;- which(evaluate_row == 1 )\n\n  if(length(repeats) == 0){\n  }\n\n  if(length(repeats) &gt; 0){\n    #add to list of repeat items\n    repeat_indices &lt;- c(repeat_indices,repeats)\n\n    # add synonyms\n    chord_properties$synonyms[i] &lt;- list(synonyms = row.names(chord_matrix)[repeats])\n  }\n\n  if(i %in% first_occurrence == FALSE){\n    if(i %in% repeat_indices == FALSE){\n      first_occurrence &lt;- c(first_occurrence,i)\n      chord_properties$database_chord[i] &lt;- TRUE\n    }\n  }\n}\n\n# keep only unique chord, recompute similarities\nchord_matrix_no_repeats &lt;- chord_matrix[first_occurrence,]\nfirst_order_no_repeats &lt;- lsa::cosine(t(chord_matrix_no_repeats))\nsecond_order_no_repeats &lt;- lsa::cosine(first_order_no_repeats)\n\n# remove scales and individual notes\nonly_chords &lt;- chord_properties %&gt;%\n  filter(type!=\"scale\",\n         type!=\"key\",\n         database_chord == TRUE)\n\nsecond_order_chords &lt;- second_order_no_repeats[only_chords$chord_names,\n                                               only_chords$chord_names]\n\n#################################\n# FUNCTIONS\n#################################\n\ngenerate_intermediate_chords &lt;- function(start,\n                                         target,\n                                         steps,\n                                         chord_similarity_matrix\n){\n  ordered_similarities &lt;- sort(chord_similarity_matrix[start,],decreasing =T)\n  target_id &lt;- which(names(ordered_similarities) == target)\n  progression &lt;- ordered_similarities[floor(seq(1,target_id,length.out = 2+steps ))]\n  return(progression)\n}\n\ngenerate_intermediate_chords(\"C7\",\"G7\", steps = 3, second_order_chords)\n\n\n       C7      Gsus  D13susb9   Em7(11)        G7 \n1.0000000 0.8677997 0.8211606 0.7944620 0.7368894 \n\n\nThat’s good enough for now. I have a bunch more things I’d want to add to this function\n\nadd option to print more choices (near neighbors)\nadd option to choose chords with X number of notes\n\n\nlistening to a progression\nFrom C7 to G7\n\n\nShow the code\nchord_progression &lt;- generate_intermediate_chords(\"C7\",\"G7\", steps = 3, second_order_chords)\nchord_progression &lt;- c(chord_progression[1],\n                       chord_progression[5],\n                       chord_progression\n                       )\n\n# Set parameters\nduration &lt;- 1  # seconds\nsampling_rate &lt;- 44100  # Hz (standard audio sampling rate)\nfrequencies &lt;- c(261.63, # Middle C, frequencies of sine waves in Hz\n                 277.18, # D\n                 293.66,\n                 311.13,\n                 329.63,\n                 349.23,\n                 369.99,\n                 392,\n                 415.3,\n                 440,\n                 466.16,\n                 493.88)\n\n# loop through chords\ncounter &lt;- 0\nfor(i in 1:length(chord_progression)) {\n  counter &lt;- counter+1\n  amplitudes &lt;- chord_matrix[names(chord_progression[i]),]\n  \n  random_frequencies &lt;- sample(c(1,1,1),12,replace=TRUE) *frequencies\n  # Generate complex tone\n  complex_tone &lt;- generate_complex_tone(duration,\n                                        sampling_rate,\n                                        random_frequencies,\n                                        amplitudes)\n  # normalize for 16 bit\n  complex_tone &lt;- complex_tone / max(abs(complex_tone))\n  complex_tone &lt;- complex_tone * 32767\n  \n  complex_tone &lt;- vector_envelope(complex_tone,.5)\n  \n  if(counter == 1){\n    tone_series &lt;- complex_tone\n  } else {\n    ##smoothing\n    smooth &lt;- round(length(complex_tone)*.1)\n    \n    tone_series[(length(tone_series)-(smooth-1)):length(tone_series)] &lt;- (tone_series[(length(tone_series)-(smooth-1)):length(tone_series)] + complex_tone[1:smooth])\n    \n    tone_series &lt;- c(tone_series, complex_tone[(smooth+1):length(complex_tone)])\n    \n    #tone_series &lt;- c(tone_series,complex_tone)\n  }\n  \n}\n\n\n# use tuneR to convert to wave\nwave &lt;- Wave(\n  left = tone_series,\n  right = tone_series,\n  samp.rate = sampling_rate,\n  bit = 16\n)\n\nsound_name &lt;- \"C7toG7\"\nwriteWave(wave,paste0(sound_name,\".wav\"))\nav::av_audio_convert(paste0(sound_name,\".wav\"),paste0(sound_name,\".mp3\"), verbose=FALSE)\n\n\n[1] \"/Users/mattcrump/Github/homophony.github.io/blog/25_1_25_24_r_sounds/C7toG7.mp3\"\n\n\nShow the code\nfile.remove(paste0(sound_name,\".wav\"))\n\n\n[1] TRUE\n\n\n  \nI think I am nearing the end of wanting to listen to things in terms of sine waves. Maybe I’ll try midi later."
  },
  {
    "objectID": "blog/25_1_25_24_r_sounds/index.html#endless-progression",
    "href": "blog/25_1_25_24_r_sounds/index.html#endless-progression",
    "title": "Listening to complex tones using sine waves and toneR",
    "section": "Endless progression",
    "text": "Endless progression\nDidn’t get this far today.\nGoals:\n\nstart on some chord\nmove away from the chord in similarity terms\nafter some steps, re-center on the current chord"
  },
  {
    "objectID": "blog/24_1_24_24_weekly_practice/index.html#section-4",
    "href": "blog/24_1_24_24_weekly_practice/index.html#section-4",
    "title": "Week of 1/21 - 1/27 daily practice post",
    "section": "1/25",
    "text": "1/25\nFound a lead sheet for Nate Smith’s Retold. Love this song. Challenging 9/8 rhythm. Messed about with this for a while."
  },
  {
    "objectID": "blog/26_1_26_24_space_vis/index.html",
    "href": "blog/26_1_26_24_space_vis/index.html",
    "title": "Steps toward visualizing chord spaces",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"synthesizer floating in space. 80s cartoon. retro.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\n\n\n\n\n\n\n\n\n\nsynthesizer floating in space. 80s cartoon. retro. - Dreamshaper v7\nNo time today for much explanation. Just some code that produces a visual. Lot’s of things to change up here later.\n\ncreate feature vectors for chords\ncalculate similarity matrix\nfind a multi-dimensional scaling solution to plot the chord names in 2-d\n\n\n\nShow the code\nlibrary(tidyverse)\n# pre-processing to get the chord vectors\n\n# load chord vectors\nc_chord_excel &lt;- rio::import(\"chord_vectors.xlsx\")\n\n# grab feature vectors\nc_chord_matrix &lt;- as.matrix(c_chord_excel[,4:15])\n\n# assign row names to the third column containing chord names\nrow.names(c_chord_matrix) &lt;- c_chord_excel[,3]\n\n# define all keys\nkeys &lt;- c(\"C\",\"Db\",\"D\",\"Eb\",\"E\",\"F\",\"Gb\",\"G\",\"Ab\",\"A\",\"Bb\",\"B\")\n\n\n# the excel sheet only has chords in C\n# loop through the keys, permute the matrix to get the chords in the next key\n# add the permuted matrix to new rows in the overall chord_matrix\nfor (i in 1:length(keys)) {\n\n  if (i == 1) {\n    # initialize chord_matrix with C matrix\n    chord_matrix &lt;- c_chord_matrix\n\n  } else {\n    #permute the matrix as a function of iterator\n    new_matrix &lt;- cbind(c_chord_matrix[, (14-i):12],c_chord_matrix[, 1:(13-i)] )\n\n    # rename the rows with the new key\n    new_names &lt;- gsub(\"C\", keys[i], c_chord_excel[,3])\n    row.names(new_matrix) &lt;- new_names\n\n    # append the new_matrix to chord_matrix\n    chord_matrix &lt;- rbind(chord_matrix,new_matrix)\n\n  }\n}\n\nchord_properties &lt;- tibble(\n  type = rep(c_chord_excel$type,length(keys)),\n  key = rep(keys, each = dim(c_chord_matrix)[1]),\n  chord_names  = row.names(chord_matrix),\n  synonyms = list(NA),\n  database_chord = FALSE\n)\n\nfirst_order &lt;- lsa::cosine(t(chord_matrix))\nsecond_order &lt;- lsa::cosine(first_order)\n\n# find repeats and build synonym list\nrepeat_indices &lt;- c()\n\nfirst_occurrence &lt;- c()\n\nfor(i in 1:dim(chord_matrix)[1]){\n  # get the current row\n  evaluate_row &lt;- first_order[i,]\n\n  # don't count the current item as a repeat\n  evaluate_row[i] &lt;- 0\n\n  # repeats are the ids for any other 1s found\n  repeats &lt;- which(evaluate_row == 1 )\n\n  if(length(repeats) == 0){\n  }\n\n  if(length(repeats) &gt; 0){\n    #add to list of repeat items\n    repeat_indices &lt;- c(repeat_indices,repeats)\n\n    # add synonyms\n    chord_properties$synonyms[i] &lt;- list(synonyms = row.names(chord_matrix)[repeats])\n  }\n\n  if(i %in% first_occurrence == FALSE){\n    if(i %in% repeat_indices == FALSE){\n      first_occurrence &lt;- c(first_occurrence,i)\n      chord_properties$database_chord[i] &lt;- TRUE\n    }\n  }\n}\n\nchord_properties &lt;- chord_properties %&gt;%\n  mutate(num_notes = rowSums(chord_matrix),\n         id = 1:dim(chord_matrix)[1])\n\n# keep only unique chord, recompute similarities\nchord_matrix_no_repeats &lt;- chord_matrix[first_occurrence,]\nfirst_order_no_repeats &lt;- lsa::cosine(t(chord_matrix_no_repeats))\nsecond_order_no_repeats &lt;- lsa::cosine(first_order_no_repeats)\n\n# remove scales and individual notes\nonly_chords &lt;- chord_properties %&gt;%\n  filter(type!=\"scale\",\n         type!=\"key\",\n         database_chord == TRUE)\n\nfirst_order_chords &lt;- first_order_no_repeats[only_chords$chord_names,\n                                               only_chords$chord_names]\nsecond_order_chords &lt;- second_order_no_repeats[only_chords$chord_names,\n                                               only_chords$chord_names]\n\n\nFirst-order similarity chord-space using 1-hot vectors.\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(ggrepel)\n\nmds_first_order &lt;- cmdscale((first_order-1))\nmds_first_order &lt;- as_tibble(mds_first_order) %&gt;%\n  cbind(chord_properties)\n\nmds_first_order &lt;- mds_first_order %&gt;%\n  mutate(key = forcats::fct_relevel(key,\"C\",\"F\",\"Bb\",\"Eb\",\"Ab\",\"Db\",\"Gb\",\"B\",\"E\",\"A\",\"D\",\"G\")) %&gt;%\n  filter(type != \"scale\")\n\np1 &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names,\n                                  color = key))+ \n  geom_point()+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  theme_void(base_size = 50)\np1"
  },
  {
    "objectID": "blog/24_1_24_24_weekly_practice/index.html#section-5",
    "href": "blog/24_1_24_24_weekly_practice/index.html#section-5",
    "title": "Week of 1/21 - 1/27 daily practice post",
    "section": "1/26",
    "text": "1/26\nMore retold"
  },
  {
    "objectID": "blog/24_1_24_24_weekly_practice/index.html#section-6",
    "href": "blog/24_1_24_24_weekly_practice/index.html#section-6",
    "title": "Week of 1/21 - 1/27 daily practice post",
    "section": "1/27",
    "text": "1/27\nThinking more about symmetry, scales, chords and the circle of fifths.\n\nStarted on C.\n\nadd G and F, the next notes going clock-wise or anti-clockwise\n\nThis creates Csus, F (add 9), Gsus7 (without the iii or V)\n\nadd D and Bb\n\nscale = C D F G Bb (a 5 note scale)\n\nadd A and Eb\n\nscale = C D Eb F G A Bb (7 note scale) Ends up as Bb major (2 fifths down from C)\n\nadd E and Ab\n\nscale = C D Eb E F G Ab A Bb (9 note scale)\nmessed about with building chords here\nChords with C (F Bb Eb Ab) are built off the anti-clockwise circle of fifths\nChords with C (G D A E) are built off the clockwise circle"
  },
  {
    "objectID": "blog/27_1_28_24_symmetry/index.html",
    "href": "blog/27_1_28_24_symmetry/index.html",
    "title": "Symmetry, high-dimensional chord space, the circle of fifths, and Sunday morning",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"synthesizer floating in space. symmetrical geometry. 80s cartoon. retro.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nIn the last few posts I’ve been making scattered notes about a chord vector space, which represents the elements of individual chords or scales as feature vectors. The feature vectors can then be analyzed in terms of their similarity relationships in high-dimensional geometric space. Although I use similar approaches in some of my cognition lab research, I’m mostly messing about in this context out of curiosity, and with some hope that this exercise may bring usable musical perspectives that I didn’t have before.\nYesterday, I managed to eke out a musical idea from this analysis that I had fun playing on the piano, and this post will attempt to describe how this idea came from the analysis, and also what the idea is."
  },
  {
    "objectID": "blog/27_1_28_24_symmetry/index.html#visualizing-chord-space",
    "href": "blog/27_1_28_24_symmetry/index.html#visualizing-chord-space",
    "title": "Symmetry, high-dimensional chord space, the circle of fifths, and Sunday morning",
    "section": "Visualizing chord space",
    "text": "Visualizing chord space\nSkipping over some details, I computed the cosine similarity between every chord in my database and every other chord. This results in a 756 x756 similarity matrix. With a few tricks (multi-dimensional scaling in this case) it’s possible to visualize the similarity relationships between chords in a 2-dimensional space. If I add all the chords into the visualization it get’s too hard to read. Here’s an example with several chords taken out to make it easier to look at it.\n\n\nShow the code\nlibrary(tidyverse)\n# pre-processing to get the chord vectors\n\n# load chord vectors\nc_chord_excel &lt;- rio::import(\"chord_vectors.xlsx\")\n\n# grab feature vectors\nc_chord_matrix &lt;- as.matrix(c_chord_excel[,4:15])\n\n# assign row names to the third column containing chord names\nrow.names(c_chord_matrix) &lt;- c_chord_excel[,3]\n\n# define all keys\nkeys &lt;- c(\"C\",\"Db\",\"D\",\"Eb\",\"E\",\"F\",\"Gb\",\"G\",\"Ab\",\"A\",\"Bb\",\"B\")\n\n\n# the excel sheet only has chords in C\n# loop through the keys, permute the matrix to get the chords in the next key\n# add the permuted matrix to new rows in the overall chord_matrix\nfor (i in 1:length(keys)) {\n\n  if (i == 1) {\n    # initialize chord_matrix with C matrix\n    chord_matrix &lt;- c_chord_matrix\n\n  } else {\n    #permute the matrix as a function of iterator\n    new_matrix &lt;- cbind(c_chord_matrix[, (14-i):12],c_chord_matrix[, 1:(13-i)] )\n\n    # rename the rows with the new key\n    new_names &lt;- gsub(\"C\", keys[i], c_chord_excel[,3])\n    row.names(new_matrix) &lt;- new_names\n\n    # append the new_matrix to chord_matrix\n    chord_matrix &lt;- rbind(chord_matrix,new_matrix)\n\n  }\n}\n\nchord_properties &lt;- tibble(\n  type = rep(c_chord_excel$type,length(keys)),\n  key = rep(keys, each = dim(c_chord_matrix)[1]),\n  chord_names  = row.names(chord_matrix),\n  synonyms = list(NA),\n  database_chord = FALSE\n)\n\nfirst_order &lt;- lsa::cosine(t(chord_matrix))\n\n# find repeats and build synonym list\nrepeat_indices &lt;- c()\n\nfirst_occurrence &lt;- c()\n\nfor(i in 1:dim(chord_matrix)[1]){\n  # get the current row\n  evaluate_row &lt;- first_order[i,]\n\n  # don't count the current item as a repeat\n  evaluate_row[i] &lt;- 0\n\n  # repeats are the ids for any other 1s found\n  repeats &lt;- which(evaluate_row == 1 )\n\n  if(length(repeats) == 0){\n  }\n\n  if(length(repeats) &gt; 0){\n    #add to list of repeat items\n    repeat_indices &lt;- c(repeat_indices,repeats)\n\n    # add synonyms\n    chord_properties$synonyms[i] &lt;- list(synonyms = row.names(chord_matrix)[repeats])\n  }\n\n  if(i %in% first_occurrence == FALSE){\n    if(i %in% repeat_indices == FALSE){\n      first_occurrence &lt;- c(first_occurrence,i)\n      chord_properties$database_chord[i] &lt;- TRUE\n    }\n  }\n}\n\nchord_properties &lt;- chord_properties %&gt;%\n  mutate(num_notes = rowSums(chord_matrix),\n         id = 1:dim(chord_matrix)[1])\n\n# keep only unique chord, recompute similarities\nchord_matrix_no_repeats &lt;- chord_matrix[first_occurrence,]\nfirst_order_no_repeats &lt;- lsa::cosine(t(chord_matrix_no_repeats))\nsecond_order_no_repeats &lt;- lsa::cosine(first_order_no_repeats)\n\n# remove scales and individual notes\nonly_chords &lt;- chord_properties %&gt;%\n  filter(type!=\"scale\",\n         type!=\"key\",\n         database_chord == TRUE)\n\nfirst_order_chords &lt;- first_order_no_repeats[only_chords$chord_names,\n                                               only_chords$chord_names]\nsecond_order_chords &lt;- second_order_no_repeats[only_chords$chord_names,\n                                               only_chords$chord_names]\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(ggrepel)\nmds_first_order &lt;- cmdscale((first_order-1))\nmds_first_order &lt;- as_tibble(mds_first_order) %&gt;%\n  cbind(chord_properties)\n\nmds_first_order &lt;- mds_first_order %&gt;%\n  mutate(key = forcats::fct_relevel(key,\"C\",\"F\",\"Bb\",\"Eb\",\"Ab\",\"Db\",\"Gb\",\"B\",\"E\",\"A\",\"D\",\"G\")) %&gt;%\n  mutate(bold_me = case_when(type == \"key\" ~ 1,\n                             type != \"key\" ~ 0)) %&gt;%\n  filter(type %in% c(\"scale\",\"other\") == FALSE,\n         num_notes &lt;= 4)\n\np1 &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names,\n                                  color = key,\n                                  size = bold_me))+ \n  geom_point()+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  theme_classic()\n\np1\n\n\n\n\n\n\n\n\n\nThe circle of fifths pops out right away. I put slightly larger dots on the location where individual notes are located in the space. At the bottom is the C note, and then going anti-clockwise we find F, Bb, Eb etc., and going clockwise we get G, D, A, etc."
  },
  {
    "objectID": "blog/27_1_28_24_symmetry/index.html#chord-location-based-on-note-vectors",
    "href": "blog/27_1_28_24_symmetry/index.html#chord-location-based-on-note-vectors",
    "title": "Symmetry, high-dimensional chord space, the circle of fifths, and Sunday morning",
    "section": "Chord location based on note vectors",
    "text": "Chord location based on note vectors\nEach of the chords and notes is a point in the multi-dimensional chord space, represented here in a compressed 2-d form. Each point is also a vector representing a direction from the origin. For example, the next graph draws lines from the origin to the C, E, and G notes, showing their respective directions in the space. These notes make up a C major triad, and it is worth considering how the vectors for the individual notes relate to the vector for the chord.\n\nThe C major triad vector is an equally weighted average of the sum of it’s note vectors. The point is roughly in the middle of the triangle between G, C, and E. Drawing a line from the origin to that point is where we find the C major triad.\nI’ve been looking for musical implications from this graph that offer new (to me) perspectives. Here’s what I’ve come up with so far."
  },
  {
    "objectID": "blog/27_1_28_24_symmetry/index.html#symmetrical-3-note-scale",
    "href": "blog/27_1_28_24_symmetry/index.html#symmetrical-3-note-scale",
    "title": "Symmetry, high-dimensional chord space, the circle of fifths, and Sunday morning",
    "section": "Symmetrical 3 note scale",
    "text": "Symmetrical 3 note scale\nI’m starting on a C note, and playing Cs across the piano as a kind of drone note. Let’s say this establishes a tonal center in C. And, without any more information, I’m heading in the C direction in musical space.\nBut, let’s say I want to branch out and add more notes, but also keep the tonal center on C. In the next several graphs I do this in steps by adding an additional note from the circle of fifths, in both directions, to preserve symmetry.\nFor example, the next graph creates a simple 3 note scale by adding the G and F notes.\n\nPlaying a drone C on the piano, and sprinkling in Gs and Fs sounds fun and very “major fifthy”. The CFG triad is a Csus chord (and the Csus chord is located where it should based on it’s constituent note elements). You also get a paired down Gsus7 (GCF) and Fadd9 (FCG) in this scale. The symmetry is fun in that you add notes, but are still going in the same direction.\nAs I’m playing this type of thing on the piano, and thinking about every note as adding to an accumulating performance vector in chord space, I would have to play these notes (especially F and G) equally frequently to preserve the C direction. If I started playing more Fs, then I would be tilting the performance vector a little bit toward the F note. Musically, this seems like an invitation to start exploring the anti-clockwise direction in terms of the circle of fifths. For example, I could sneak in a Bb, which is the next note past F. This brings in C7sus vibes."
  },
  {
    "objectID": "blog/27_1_28_24_symmetry/index.html#symmetrical-5-note-scale",
    "href": "blog/27_1_28_24_symmetry/index.html#symmetrical-5-note-scale",
    "title": "Symmetry, high-dimensional chord space, the circle of fifths, and Sunday morning",
    "section": "Symmetrical 5 note scale",
    "text": "Symmetrical 5 note scale\nI’m playing Csus (CFG) around the piano and want to two more notes to make it interesting. But, for now, let’s keep it symmetrical. So, I add D and Bb.\n\nNow I’m having fun droning on C, and sprinkling in F Bb and D G everywhere. The 5 note scale works as a C chord = C9sus (C D9 Fsus GV Bb7), or a Gm7sus chord (and more for the other notes). But, let’s keep going with the idea of adding more notes and keeping the direction symmetrical about C."
  },
  {
    "objectID": "blog/27_1_28_24_symmetry/index.html#note-symmetrical-scale",
    "href": "blog/27_1_28_24_symmetry/index.html#note-symmetrical-scale",
    "title": "Symmetry, high-dimensional chord space, the circle of fifths, and Sunday morning",
    "section": "7 note symmetrical scale",
    "text": "7 note symmetrical scale\nAdding the next two notes (A and Eb), we get a 7 note scale. This scale is centered on C in terms of equal weighting the other notes on either side of the circle of fifths.\n\nArranging the notes starting on C, gives: C D Eb F G A Bb, which is the same as a C dorian scale. Or, if I was hammering away on a Bb as the drone note to establish Bb as the tonal center, and then used these same notes (even though they are not symmetrical around Bb), we get the Bb major scale (ionian).\nDare we go to a 9 note scale?"
  },
  {
    "objectID": "blog/27_1_28_24_symmetry/index.html#note-symmetrical-scale-1",
    "href": "blog/27_1_28_24_symmetry/index.html#note-symmetrical-scale-1",
    "title": "Symmetry, high-dimensional chord space, the circle of fifths, and Sunday morning",
    "section": "9 note symmetrical scale",
    "text": "9 note symmetrical scale\nLet’s do it by adding the E and Ab.\n\nThis is a really fun scale to mess around with, and it’s what I’m spending time on right now.\nIt’s big and harmonically rich, and helps me connect to understanding chords as compound vectors that go off in some direction in the circle of fifths.\nFor example, I could play a 5 note C chord with CDFGBb, this type of chord is symmetrical about C.\nWhich chords would I play for each note in the scale? There are options here, but let’s say I start with C Eb F Ab Bb (Cm7sus#5). This chord works, but it’s direction is centered about Bb. To keep a balance around C, I might want the next chord to shoot off on the other side of the circle of fifths. So, to play D, I could do EA D GC (D7sus (add 13)).\nFollowing that pattern, every other note has a chord that alternates between groups of notes from either side of the circle of fifths (about C).\n\nChords as adding fifths\nAnother little practice I am doing is trying to get used to thinking of the above chords as adding notes in terms of the circle of fifths.\nFor example, I’ll play a C, and think let’s add fifths symmetrically, so that means add FG, and then add BbD. Or, let’s say I want to go more minor, that would be playing a C and adding more fifths on the anti-clockwise side. C FBb G, or go further C FBb Eb G.\nWhen I play these chords I’m also playing them as arpeggios, but playing the notes in terms of the circle of fifths. This brings a sense of direction, and control over direction. Even with something simple like a triad with C, F, and G. I’ll start with C, and I could go F to G, or G to F. The first choice seems to nudge the feeling to one side or the other of the circle of fifths.\nI’m assuming that if I practice chords this way I’ll get better at sensing which direction I’m headed in, and what kind of tension is being built when the direction is ambiguous."
  },
  {
    "objectID": "blog/28_1_28_24_weekly_practice/index.html",
    "href": "blog/28_1_28_24_weekly_practice/index.html",
    "title": "Week of 1/28 - 2/3 daily practice post",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"architecture drawing piano. constructing a piano. cartoon. retro 80s. blueprint.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))"
  },
  {
    "objectID": "blog/28_1_28_24_weekly_practice/index.html#section",
    "href": "blog/28_1_28_24_weekly_practice/index.html#section",
    "title": "Week of 1/28 - 2/3 daily practice post",
    "section": "1/28",
    "text": "1/28\nI suspect practice this week will be inspired by circle of fifths symmetry (as discussed in some previous posts) and concepts of chords as vectors that go in a direction in chord space.\nSus practice.\n\nDrone on a C note\n\nadd one note from each side of the circle of fifths (F and G)\nPlay the C sus chord in a droney way across the keyboard spattering Cs and Fs and Gs everywhere\n\nDrone around the Circle of fifths.\n\ntransition to the next sus chord going clockwise (start playing G with C and D around it, then start playing D with G and A around it)\ndo the same thing but going anti-clockwise\nget used to the sus\n\n\nRotate the fifth pairs, keep the drone the same\n\nDrone on a C note\n\nadd the F G around the C note\nkeep the C note, but start moving the other notes around the circle of the fifths.\nF G -&gt; C D -&gt; G A -&gt; D E -&gt; etc.\n\n\nKeep the fifth pairs the same, rotate the drone\n\nDrone on a C, add the F and G.\n\nkeep playing F and G, but rotate C through the circle of fifths\nCFG -&gt; FFG -&gt; BbFG -&gt; EbFG -&gt; etc.\n\n\nThroughout these exercises, try to keep track of what the note relationships are in whichever scale is considered the center.\nPracticing 5 note C chords that are constructed from rotations of neighboring 5ths."
  },
  {
    "objectID": "blog/29_1_29_24_cmajor/index.html",
    "href": "blog/29_1_29_24_cmajor/index.html",
    "title": "C major scale analysis",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"Letter C. Brought to you by the letter C. C. Sesame Street. Muppets.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nI’ve been representing chords as vectors and plotting them in 2-dimensional space to visualize similarity relationships.\nSome of the graphs in previous posts are very busy to look at, and it is difficult for me to appreciate what is going on. This post is about interpreting something very familiar, the C major scale, in terms of that similarity space.\nIn two dimensions, the multi-dimensional scaling algorithm positions notes in terms of the circle of fifths. So, this analysis is similar to what you would get thinking about the C major scale using the circle of fifths. The major difference is the idea of “vectors” that go in a particular direction.\nWithout further ado, here is a plot of all the notes, all of the notes in the C major scale, and the triad chords from the C major scale."
  },
  {
    "objectID": "blog/29_1_29_24_cmajor/index.html#c-major-scale-components-in-vector-space",
    "href": "blog/29_1_29_24_cmajor/index.html#c-major-scale-components-in-vector-space",
    "title": "C major scale analysis",
    "section": "C major scale components in vector space",
    "text": "C major scale components in vector space\n\n\nShow the code\nlibrary(tidyverse)\n# pre-processing to get the chord vectors\n\n# load chord vectors\nc_chord_excel &lt;- rio::import(\"chord_vectors.xlsx\")\n\n# grab feature vectors\nc_chord_matrix &lt;- as.matrix(c_chord_excel[,4:15])\n\n# assign row names to the third column containing chord names\nrow.names(c_chord_matrix) &lt;- c_chord_excel[,3]\n\n# define all keys\nkeys &lt;- c(\"C\",\"Db\",\"D\",\"Eb\",\"E\",\"F\",\"Gb\",\"G\",\"Ab\",\"A\",\"Bb\",\"B\")\n\n\n# the excel sheet only has chords in C\n# loop through the keys, permute the matrix to get the chords in the next key\n# add the permuted matrix to new rows in the overall chord_matrix\nfor (i in 1:length(keys)) {\n\n  if (i == 1) {\n    # initialize chord_matrix with C matrix\n    chord_matrix &lt;- c_chord_matrix\n\n  } else {\n    #permute the matrix as a function of iterator\n    new_matrix &lt;- cbind(c_chord_matrix[, (14-i):12],c_chord_matrix[, 1:(13-i)] )\n\n    # rename the rows with the new key\n    new_names &lt;- gsub(\"C\", keys[i], c_chord_excel[,3])\n    row.names(new_matrix) &lt;- new_names\n\n    # append the new_matrix to chord_matrix\n    chord_matrix &lt;- rbind(chord_matrix,new_matrix)\n\n  }\n}\n\nchord_properties &lt;- tibble(\n  type = rep(c_chord_excel$type,length(keys)),\n  key = rep(keys, each = dim(c_chord_matrix)[1]),\n  chord_names  = row.names(chord_matrix),\n  synonyms = list(NA),\n  database_chord = FALSE\n)\n\nfirst_order &lt;- lsa::cosine(t(chord_matrix))\n\n# find repeats and build synonym list\nrepeat_indices &lt;- c()\n\nfirst_occurrence &lt;- c()\n\nfor(i in 1:dim(chord_matrix)[1]){\n  # get the current row\n  evaluate_row &lt;- first_order[i,]\n\n  # don't count the current item as a repeat\n  evaluate_row[i] &lt;- 0\n\n  # repeats are the ids for any other 1s found\n  repeats &lt;- which(evaluate_row == 1 )\n\n  if(length(repeats) == 0){\n  }\n\n  if(length(repeats) &gt; 0){\n    #add to list of repeat items\n    repeat_indices &lt;- c(repeat_indices,repeats)\n\n    # add synonyms\n    chord_properties$synonyms[i] &lt;- list(synonyms = row.names(chord_matrix)[repeats])\n  }\n\n  if(i %in% first_occurrence == FALSE){\n    if(i %in% repeat_indices == FALSE){\n      first_occurrence &lt;- c(first_occurrence,i)\n      chord_properties$database_chord[i] &lt;- TRUE\n    }\n  }\n}\n\nchord_properties &lt;- chord_properties %&gt;%\n  mutate(num_notes = rowSums(chord_matrix),\n         id = 1:dim(chord_matrix)[1])\n\n# keep only unique chord, recompute similarities\nchord_matrix_no_repeats &lt;- chord_matrix[first_occurrence,]\nfirst_order_no_repeats &lt;- lsa::cosine(t(chord_matrix_no_repeats))\nsecond_order_no_repeats &lt;- lsa::cosine(first_order_no_repeats)\n\n# remove scales and individual notes\nonly_chords &lt;- chord_properties %&gt;%\n  filter(type!=\"scale\",\n         type!=\"key\",\n         database_chord == TRUE)\n\nfirst_order_chords &lt;- first_order_no_repeats[only_chords$chord_names,\n                                               only_chords$chord_names]\nsecond_order_chords &lt;- second_order_no_repeats[only_chords$chord_names,\n                                               only_chords$chord_names]\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(ggrepel)\nmds_first_order &lt;- cmdscale((first_order-1))\nmds_first_order &lt;- as_tibble(mds_first_order) %&gt;%\n  cbind(chord_properties)\n\nmds_first_order &lt;- mds_first_order %&gt;%\n  mutate(key = forcats::fct_relevel(key,\"C\",\"F\",\"Bb\",\"Eb\",\"Ab\",\"Db\",\"Gb\",\"B\",\"E\",\"A\",\"D\",\"G\")) %&gt;%\n  mutate(bold_me = case_when(key == \"C\" ~ 1,\n                             key != \"C\" ~ 0)) %&gt;%\n  filter(type == \"key\" | chord_names %in% c(\"C major triad\",\n                                             \"D minor triad\",\n                                             \"E minor triad\",\n                                             \"F major triad\",\n                                             \"G major triad\",\n                                             \"A minor triad\",\n                                             \"Bdim\",\n                                             \"C major scale\") == TRUE) %&gt;%\n  mutate(V1 = V1*-1,\n         V2 = V2*-1) %&gt;%\n  mutate(line_V1 = case_when(chord_names %in% c(\"C major triad\",\n                                             \"D minor triad\",\n                                             \"E minor triad\",\n                                             \"F major triad\",\n                                             \"G major triad\",\n                                             \"A minor triad\",\n                                             \"Bdim\",\n                                             \"C major scale\",\n                                             \"C note\",\n                                             \"D note\",\n                                             \"E note\",\n                                             \"F note\",\n                                             \"G note\",\n                                             \"A note\",\n                                             \"B note\") ==TRUE ~ V1),\n         line_V2 = case_when(chord_names %in% c(\"C major triad\",\n                                             \"D minor triad\",\n                                             \"E minor triad\",\n                                             \"F major triad\",\n                                             \"G major triad\",\n                                             \"A minor triad\",\n                                             \"Bdim\",\n                                             \"C major scale\",\n                                             \"C note\",\n                                             \"D note\",\n                                             \"E note\",\n                                             \"F note\",\n                                             \"G note\",\n                                             \"A note\",\n                                             \"B note\") ==TRUE ~ V2))\n\np1 &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = line_V1,\n                   yend = line_V2,\n                   color = type))+\n  theme_classic() \np1\n\n\n\n\n\n\n\n\n\nIt’s morning coffee time and my goal is to make observations about the C major scale from these kinds of graphs. A big theme for this analysis is to better understand “where I am going” in chord space when playing different chords or notes. Direction is denoted in the graph using lines and arrows.\nThe graph shows the notes in C in the order of the circle of fifths = F C G D A E B (red lines). This is different from the order of white keys on the keyboard = C D E F G A B.\nThe 7 notes in the scale are symmetrical about the D note. The green arrow shows the vector for the C major scale, which is created by adding all of the notes together. It points in the same direction as the D note. The C major scale (Ionian) is the same as D Dorian. D Dorian has symmetrical 5th components, there are three anti-clockwise (GCF) and three clockwise (AEB). The C major scale is asymmetrical, there is only one anti-clockwise 5th (F), and 5 clockwise 5ths (GDAEB).\nIn some sense the D dorian scale is comfortably balanced and locked into the D direction. The C scale is more off-kilter, like it is in the middle of taking step, it’s going somewhere."
  },
  {
    "objectID": "blog/29_1_29_24_cmajor/index.html#c-major-triad",
    "href": "blog/29_1_29_24_cmajor/index.html#c-major-triad",
    "title": "C major scale analysis",
    "section": "C major triad",
    "text": "C major triad\nThe C major triad has C, E, and G. The vector for the triad is in red. The triad vector is the average of the component vectors for C, E, and G. It’s closer to C and G, than E. The major triad is asymmetrical. It’s direction lands between a G note and a D note. C and G form a perfect fifth, and as a simultaneous pair, their potential for circular motion is ambiguous. Adding the E applies a torque in the clock-wise direction. Similarly, with CE or GE, adding the missing note in the triad twists anti-clockwise.\nIt’s been a while since I took physics statics and dynamics…but, considering C major triad as a static body, with the elements, C, E, and G, as force vectors, does C major triad have an inherent spin? If we take the D note as the center position, the C major scale is a weighted a bit anti-clockwise from D. If we arranged everything so that D was 90 degrees, and then set up the triad, it looks like it would fall to the left. The curvy arrows represents the direction of spin.\nPlaying more E (saying scattering Es around the piano) would add to the weight of the E vector, and apply force consistent with spinning the other way.\n\n\nShow the code\nCmajor &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"C note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"C note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"E note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"E note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"G note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"G note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"C major triad\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"C major triad\")$V2)+\n  geom_curve(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               curvature = 1,\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"C major triad\")$V1/2),\n                   yend = filter(mds_first_order,chord_names == \"C major triad\")$V2/2)+\n  theme_classic()+\n  ggtitle(\"C major triad\")+ \n  theme(plot.title = element_text(size=40))\nCmajor"
  },
  {
    "objectID": "blog/29_1_29_24_cmajor/index.html#d-minor-triad",
    "href": "blog/29_1_29_24_cmajor/index.html#d-minor-triad",
    "title": "C major scale analysis",
    "section": "D minor triad",
    "text": "D minor triad\nThe D minor triad has D, F, and A. The vector for the triad is in red. Its location is right on top of the C major triad, but it gets there from different elements.\nInteresting that C major triad and D minor triads go in the same direction. The D minor notes are also the 9th, 11th, and 13th, notes in C extended chords. Perhaps more fittingly, the C major notes are the 7th, 9th, and 11th extensions of D minor.\nIf the center point is the G note, the D minor triad appears biased to spin clock-wise.\nThe overall pattern is similar to a major chord, 2 contiguous fifths (DA), and one that is 3 away (F). In both the major and the minor, the outlying fifth is the 3rd. However, in the minor chord, the root (D) is closer to the third, than in the major chord.\n\n\nShow the code\nDminor &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"D note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"D note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"F note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"F note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"A note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"A note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"D minor triad\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"D minor triad\")$V2)+\n  geom_curve(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               curvature = -1,\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"D minor triad\")$V1/2),\n                   yend = filter(mds_first_order,chord_names == \"D minor triad\")$V2/2)+\n  theme_classic()+\n  ggtitle(\"D minor triad\")+ \n  theme(plot.title = element_text(size=40))\nDminor"
  },
  {
    "objectID": "blog/29_1_29_24_cmajor/index.html#g-major-and-a-minor",
    "href": "blog/29_1_29_24_cmajor/index.html#g-major-and-a-minor",
    "title": "C major scale analysis",
    "section": "G major and A minor",
    "text": "G major and A minor\nThe patterns for G major and A minor are similar to C major and D minor. The G major and A minor triads point in the same direction as each other.\nThe G major triad is notionally centered on A, and wants to spin anti-clockwise.\nThe A minor triad it notionally centered on D, but it wants to spin clockwise.\n\n\nShow the code\nGmajor &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"G note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"G note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"B note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"B note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"D note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"D note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"G major triad\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"G major triad\")$V2)+\n  geom_curve(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               curvature = 1,\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"G major triad\")$V1/2),\n                   yend = filter(mds_first_order,chord_names == \"G major triad\")$V2/2)+\n  theme_classic()+\n  ggtitle(\"G major triad\")+ \n  theme(plot.title = element_text(size=40))\n\n\nAminor &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"A note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"A note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"C note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"C note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"E note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"E note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"A minor triad\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"A minor triad\")$V2)+\n  geom_curve(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               curvature = -1,\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"A minor triad\")$V1/2),\n                   yend = filter(mds_first_order,chord_names == \"A minor triad\")$V2/2)+\n  theme_classic()+\n  ggtitle(\"A minor triad\")+ \n  theme(plot.title = element_text(size=40))\n#Aminor\n\nlibrary(patchwork)\n\nGmajor+Aminor"
  },
  {
    "objectID": "blog/29_1_29_24_cmajor/index.html#eminor-and-bminor",
    "href": "blog/29_1_29_24_cmajor/index.html#eminor-and-bminor",
    "title": "C major scale analysis",
    "section": "Eminor and Bminor",
    "text": "Eminor and Bminor"
  },
  {
    "objectID": "blog/29_1_29_24_cmajor/index.html#fmajor-eminor-and-bdim",
    "href": "blog/29_1_29_24_cmajor/index.html#fmajor-eminor-and-bdim",
    "title": "C major scale analysis",
    "section": "Fmajor, Eminor and Bdim",
    "text": "Fmajor, Eminor and Bdim\nThere’s only three left. F major wants to spin anti-clockwise. E minor wants to spin clockwise. In terms of the other chords, F major and E minor are further from the notional center (D note), and they have a weaker orbit, compared to Cmajor/Dminor and Gmajor/Aminor, which are both closer to the center.\nThe last chord is B diminished. It is symmetrical about the center (D note). It doesn’t have a spin, but is rather precariously balanced. The D note is keeping it centered, the other two could tip easily.\n\n\nShow the code\nFmajor &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"F note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"F note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"A note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"A note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"C note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"C note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"F major triad\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"F major triad\")$V2)+\n  geom_curve(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               curvature = 1,\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"F major triad\")$V1/2),\n                   yend = filter(mds_first_order,chord_names == \"F major triad\")$V2/2)+\n  theme_classic()+\n  ggtitle(\"F major triad\")+ \n  theme(plot.title = element_text(size=40))\n\n\nEminor &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"E note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"E note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"G note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"G note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"B note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"B note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"E minor triad\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"E minor triad\")$V2)+\n  geom_curve(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               curvature = -1,\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"E minor triad\")$V1/2),\n                   yend = filter(mds_first_order,chord_names == \"E minor triad\")$V2/2)+\n  theme_classic()+\n  ggtitle(\"E minor triad\")+ \n  theme(plot.title = element_text(size=40))\n#Aminor\n\nBdim &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"B note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"B note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"D note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"D note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"F note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"F note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"Bdim\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"Bdim\")$V2)+\n  theme_classic()+\n  ggtitle(\"Bdim\")+ \n  theme(plot.title = element_text(size=40))\n\nlibrary(patchwork)\n\nFmajor+Eminor+Bdim"
  },
  {
    "objectID": "blog/29_1_29_24_cmajor/index.html#wanting-to-go-somewhere",
    "href": "blog/29_1_29_24_cmajor/index.html#wanting-to-go-somewhere",
    "title": "C major scale analysis",
    "section": "Wanting to go somewhere",
    "text": "Wanting to go somewhere\nSequences of chords can produce feelings of tension and resolution. For some sequences it really feels like a particular chord “wants to go” to the resolving chord.\nThe above analysis of triad vectors puts a “spin” on each chord vector, showing which way the chord may want to spin in terms of the circle of fifths. Perhaps this is useful in terms of thinking about movement between chords."
  },
  {
    "objectID": "blog/29_1_29_24_cmajor/index.html#ii-v-i",
    "href": "blog/29_1_29_24_cmajor/index.html#ii-v-i",
    "title": "C major scale analysis",
    "section": "ii-V-i",
    "text": "ii-V-i\n\n\nShow the code\nDminor+Gmajor+Cmajor\n\n\n\n\n\n\n\n\n\nii-V-i progressions are super common. The progression is consistent with the spin of these chords. D minor wants to go clockwise, and is met by a G major triad. G major triad wants to anti-clockwise, is met by a C major triad.\nii-VI is an option too, but the A minor wants to keep spinning clockwise to E minor, and E minor wants to keep spinning in the same direction. However, there are no more chords in the clockwise direction, and spinning out more would quickly deteriorate the key center. E minor doesn’t “want to go” to F major, but going there is a good option for restoring balance to the center."
  },
  {
    "objectID": "blog/29_1_29_24_cmajor/index.html#c-chord-charts",
    "href": "blog/29_1_29_24_cmajor/index.html#c-chord-charts",
    "title": "C major scale analysis",
    "section": "C chord charts",
    "text": "C chord charts\n\n\nShow the code\n(Cmajor | Fmajor | Gmajor) / (Dminor| Eminor | Aminor) / (Bdim | plot_spacer() | plot_spacer())"
  },
  {
    "objectID": "blog/29_1_29_24_cmajor/index.html#general-conclusions",
    "href": "blog/29_1_29_24_cmajor/index.html#general-conclusions",
    "title": "C major scale analysis",
    "section": "General conclusions",
    "text": "General conclusions\nHere are some of my general conclusions.\nA scale of notes, like C major scale, is a compound vector that has a direction in chord space. For example, C major happens to be centered on the D note, and goes in that direction. The triads for this scale reinforce the direction of the scale, as their individual chord vectors are arranged symmetrically about the same key center.\nIndividual chords may have natural rotational motion with respect to the key center of the scale. The major chords want to spin anti-clockwise, and the minor chords want to spin clockwise. The diminished chord doesn’t have a spin.\nThe spin of an individual chord can suggest a chord-progression. For example, D minor spins toward G major. G major spins back toward C major. These, oscillations are also symmetrical about the key center.\nThe well-formedness of the key center can suggest a chord-progression. For example, E minor spins out of C major scale, but can be counter-balanced by F major triad."
  },
  {
    "objectID": "blog/29_1_29_24_cmajor/index.html#performance-vector-concept",
    "href": "blog/29_1_29_24_cmajor/index.html#performance-vector-concept",
    "title": "C major scale analysis",
    "section": "Performance vector concept",
    "text": "Performance vector concept\nThis is a short digression. I’m thinking about applying these ideas to my playing. Let’s say I’m going to improvise in the C major scale using only the notes from the C major scale. And, let’s also say that I want my performance to maintain symmetry about the key center (D note).\nFrom the chord vector space perspective, every time I play a note I am adding another feature into my “performance vector”. I’m thinking of this as a single vector that aggregates all the notes I play. If I played all the notes of the C major scale once, the performance vector would equal the “C major scale” vector shown in the first graph.\nIf I then started playing a lot more Es and As, I would add more E and A features into the vector, and in that moment the cumulative direction of the vector would tilt away from center, in the clockwise direction. If I wanted to re-balance, I would need too play notes on the anti-clockwise side of D. This would accumulate the features in the performance vector that are necessary to shift the direction back to center.\nOverall, it seems interesting to think about how chord progressions and note choices in the context of improvisation add to the performance vector, and either keep it on target, spin it away from center (creating tension), or spin it back (creating resolution?) to center.\nThat’s it for now. Maybe I’ll add the 7th chords another day."
  },
  {
    "objectID": "blog/29_1_29_24_cmajor/index.html#f-major-e-minor-and-b-dim",
    "href": "blog/29_1_29_24_cmajor/index.html#f-major-e-minor-and-b-dim",
    "title": "C major scale analysis",
    "section": "F major, E minor and B dim",
    "text": "F major, E minor and B dim\nThere’s only three left. F major wants to spin anti-clockwise. E minor wants to spin clockwise. In terms of the other chords, F major and E minor are further from the notional center (D note), and they have a weaker orbit, compared to Cmajor/Dminor and Gmajor/Aminor, which are both closer to the center.\nThe last chord is B diminished. It is symmetrical about the center (D note). It doesn’t have a spin, but is rather precariously balanced. The D note is keeping it centered, the other two could tip easily.\n\n\nShow the code\nFmajor &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"F note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"F note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"A note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"A note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"C note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"C note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"F major triad\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"F major triad\")$V2)+\n  geom_curve(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               curvature = 1,\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"F major triad\")$V1/2),\n                   yend = filter(mds_first_order,chord_names == \"F major triad\")$V2/2)+\n  theme_classic()+\n  ggtitle(\"F major triad\")+ \n  theme(plot.title = element_text(size=40))\n\n\nEminor &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"E note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"E note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"G note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"G note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"B note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"B note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"E minor triad\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"E minor triad\")$V2)+\n  geom_curve(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               curvature = -1,\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"E minor triad\")$V1/2),\n                   yend = filter(mds_first_order,chord_names == \"E minor triad\")$V2/2)+\n  theme_classic()+\n  ggtitle(\"E minor triad\")+ \n  theme(plot.title = element_text(size=40))\n#Aminor\n\nBdim &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"B note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"B note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"D note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"D note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"F note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"F note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"Bdim\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"Bdim\")$V2)+\n  theme_classic()+\n  ggtitle(\"Bdim\")+ \n  theme(plot.title = element_text(size=40))\n\nlibrary(patchwork)\n\nFmajor+Eminor+Bdim"
  },
  {
    "objectID": "blog/29_1_29_24_cmajor/index.html#bar-blues-analysis",
    "href": "blog/29_1_29_24_cmajor/index.html#bar-blues-analysis",
    "title": "C major scale analysis",
    "section": "12-bar blues analysis",
    "text": "12-bar blues analysis\nThis probably deserves it’s own post, but I want to quickly take a look at the 12 bar blues in terms of chord space analysis.\nThe 12-bar blues starting with C7 is: C7 (4 bars), F7 (2 bars), C7 (2 bars), G7, F7, C7, G7.\nI (4), IV(2), I(2), V, VI, I, V\nFirst, I’m curious what the performance vector for this series of chords looks like. I heart R indexing :)\nHere are the chord vectors for the 12-bar blues:\n\n\nShow the code\nblues_progression_matrix &lt;-  chord_matrix[c(rep('C7',4),\n               rep('F7',2),\n               rep('C7',2),\n               'G7','F7','C7','G7'),]\nknitr::kable(blues_progression_matrix)\n\n\n\n\n\n\nC\nDb\nD\nEb\nE\nF\nGb\nG\nAb\nA\nBb\nB\n\n\n\n\nC7\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n\n\nC7\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n\n\nC7\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n\n\nC7\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n\n\nF7\n1\n0\n0\n1\n0\n1\n0\n0\n0\n1\n0\n0\n\n\nF7\n1\n0\n0\n1\n0\n1\n0\n0\n0\n1\n0\n0\n\n\nC7\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n\n\nC7\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n\n\nG7\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\n0\n1\n\n\nF7\n1\n0\n0\n1\n0\n1\n0\n0\n0\n1\n0\n0\n\n\nC7\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0\n\n\nG7\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n\n\n\nI’ll take the column sum to compute the performance vector.\n\n\nShow the code\nblues_performance &lt;- colSums(blues_progression_matrix)\nknitr::kable(t(blues_performance))\n\n\n\n\n\nC\nDb\nD\nEb\nE\nF\nGb\nG\nAb\nA\nBb\nB\n\n\n\n\n10\n0\n2\n3\n7\n5\n0\n9\n0\n3\n7\n2\n\n\n\n\n\nNext, I’d like to project this vector into the chord space and see where it is heading.\n\n\nShow the code\n# row bind blues_performance\n\nchord_matrix &lt;- rbind(chord_matrix,blues_performance)\nchord_properties &lt;- rbind(chord_properties,tibble(type = \"performance\",\n                                                  key = NA,\n                                                  chord_names = \"blues_performance\",\n                                                  synonyms = NA,\n                                                  database_chord =NA,\n                                                  num_notes=NA,\n                                                  id = 757))\n\nfirst_order &lt;- lsa::cosine(t(chord_matrix))\n\nmds_first_order &lt;- cmdscale((first_order-1))\nmds_first_order &lt;- as_tibble(mds_first_order) %&gt;%\n  cbind(chord_properties)\n\nmds_first_order &lt;- mds_first_order %&gt;%\n  mutate(key = forcats::fct_relevel(key,\"C\",\"F\",\"Bb\",\"Eb\",\"Ab\",\"Db\",\"Gb\",\"B\",\"E\",\"A\",\"D\",\"G\")) %&gt;%\n  mutate(bold_me = case_when(key == \"C\" ~ 1,\n                             key != \"C\" ~ 0)) %&gt;%\n  filter(type == \"key\" | chord_names %in% c(\"C7\",\n                                             \"F7\",\n                                             \"G7\",\n                                            \"blues_performance\") == TRUE) %&gt;%\n  mutate(V1 = V1*-1,\n         V2 = V2*-1) %&gt;%\n  mutate(line_V1 = case_when(chord_names %in% c(\"C7\",\n                                             \"F7\",\n                                             \"G7\",\n                                             \"blues_performance\",\n                                             \"C note\",\n                                             \"D note\",\n                                             \"Eb note\",\n                                             \"E note\",\n                                             \"F note\",\n                                             \"G note\",\n                                             \"A note\",\n                                             \"Bb note\",\n                                             \"B note\") ==TRUE ~ V1),\n         line_V2 = case_when(chord_names %in% c(\"C7\",\n                                             \"F7\",\n                                             \"G7\",\n                                             \"blues_performance\",\n                                             \"C note\",\n                                             \"D note\",\n                                             \"Eb note\",\n                                             \"E note\",\n                                             \"F note\",\n                                             \"G note\",\n                                             \"A note\",\n                                             \"Bb note\",\n                                             \"B note\") ==TRUE ~ V2))\n\np1 &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = line_V1,\n                   yend = line_V2,\n                   color = type))+\n  theme_classic() +\n  coord_flip()\np1\n\n\n\n\n\n\n\n\n\nThere are three chords are centered around C7 (middle). I didn’t draw the spin, but C7 (CEGBb) is loaded to spin anti-clockwise. And, that is what happens after four bars of C7, we take a spin anti-clockwise to F7.\nIn general, dominant 7th chords want to spin anti-clockwise. However, going to Bb7 from F7 would shift the center of the performance vector. The four bars of C7 at the beginning can be viewed as impressing C7 as the center into the performance vector. If F7 is a deviation for two bars, then it needs to be re-balanced. In 12 bar blues, we go back to C7 for 2 bars, which does some re-balancing. I haven’t plotted this stuff, but the performance vector would still be tilted off C7. There would be six C7s and two F7s in the performance vector, which slightly tilts toward F7. As a result, the last four bars add in G7 twice, and C7 and F7 once each. The G7 counterbalances the F7s, and helps hold the center on C7. Nevertheless, there is one extra F7, as a result the performance vector is slightly off from C7 in the F direction.\nThe very last G7 is a valiant attempt to rotate the performance vector back onto C7, but it doesn’t quite get there by itself. It could use some extra notes to strengthen the force of the rotation. But, it seems palpable what the G7 is trying to do to the performance vector. It doesn’t really matter that it doesn’t get the performance vector to C7 because the 12-bar blues repeats, and the next chord is…C7, which resolves the question if there was any doubt."
  },
  {
    "objectID": "blog/28_1_28_24_weekly_practice/index.html#section-1",
    "href": "blog/28_1_28_24_weekly_practice/index.html#section-1",
    "title": "Week of 1/28 - 2/3 daily practice post",
    "section": "1/29",
    "text": "1/29\nI’m still on C from above.\nPracticing being able transition between those chords and being able to “clock” how many fifths in the anti-clockwise or clockwise direction there are in a chord.\nAlso, started playing the first chord from above with all other notes as possible bass notes. It starts getting pretty hairy with A and E as bass notes."
  },
  {
    "objectID": "blog/30_1_30_24_spinning_chords/index.html",
    "href": "blog/30_1_30_24_spinning_chords/index.html",
    "title": "Charting out chordal spin",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"piano chord. spinning circles. spinning piano. retro. 80s cartoon.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nIn recent past posts, I’ve been representing chords as vectors and plotting them in 2-dimensional space to visualize similarity relationships. Yesterday, I made some charts to look at basic triads in the C scale, and did a little bit of analysis on the 12-bar blues progression. I’m mostly doing this stuff out of general interest, and with some hope that I can learn something to inform my own playing.\nThis morning I’ll continue in the same direction by making the same kind of graphs for more chord types besides triads.\nThe basic idea is to plot a chord as a compound vector inside the circle of fifths. Each of the notes in the chord are vectors that point at their respective notes in the circle of fifths. The chord vector is the average direction of all the note vectors, and it points somewhere in the circle of fifths. Finally, yesterday I added a “spinning” force to some chords that I thought had natural rotational motion with respect to a key center. This last part had potential musical implications in terms of chord progressions that I might find useful when I’m playing.\nI’d like to know how other chords “spin”, so I’m making these here graphs."
  },
  {
    "objectID": "blog/30_1_30_24_spinning_chords/index.html#example-spinning-chord-chart",
    "href": "blog/30_1_30_24_spinning_chords/index.html#example-spinning-chord-chart",
    "title": "Charting out chordal spin",
    "section": "Example spinning chord chart",
    "text": "Example spinning chord chart\n\n\nShow the code\nlibrary(tidyverse)\n# pre-processing to get the chord vectors\n\n# load chord vectors\nc_chord_excel &lt;- rio::import(\"chord_vectors.xlsx\")\n\n# grab feature vectors\nc_chord_matrix &lt;- as.matrix(c_chord_excel[,4:15])\n\n# assign row names to the third column containing chord names\nrow.names(c_chord_matrix) &lt;- c_chord_excel[,3]\n\n# define all keys\nkeys &lt;- c(\"C\",\"Db\",\"D\",\"Eb\",\"E\",\"F\",\"Gb\",\"G\",\"Ab\",\"A\",\"Bb\",\"B\")\n\n\n# the excel sheet only has chords in C\n# loop through the keys, permute the matrix to get the chords in the next key\n# add the permuted matrix to new rows in the overall chord_matrix\nfor (i in 1:length(keys)) {\n\n  if (i == 1) {\n    # initialize chord_matrix with C matrix\n    chord_matrix &lt;- c_chord_matrix\n\n  } else {\n    #permute the matrix as a function of iterator\n    new_matrix &lt;- cbind(c_chord_matrix[, (14-i):12],c_chord_matrix[, 1:(13-i)] )\n\n    # rename the rows with the new key\n    new_names &lt;- gsub(\"C\", keys[i], c_chord_excel[,3])\n    row.names(new_matrix) &lt;- new_names\n\n    # append the new_matrix to chord_matrix\n    chord_matrix &lt;- rbind(chord_matrix,new_matrix)\n\n  }\n}\n\nchord_properties &lt;- tibble(\n  type = rep(c_chord_excel$type,length(keys)),\n  key = rep(keys, each = dim(c_chord_matrix)[1]),\n  chord_names  = row.names(chord_matrix),\n  synonyms = list(NA),\n  database_chord = FALSE\n)\n\nfirst_order &lt;- lsa::cosine(t(chord_matrix))\n\n# find repeats and build synonym list\nrepeat_indices &lt;- c()\n\nfirst_occurrence &lt;- c()\n\nfor(i in 1:dim(chord_matrix)[1]){\n  # get the current row\n  evaluate_row &lt;- first_order[i,]\n\n  # don't count the current item as a repeat\n  evaluate_row[i] &lt;- 0\n\n  # repeats are the ids for any other 1s found\n  repeats &lt;- which(evaluate_row == 1 )\n\n  if(length(repeats) == 0){\n  }\n\n  if(length(repeats) &gt; 0){\n    #add to list of repeat items\n    repeat_indices &lt;- c(repeat_indices,repeats)\n\n    # add synonyms\n    chord_properties$synonyms[i] &lt;- list(synonyms = row.names(chord_matrix)[repeats])\n  }\n\n  if(i %in% first_occurrence == FALSE){\n    if(i %in% repeat_indices == FALSE){\n      first_occurrence &lt;- c(first_occurrence,i)\n      chord_properties$database_chord[i] &lt;- TRUE\n    }\n  }\n}\n\nchord_properties &lt;- chord_properties %&gt;%\n  mutate(num_notes = rowSums(chord_matrix),\n         id = 1:dim(chord_matrix)[1])\n\n# keep only unique chord, recompute similarities\nchord_matrix_no_repeats &lt;- chord_matrix[first_occurrence,]\nfirst_order_no_repeats &lt;- lsa::cosine(t(chord_matrix_no_repeats))\nsecond_order_no_repeats &lt;- lsa::cosine(first_order_no_repeats)\n\n# remove scales and individual notes\nonly_chords &lt;- chord_properties %&gt;%\n  filter(type!=\"scale\",\n         type!=\"key\",\n         database_chord == TRUE)\n\nfirst_order_chords &lt;- first_order_no_repeats[only_chords$chord_names,\n                                               only_chords$chord_names]\nsecond_order_chords &lt;- second_order_no_repeats[only_chords$chord_names,\n                                               only_chords$chord_names]\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(ggrepel)\nmds_first_order &lt;- cmdscale((first_order-1))\nmds_first_order &lt;- as_tibble(mds_first_order) %&gt;%\n  cbind(chord_properties)\n\nmds_first_order &lt;- mds_first_order %&gt;%\n  mutate(key = forcats::fct_relevel(key,\"C\",\"F\",\"Bb\",\"Eb\",\"Ab\",\"Db\",\"Gb\",\"B\",\"E\",\"A\",\"D\",\"G\")) %&gt;%\n  mutate(bold_me = case_when(key == \"C\" ~ 1,\n                             key != \"C\" ~ 0)) %&gt;%\n  filter(type == \"key\" | chord_names %in% c(\"C major triad\",\n                                             \"D minor triad\",\n                                             \"E minor triad\",\n                                             \"F major triad\",\n                                             \"G major triad\",\n                                             \"A minor triad\",\n                                             \"Bdim\",\n                                             \"C major scale\") == TRUE) %&gt;%\n  mutate(V1 = V1*-1,\n         V2 = V2*-1) %&gt;%\n  mutate(line_V1 = case_when(chord_names %in% c(\"C major triad\",\n                                             \"D minor triad\",\n                                             \"E minor triad\",\n                                             \"F major triad\",\n                                             \"G major triad\",\n                                             \"A minor triad\",\n                                             \"Bdim\",\n                                             \"C major scale\",\n                                             \"C note\",\n                                             \"D note\",\n                                             \"E note\",\n                                             \"F note\",\n                                             \"G note\",\n                                             \"A note\",\n                                             \"B note\") ==TRUE ~ V1),\n         line_V2 = case_when(chord_names %in% c(\"C major triad\",\n                                             \"D minor triad\",\n                                             \"E minor triad\",\n                                             \"F major triad\",\n                                             \"G major triad\",\n                                             \"A minor triad\",\n                                             \"Bdim\",\n                                             \"C major scale\",\n                                             \"C note\",\n                                             \"D note\",\n                                             \"E note\",\n                                             \"F note\",\n                                             \"G note\",\n                                             \"A note\",\n                                             \"B note\") ==TRUE ~ V2))\n\np1 &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = line_V1,\n                   yend = line_V2,\n                   color = type))+\n  theme_classic() + ggtitle(\"C major scale\")+ \n  theme(plot.title = element_text(size=40))\n\nCmajor &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"C note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"C note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"E note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"E note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"G note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"G note\")$V2)+\n  geom_segment(\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"D note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"D note\")$V2,\n               linetype = \"dotted\")+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"C major triad\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"C major triad\")$V2)+\n  geom_curve(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               curvature = 1,\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"C major triad\")$V1/2),\n                   yend = filter(mds_first_order,chord_names == \"C major triad\")$V2/2)+\n  theme_classic()+\n  ggtitle(\"C major triad\")+ \n  theme(plot.title = element_text(size=40))\n\nlibrary(patchwork)\n\np1 + Cmajor\n\n\n\n\n\n\n\n\n\nThese are repeat graphs from yesterday. I have coded a whole bunch of chords as feature vectors, calculated cosine similarities between all chords, and then used multi-dimensional scaling to render the chords in a compressed 2-dimensional similarity space. This procedure puts each note into a circle of fifths relationship, as shown in the graph.\nThe C major scale graph draws lines from the origin to each note in the C scale. The graph also has compound vectors showing the direction of each triad, and the direction of the whole scale. The C major scale is symmetrical about the D note, and it points in the direction of the D note. So, I take D as the center.\nThe C major triad graph shows lines for the notes C, E, and G, and a red line for the compound vector, which indicates the direction of the chord. It’s going somewhere between the G note and D note in chord space. The red line is the center of C major triad. If this was a physical object, like an unusual fork with three tines (for C, E, and G), it should balance on its point by tilting it so the red line points up at 90 degrees. From this perspective, the triad does not a have any spin. It wants to go straight ahead, and does not want to spin around the circle.\nHowever, from the perspective of the key center, which is indicated by the dotted line, the chord does have a spin. If the object was tilted so that the dotted line was at 90 degrees, the chord would fall to the left because it is unbalanced. In recognition of this potential for spin, I put a little curvy arrow on the C major triad pointing in the anti-clockwise direction, which is the direction it would “fall”, or spin on the circle."
  },
  {
    "objectID": "blog/30_1_30_24_spinning_chords/index.html#where-is-the-center",
    "href": "blog/30_1_30_24_spinning_chords/index.html#where-is-the-center",
    "title": "Charting out chordal spin",
    "section": "Where is the center?",
    "text": "Where is the center?\nIn the above I use the D note as center, seems pretty obvious based on how the C major scale spreads across the circle of fifths. However, I just realized that in the graphs I made yesterday, I didn’t use the key center to compute spin. In the case of the C major triad, the D note direction is also the bisecting line for the largest angle (CE) in the chord. I had used the bisecting line to compute spin for the other chords.\nUsing the bisecting line as center, major triads spin anti-clockwise and minor triads spin clockwise. The one diminished chord doesn’t spin. I’m not sure that assigning spin this way is a reasonable thing to do or not?\nIf I keep pursuing a physical object analogy, let’s think about welding a little ringlet onto the point of the chordal fork, and then placing the chord object tines up on a nail. The C major triad object should balance when the notional red line points at 90 degrees. But it will fall over to the left, or to the right depending on the starting angle. Yesterday, I had the G major triad spinning anti-clockwise. It would do this in the G major scale. However, if I take the D note as the center, there is a rationale for having the G major chord spin clockwise.\nIn short, if I try to use the key center to compute spin, things could get complicated, especially if it’s not clear what the key center is. I’ll leave that for another day.\nToday, I guess I will use the bisecting line from the largest angle."
  },
  {
    "objectID": "blog/30_1_30_24_spinning_chords/index.html#th-chords-in-c-major",
    "href": "blog/30_1_30_24_spinning_chords/index.html#th-chords-in-c-major",
    "title": "Charting out chordal spin",
    "section": "7th chords in C major",
    "text": "7th chords in C major\nThe chord charts I made yesterday were done in ggplot2. It would be nice to functionalize the code so I could enter some chord names and get the charts without having to hand code everything. I’m having some indecision on whether or not to spend time writing those functions, or just make the graphs.\nI’ll do four by hand to see what I’m dealing with.\n\n\nShow the code\nmds_first_order &lt;- cmdscale((first_order-1))\nmds_first_order &lt;- as_tibble(mds_first_order) %&gt;%\n  cbind(chord_properties)\n\nmds_first_order &lt;- mds_first_order %&gt;%\n  mutate(key = forcats::fct_relevel(key,\"C\",\"F\",\"Bb\",\"Eb\",\"Ab\",\"Db\",\"Gb\",\"B\",\"E\",\"A\",\"D\",\"G\")) %&gt;%\n  mutate(bold_me = case_when(key == \"C\" ~ 1,\n                             key != \"C\" ~ 0)) %&gt;%\n  filter(type == \"key\" | chord_names %in% c(\"C∆7\",\n                                             \"Dm7\",\n                                             \"G7\",\n                                             \"Bm7(b5)\") == TRUE) %&gt;%\n  mutate(V1 = V1*-1,\n         V2 = V2*-1) %&gt;%\n  mutate(line_V1 = case_when(chord_names %in% c(\"C∆7\",\n                                             \"Dm7\",\n                                             \"G7\",\n                                             \"Bm7(b5)\",\n                                             \"C note\",\n                                             \"D note\",\n                                             \"E note\",\n                                             \"F note\",\n                                             \"G note\",\n                                             \"A note\",\n                                             \"B note\") ==TRUE ~ V1),\n         line_V2 = case_when(chord_names %in% c(\"C∆7\",\n                                             \"Dm7\",\n                                             \"G7\",\n                                             \"Bm7(b5)\",\n                                             \"C note\",\n                                             \"D note\",\n                                             \"E note\",\n                                             \"F note\",\n                                             \"G note\",\n                                             \"A note\",\n                                             \"B note\") ==TRUE ~ V2))\n\nCmajor7 &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"C note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"C note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"E note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"E note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"G note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"G note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"B note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"B note\")$V2)+\n  geom_segment(\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"D note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"D note\")$V2,\n               linetype = \"dotted\")+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"C∆7\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"C∆7\")$V2)+\n  geom_curve(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"white\",\n               curvature = 1,\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"C∆7\")$V1/2),\n                   yend = filter(mds_first_order,chord_names == \"C∆7\")$V2/2)+\n  theme_classic()+\n  ggtitle(\"C∆7\")+ \n  theme(plot.title = element_text(size=40))\n\n#Cmajor7\n\nDminor7 &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"D note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"D note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"F note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"F note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"A note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"A note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"C note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"C note\")$V2)+\n  geom_segment(\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"D note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"D note\")$V2,\n               linetype = \"dotted\")+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"Dm7\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"Dm7\")$V2)+\n  geom_curve(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"white\",\n               curvature = 1,\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"Dm7\")$V1/2),\n                   yend = filter(mds_first_order,chord_names == \"Dm7\")$V2/2)+\n  theme_classic()+\n  ggtitle(\"Dm7\")+ \n  theme(plot.title = element_text(size=40))\n\n#Dminor7\n\nG7 &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"G note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"G note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"B note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"B note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"D note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"D note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"F note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"F note\")$V2)+\n  geom_segment(\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"D note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"D note\")$V2,\n               linetype = \"dotted\")+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"G7\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"G7\")$V2)+\n  geom_curve(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               curvature = 1,\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"G7\")$V1/2),\n                   yend = filter(mds_first_order,chord_names == \"G7\")$V2/2)+\n  theme_classic()+\n  ggtitle(\"G7\")+ \n  theme(plot.title = element_text(size=40))\n\n#G7\n\nBm7b5 &lt;- ggplot(mds_first_order, aes(V1, V2, \n                                  label = chord_names))+ \n  geom_point(color=\"black\")+\n  geom_text_repel(size=8, max.overlaps = 500)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"B note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"B note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"D note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"D note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"F note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"F note\")$V2)+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"A note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"A note\")$V2)+\n  geom_segment(\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"D note\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"D note\")$V2,\n               linetype = \"dotted\")+\n  geom_segment(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"Bm7(b5)\")$V1),\n                   yend = filter(mds_first_order,chord_names == \"Bm7(b5)\")$V2)+\n  geom_curve(arrow = arrow(length = unit(0.5, \"cm\")),\n               color = \"red\",\n               curvature = -1,\n               aes(x=0,\n                   y=0,\n                   xend = filter(mds_first_order,chord_names == \"Bm7(b5)\")$V1/2),\n                   yend = filter(mds_first_order,chord_names == \"Bm7(b5)\")$V2/2)+\n  theme_classic()+\n  ggtitle(\"Bm7(b5)\")+ \n  theme(plot.title = element_text(size=40))\n\n#Bm7b5\n\nlibrary(patchwork)\n(Cmajor7 + Dminor7) / (G7 +Bm7b5)\n\n\n\n\n\n\n\n\n\nIn the C∆7 graph I kept the dotted line to the D note as a reference for the key center in C major scale. For all of the chords I used the bisecting line between the largest angle to compute spin.\nC∆7 is a symmetrical chord that points in between D and A. It has no spin. Similarly, Dm7 is a symmetrical chord centered on G that has no spin.\nG7 is not a symmetrical chord and spins anti-clockwise. Bm7(b5) is not symmetrical and spins clockwise–it also points in the same direction as C7.\nI need to call it a morning on this side adventure very soon. But, before I go, I’m thinking again about chord progressions and this spin concept.\nii-V-I is very common in jazz, and I talked about this progression in relation to triads and their spin yesterday. Is there a different story with these seventh chords?\nDm7 is ii, and it doesn’t have a spin. It’s off anti-clockwise from the key center. It is close to G7 (V). Perhaps there is some similarity based gravitational force making Dm7 want to go to G7. Based on spin, it’s more like G7 wants to go to Dm7. Both Dm7 and G7 are on the anti-clockwise side of the key center (D note), so to remain in the key center balance needs to be restored after playing II and V, hence the C∆7.\nIn terms of voicing chords, the spin could change depending on how much weight a given note is given. For example, if C∆7 was played as four notes it would not have a spin. But, if some of the notes were duplicated, then the vector for those notes would get longer and carry more weight. For example, playing a C∆7 chord in the right hand on piano, and with an E bass note would tip the chord to spinning clockwise. Emphasizing a C on the bass would spin it anti-clockwise."
  },
  {
    "objectID": "blog/31_1_30_24_sphere/index.html",
    "href": "blog/31_1_30_24_sphere/index.html",
    "title": "A sphere of fifths",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"piano chord. 3d sphere. piano sphere. retro. 80s cartoon.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nMy previous graphs of chord space have been 2-dimensional. I haven’t looked at the eigendecomposition of the similarity matrix, so I’m not sure how many “meaningful” dimensions there are here. Nevertheless, there are more dimensions than implied by the 2d graph. I’ve been meaning to use MDS to create a 3d plot, and that’s what I’m doing here, hopefully with plotly so it can be spun around interactively in the browser."
  },
  {
    "objectID": "blog/31_1_30_24_sphere/index.html#example-spinning-chord-chart",
    "href": "blog/31_1_30_24_sphere/index.html#example-spinning-chord-chart",
    "title": "A sphere of fifths",
    "section": "Example spinning chord chart",
    "text": "Example spinning chord chart\n\n\nShow the code\nlibrary(tidyverse)\n# pre-processing to get the chord vectors\n\n# load chord vectors\nc_chord_excel &lt;- rio::import(\"chord_vectors.xlsx\")\n\n# grab feature vectors\nc_chord_matrix &lt;- as.matrix(c_chord_excel[,4:15])\n\n# assign row names to the third column containing chord names\nrow.names(c_chord_matrix) &lt;- c_chord_excel[,3]\n\n# define all keys\nkeys &lt;- c(\"C\",\"Db\",\"D\",\"Eb\",\"E\",\"F\",\"Gb\",\"G\",\"Ab\",\"A\",\"Bb\",\"B\")\n\n\n# the excel sheet only has chords in C\n# loop through the keys, permute the matrix to get the chords in the next key\n# add the permuted matrix to new rows in the overall chord_matrix\nfor (i in 1:length(keys)) {\n\n  if (i == 1) {\n    # initialize chord_matrix with C matrix\n    chord_matrix &lt;- c_chord_matrix\n\n  } else {\n    #permute the matrix as a function of iterator\n    new_matrix &lt;- cbind(c_chord_matrix[, (14-i):12],c_chord_matrix[, 1:(13-i)] )\n\n    # rename the rows with the new key\n    new_names &lt;- gsub(\"C\", keys[i], c_chord_excel[,3])\n    row.names(new_matrix) &lt;- new_names\n\n    # append the new_matrix to chord_matrix\n    chord_matrix &lt;- rbind(chord_matrix,new_matrix)\n\n  }\n}\n\nchord_properties &lt;- tibble(\n  type = rep(c_chord_excel$type,length(keys)),\n  key = rep(keys, each = dim(c_chord_matrix)[1]),\n  chord_names  = row.names(chord_matrix),\n  synonyms = list(NA),\n  database_chord = FALSE\n)\n\nfirst_order &lt;- lsa::cosine(t(chord_matrix))\n\n# find repeats and build synonym list\nrepeat_indices &lt;- c()\n\nfirst_occurrence &lt;- c()\n\nfor(i in 1:dim(chord_matrix)[1]){\n  # get the current row\n  evaluate_row &lt;- first_order[i,]\n\n  # don't count the current item as a repeat\n  evaluate_row[i] &lt;- 0\n\n  # repeats are the ids for any other 1s found\n  repeats &lt;- which(evaluate_row == 1 )\n\n  if(length(repeats) == 0){\n  }\n\n  if(length(repeats) &gt; 0){\n    #add to list of repeat items\n    repeat_indices &lt;- c(repeat_indices,repeats)\n\n    # add synonyms\n    chord_properties$synonyms[i] &lt;- list(synonyms = row.names(chord_matrix)[repeats])\n  }\n\n  if(i %in% first_occurrence == FALSE){\n    if(i %in% repeat_indices == FALSE){\n      first_occurrence &lt;- c(first_occurrence,i)\n      chord_properties$database_chord[i] &lt;- TRUE\n    }\n  }\n}\n\nchord_properties &lt;- chord_properties %&gt;%\n  mutate(num_notes = rowSums(chord_matrix),\n         id = 1:dim(chord_matrix)[1])\n\n# keep only unique chord, recompute similarities\nchord_matrix_no_repeats &lt;- chord_matrix[first_occurrence,]\nfirst_order_no_repeats &lt;- lsa::cosine(t(chord_matrix_no_repeats))\nsecond_order_no_repeats &lt;- lsa::cosine(first_order_no_repeats)\n\n# remove scales and individual notes\nonly_chords &lt;- chord_properties %&gt;%\n  filter(type!=\"scale\",\n         type!=\"key\",\n         database_chord == TRUE)\n\nfirst_order_chords &lt;- first_order_no_repeats[only_chords$chord_names,\n                                               only_chords$chord_names]\nsecond_order_chords &lt;- second_order_no_repeats[only_chords$chord_names,\n                                               only_chords$chord_names]\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(plotly)\nlibrary(bslib)\n\nmds_first_order &lt;- cmdscale((first_order-1),k=3)\nmds_first_order &lt;- as_tibble(mds_first_order) %&gt;%\n  cbind(chord_properties) %&gt;%\n  mutate(bold_me = case_when(type == \"key\" ~ 8,\n                             type != \"key\" ~ 7)) %&gt;%\n  filter(type %in% c(\"scale\",\"other\") == FALSE,\n         num_notes &lt;= 4)\n\nfig &lt;- plot_ly(mds_first_order, x = ~V1, y = ~V2, z = ~V3, color = ~type, mode='text', text = ~chord_names)\n\nfig |&gt;\n  bslib::card(full_screen = TRUE)"
  },
  {
    "objectID": "blog/31_1_30_24_sphere/index.html#chord-sphere",
    "href": "blog/31_1_30_24_sphere/index.html#chord-sphere",
    "title": "A sphere of fifths",
    "section": "Chord sphere",
    "text": "Chord sphere\n\n\nShow the code\nlibrary(tidyverse)\n# pre-processing to get the chord vectors\n\n# load chord vectors\nc_chord_excel &lt;- rio::import(\"chord_vectors.xlsx\")\n\n# grab feature vectors\nc_chord_matrix &lt;- as.matrix(c_chord_excel[,4:15])\n\n# assign row names to the third column containing chord names\nrow.names(c_chord_matrix) &lt;- c_chord_excel[,3]\n\n# define all keys\nkeys &lt;- c(\"C\",\"Db\",\"D\",\"Eb\",\"E\",\"F\",\"Gb\",\"G\",\"Ab\",\"A\",\"Bb\",\"B\")\n\n\n# the excel sheet only has chords in C\n# loop through the keys, permute the matrix to get the chords in the next key\n# add the permuted matrix to new rows in the overall chord_matrix\nfor (i in 1:length(keys)) {\n\n  if (i == 1) {\n    # initialize chord_matrix with C matrix\n    chord_matrix &lt;- c_chord_matrix\n\n  } else {\n    #permute the matrix as a function of iterator\n    new_matrix &lt;- cbind(c_chord_matrix[, (14-i):12],c_chord_matrix[, 1:(13-i)] )\n\n    # rename the rows with the new key\n    new_names &lt;- gsub(\"C\", keys[i], c_chord_excel[,3])\n    row.names(new_matrix) &lt;- new_names\n\n    # append the new_matrix to chord_matrix\n    chord_matrix &lt;- rbind(chord_matrix,new_matrix)\n\n  }\n}\n\nchord_properties &lt;- tibble(\n  type = rep(c_chord_excel$type,length(keys)),\n  key = rep(keys, each = dim(c_chord_matrix)[1]),\n  chord_names  = row.names(chord_matrix),\n  synonyms = list(NA),\n  database_chord = FALSE\n)\n\nfirst_order &lt;- lsa::cosine(t(chord_matrix))\n\n# find repeats and build synonym list\nrepeat_indices &lt;- c()\n\nfirst_occurrence &lt;- c()\n\nfor(i in 1:dim(chord_matrix)[1]){\n  # get the current row\n  evaluate_row &lt;- first_order[i,]\n\n  # don't count the current item as a repeat\n  evaluate_row[i] &lt;- 0\n\n  # repeats are the ids for any other 1s found\n  repeats &lt;- which(evaluate_row == 1 )\n\n  if(length(repeats) == 0){\n  }\n\n  if(length(repeats) &gt; 0){\n    #add to list of repeat items\n    repeat_indices &lt;- c(repeat_indices,repeats)\n\n    # add synonyms\n    chord_properties$synonyms[i] &lt;- list(synonyms = row.names(chord_matrix)[repeats])\n  }\n\n  if(i %in% first_occurrence == FALSE){\n    if(i %in% repeat_indices == FALSE){\n      first_occurrence &lt;- c(first_occurrence,i)\n      chord_properties$database_chord[i] &lt;- TRUE\n    }\n  }\n}\n\nchord_properties &lt;- chord_properties %&gt;%\n  mutate(num_notes = rowSums(chord_matrix),\n         id = 1:dim(chord_matrix)[1])\n\n# keep only unique chord, recompute similarities\nchord_matrix_no_repeats &lt;- chord_matrix[first_occurrence,]\nfirst_order_no_repeats &lt;- lsa::cosine(t(chord_matrix_no_repeats))\nsecond_order_no_repeats &lt;- lsa::cosine(first_order_no_repeats)\n\n# remove scales and individual notes\nonly_chords &lt;- chord_properties %&gt;%\n  filter(type!=\"scale\",\n         type!=\"key\",\n         database_chord == TRUE)\n\nfirst_order_chords &lt;- first_order_no_repeats[only_chords$chord_names,\n                                               only_chords$chord_names]\nsecond_order_chords &lt;- second_order_no_repeats[only_chords$chord_names,\n                                               only_chords$chord_names]\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(plotly)\nlibrary(bslib)\n\nmds_first_order &lt;- cmdscale((first_order-1),k=3)\nmds_first_order &lt;- as_tibble(mds_first_order) %&gt;%\n  cbind(chord_properties) %&gt;%\n  mutate(bold_me = case_when(type == \"key\" ~ 8,\n                             type != \"key\" ~ 7)) %&gt;%\n  filter(type %in% c(\"scale\",\"other\") == FALSE,\n         num_notes &lt;= 4)\n\nfig &lt;- plot_ly(mds_first_order, x = ~V1, y = ~V2, z = ~V3, color = ~type, mode='text', text = ~chord_names)\n\nfig |&gt;\n  bslib::card(full_screen = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI took out a bunch of chords so this was easier to look at. The plotly graph should go full screen and has some controls for spinning the sphere around and zooming in etc. If you click the names in the legend you can hide or show the corresponding chords, which can make it easier to see how things shake out.\nIn terms of the circle of fifths, the new z dimension puts 6 of them up top, and 6 of them down below. Within the X-Y plane, there are two alternating circles of fifths (CDEGbAbBb, and FGABDbEb).\nNeat to poke around here."
  },
  {
    "objectID": "blog/32_1_30_24_R_synth/index.html",
    "href": "blog/32_1_30_24_R_synth/index.html",
    "title": "Midi and synthesis in R",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"computer music. musical computer. music represented as bits going into the fabric of the universe. 80s cartoon retro.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nI’m going to be running a cognition experiment or two this semester that will involve creating musical stimuli. I would like programmatic control over that, so I’m delighted to learn that there are existing R packages that will help me with a few things.\nI’m just testing a few things out here, which means this page will be a loose collection of notes and scraps of code."
  },
  {
    "objectID": "blog/32_1_30_24_R_synth/index.html#reading-in-midi-with-pyramidi",
    "href": "blog/32_1_30_24_R_synth/index.html#reading-in-midi-with-pyramidi",
    "title": "Midi and synthesis in R",
    "section": "Reading in MIDI with pyramidi",
    "text": "Reading in MIDI with pyramidi\nIt looks like I can read in MIDI data to a dataframe with pyramidi.\nRequires some python stuff, but it is working. Kudos to Urs Wilke for developing pyramidi! It uses R6, which I don’t use very often.\n\n\nShow the code\nlibrary(pyramidi)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\n\nmidifile &lt;- MidiFramer$new(\"Top Gun Theme.mid\")\n\nknitr::kable(midifile$df_notes_wide[1:10,])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ni_track\nmeta\nprogram\nchannel\ncontrol\nvalue\nnote\ni_note\nvelocity_note_on\nticks_note_on\nb_note_on\n\n\n\n\n1\nFALSE\nNaN\n0\nNaN\nNaN\n24\n1\n110\n2304\n48\n\n\n1\nFALSE\nNaN\n0\nNaN\nNaN\n24\n2\n0\n3792\n79\n\n\n1\nFALSE\nNaN\n0\nNaN\nNaN\n24\n3\n110\n3840\n80\n\n\n1\nFALSE\nNaN\n0\nNaN\nNaN\n24\n4\n0\n5328\n111\n\n\n1\nFALSE\nNaN\n0\nNaN\nNaN\n24\n5\n110\n5376\n112\n\n\n1\nFALSE\nNaN\n0\nNaN\nNaN\n36\n1\n110\n5376\n112\n\n\n1\nFALSE\nNaN\n0\nNaN\nNaN\n24\n6\n0\n6096\n127\n\n\n1\nFALSE\nNaN\n0\nNaN\nNaN\n36\n2\n0\n6096\n127\n\n\n1\nFALSE\nNaN\n0\nNaN\nNaN\n24\n7\n110\n6144\n128\n\n\n1\nFALSE\nNaN\n0\nNaN\nNaN\n41\n1\n110\n6144\n128"
  },
  {
    "objectID": "blog/32_1_30_24_R_synth/index.html#writing-midi",
    "href": "blog/32_1_30_24_R_synth/index.html#writing-midi",
    "title": "Midi and synthesis in R",
    "section": "Writing Midi",
    "text": "Writing Midi\nHaven’t tried this yet."
  },
  {
    "objectID": "blog/32_1_30_24_R_synth/index.html#synthesing-midi-to-wav-and-mp3",
    "href": "blog/32_1_30_24_R_synth/index.html#synthesing-midi-to-wav-and-mp3",
    "title": "Midi and synthesis in R",
    "section": "Synthesing midi to wav and mp3",
    "text": "Synthesing midi to wav and mp3\nraudiomate\nalso need fluid synth\nAnd, apparently fluid synth needs sound fonts. Got this one https://member.keymusician.com/Member/FluidR3_GM/index.html\nI could not get raudiomate to work. The processx:run command kept putting quotes where they didn’t belong.\nUsed the av package to turn the wav into an mp3\n\n\nShow the code\nsystem(\"fluidsynth -F out.wav ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 'Top Gun Theme.mid'\")\n\nav::av_audio_convert(\"out.wav\",\"out.mp3\")\n\n\n  \nThis all took way longer than I expected. Mostly fiddling with python packages and paths to things. But, I declare victory because it made the Top Gun theme song into an mp3."
  },
  {
    "objectID": "blog/32_1_30_24_R_synth/index.html#synthesizing-midi-to-wav-and-mp3",
    "href": "blog/32_1_30_24_R_synth/index.html#synthesizing-midi-to-wav-and-mp3",
    "title": "Midi and synthesis in R",
    "section": "Synthesizing midi to wav and mp3",
    "text": "Synthesizing midi to wav and mp3\nraudiomate\nalso need fluid synth\nAnd, apparently fluid synth needs sound fonts. Got this one https://member.keymusician.com/Member/FluidR3_GM/index.html\nI could also try timidity, but I haven’t gone there yet.\nI could not get raudiomate helper functions to work. The processx::run command kept putting quotes where they didn’t belong\n\nused the system command to run fluidsynth\nused the av package to turn the wav into an mp3\n\n\n\nShow the code\nsystem(\"fluidsynth -F out.wav ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 'Top Gun Theme.mid'\")\n\nav::av_audio_convert(\"out.wav\",\"out.mp3\")\n\n\n  \nThis all took way longer than I expected. Mostly fiddling with python packages and paths to things. But, I declare victory because it made the Top Gun theme song into an mp3.\n\nVery happy that it is possible to import, manipulate, and render midi files using R as a central command language. Although I have been messing with midi stuff for since the 80s, I have not tried to programmatically mess with it, or dug into midi file convention and structure.\nI have some work ahead if I want to compose “computer music” with R. That would be fun though. I would probably glitch out until everything descends into modem sounds. Actually, I’m secretly extremely excited about some possibilities that I’ve wanted to explore.\nAnyway, below is scraps of code trying out various things and making notes to my future self."
  },
  {
    "objectID": "blog/32_1_30_24_R_synth/index.html#questions",
    "href": "blog/32_1_30_24_R_synth/index.html#questions",
    "title": "Midi and synthesis in R",
    "section": "Questions",
    "text": "Questions\nI’ve only got 30 minutes right now, can I make this thing randomize notes?\nanswer, close enough for now.\n\n\nShow the code\n# trying stuff from the pyramidi docs\n\n# load in a basic midi file\nmidi_file_string &lt;- system.file(\"extdata\", \"test_midi_file.mid\", package = \"pyramidi\")\n\nmfr &lt;- MidiFramer$new(midi_file_string)\n\n# mfr has a bunch of midi dataframes in it\n#mfr$df_notes_wide[1:10]\n\n# helper function\nbeats_to_ticks &lt;- function(notes_wide) {\n  notes_wide %&gt;%\n    mutate(\n      ticks_note_on  = b_note_on  * ticks_per_beat,\n      ticks_note_off = b_note_off * ticks_per_beat\n    )\n}\n\nn_beats &lt;- 16\nticks_per_beat &lt;- 960L\n\n#(b_note_on = (0:(n_beats-1) %/% 4) * 4)\n\nb_note_on &lt;- 0:(n_beats-1)\n\n\nnotes &lt;- tibble(\n  i_track = 0,\n  meta = FALSE,\n  note = sample(c(60, 64, 67, 72), 16, replace=T),\n  channel = 1,\n  i_note = 1:n_beats,\n  velocity_note_on = 100,\n  velocity_note_off = 0,\n  b_note_on = b_note_on,\n  b_note_off = b_note_on + 1,\n)\n\nmfr$update_notes_wide(beats_to_ticks(notes))\n\ndf_notes_long &lt;- pivot_long_notes(mfr$df_notes_wide)\ndf_midi_out &lt;- merge_midi_frames(mfr$df_meta, mfr$df_notes_long, mfr$df_not_notes)\n\ndfc2 &lt;- df_midi_out %&gt;%\n        miditapyr$nest_midi(repair_reticulate_conversion = TRUE)\n\nmiditapyr$write_midi(dfc2, ticks_per_beat, \"test.mid\")\n\nsystem(\"fluidsynth -F test.wav ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 'test.mid'\")\n\nav::av_audio_convert(\"test.wav\",\"test.mp3\")"
  },
  {
    "objectID": "blog/32_1_30_24_R_synth/index.html#writing-some-random-notes",
    "href": "blog/32_1_30_24_R_synth/index.html#writing-some-random-notes",
    "title": "Midi and synthesis in R",
    "section": "Writing some random notes",
    "text": "Writing some random notes\nI’ve only got 30 minutes right now, can I make this thing randomize notes?\nStrategy: Read the pyramidi docs, borrow some code from there, and try to play some random notes.\nAnswer: yes, close enough for now.\n\n\nShow the code\n# trying stuff from the pyramidi docs\n\n# load in a basic midi file\nmidi_file_string &lt;- system.file(\"extdata\", \"test_midi_file.mid\", package = \"pyramidi\")\n\nmfr &lt;- MidiFramer$new(midi_file_string)\n\n# mfr has a bunch of midi dataframes in it\n#mfr$df_notes_wide[1:10]\n\n# helper function\nbeats_to_ticks &lt;- function(notes_wide) {\n  notes_wide %&gt;%\n    mutate(\n      ticks_note_on  = b_note_on  * ticks_per_beat,\n      ticks_note_off = b_note_off * ticks_per_beat\n    )\n}\n\nn_beats &lt;- 16\nticks_per_beat &lt;- 960L\n\n#(b_note_on = (0:(n_beats-1) %/% 4) * 4)\n\nb_note_on &lt;- 0:(n_beats-1)\n\n\nnotes &lt;- tibble(\n  i_track = 0,\n  meta = FALSE,\n  note = sample(c(60, 64, 67, 72), 16, replace=T),\n  channel = 1,\n  i_note = 1:n_beats,\n  velocity_note_on = 100,\n  velocity_note_off = 0,\n  b_note_on = b_note_on,\n  b_note_off = b_note_on + 1,\n)\n\nmfr$update_notes_wide(beats_to_ticks(notes))\n\ndf_notes_long &lt;- pivot_long_notes(mfr$df_notes_wide)\ndf_midi_out &lt;- merge_midi_frames(mfr$df_meta, mfr$df_notes_long, mfr$df_not_notes)\n\ndfc2 &lt;- df_midi_out %&gt;%\n        miditapyr$nest_midi(repair_reticulate_conversion = TRUE)\n\nmiditapyr$write_midi(dfc2, ticks_per_beat, \"test.mid\")\n\nsystem(\"fluidsynth -F test.wav ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 'test.mid'\")\n\nav::av_audio_convert(\"test.wav\",\"test.mp3\")"
  },
  {
    "objectID": "blog/32_1_30_24_R_synth/index.html#midi-specs",
    "href": "blog/32_1_30_24_R_synth/index.html#midi-specs",
    "title": "Midi and synthesis in R",
    "section": "Midi specs",
    "text": "Midi specs\nDon’t have time to read through this today, but here it is.\n\nhttps://www.midi.org/specifications"
  },
  {
    "objectID": "blog/32_1_30_24_R_synth/index.html#drums",
    "href": "blog/32_1_30_24_R_synth/index.html#drums",
    "title": "Midi and synthesis in R",
    "section": "Drums",
    "text": "Drums\nOn BPM, tempo, and midi ticks\n\nhttps://majicdesigns.github.io/MD_MIDIFile/page_timing.html\n\nThe next bit is straight from https://urswilke.github.io/pyramidi/articles/compose.html, with a few minor modifications to get things working on my machine.\n\n\nShow the code\n# load system midi file to start\nmidi_file_string &lt;- system.file(\"extdata\", \"test_midi_file.mid\", package = \"pyramidi\")\n\nmfr &lt;- MidiFramer$new(midi_file_string)\n\n# set some timing params\nn_beats &lt;- 16\nticks_per_beat &lt;- 960L\n\n# construct a tibble\ndrum &lt;- tibble(\n  i_track = 0,\n  meta = FALSE,\n  # This is just a repetition of a classical rock beat:\n  note = rep(c(36, 38), n_beats / 2),\n  channel = 9,\n  i_note = 1:n_beats,\n  velocity_note_on = 100,\n  velocity_note_off = 0,\n  b_note_on = 0:(n_beats - 1),\n  b_note_off = b_note_on + 1 / 2,\n)\n\n# helper function\nbeats_to_ticks &lt;- function(notes_wide) {\n  notes_wide %&gt;%\n    mutate(\n      ticks_note_on  = b_note_on  * ticks_per_beat,\n      ticks_note_off = b_note_off * ticks_per_beat\n    )\n}\n\n# pass the tibble back to the midi df\nmfr$update_notes_wide(beats_to_ticks(drum))\n\nmfr$mf$write_file(\"testagain.mid\")\n\n# do pyramidi stuff to get the tibble back into the format it needs to be\ndf_notes_long &lt;- pivot_long_notes(mfr$df_notes_wide)\ndf_midi_out &lt;- merge_midi_frames(mfr$df_meta, mfr$df_notes_long, mfr$df_not_notes)\n\ndfc2 &lt;- df_midi_out %&gt;%\n        miditapyr$nest_midi(repair_reticulate_conversion = TRUE)\n\n# write the midi file to disk\nmiditapyr$write_midi(dfc2, ticks_per_beat, \"test_drums.mid\")\n\n# synthesize midi file to wav with fluid synth\nsystem(\"fluidsynth -F test_drums.wav ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 'test_drums.mid'\")\n\n# convert wav to mp3\nav::av_audio_convert(\"test_drums.wav\",\"test_drums.mp3\")\n\n\n[1] \"/Users/mattcrump/Github/homophony.github.io/blog/32_1_30_24_R_synth/test_drums.mp3\"\n\n\nShow the code\n# clean up and delete wav\nif(file.exists(\"test_drums.wav\")){\n  file.remove(\"test_drums.wav\")\n}\n\n\n[1] TRUE\n\n\n  \nCool, a basic rock beat.\n\nAttempting to add hi-hats, change tempo, and generally mess about.\nhopefully this percussion midi map helps https://usermanuals.finalemusic.com/SongWriter2012Win/Content/PercussionMaps.htm\nGot the hihats, changed the tempo. I’m missing something about timing, the resulting wav file is playing with silence at the end, and that seems wrong. Need to tinker some more.\nhttps://mido.readthedocs.io/en/latest/files/midi.html?highlight=set_tempo#about-the-time-attribute\nThere seems to be some issues with merge_midi_frames(). The merged dataframe may contain some out of order rows.\n\narranging by ticks gets screwed up when there are 0 ticks in meta and note dfs\n\n\n\nShow the code\n# add additional sorting by i_note, seems to solve the curent problem\nmerge_midi_frames_matt &lt;-\n  function(df_meta, df_notes_long, df_not_notes) {\n    if (is.null(df_meta) &\n        is.null(df_notes_long) & is.null(df_not_notes)) {\n      return(NULL)\n    }\n    cols_to_remove &lt;- c(\"i_note\", \"ticks\", \"t\", \"m\", \"b\")\n    \n    res &lt;- df_notes_long %&gt;%\n      dplyr::bind_rows(df_meta) %&gt;%\n      dplyr::bind_rows(df_not_notes)\n    \n    # if in tab_measures() there weren't all columns built, we have to remove them here:\n    cols_to_remove &lt;- intersect(cols_to_remove, names(res))\n    res %&gt;%\n      dplyr::arrange(.data$i_track, .data$ticks, .data$i_note) %&gt;%\n      dplyr::group_by(.data$i_track) %&gt;%\n      dplyr::mutate(time = .data$ticks - dplyr::lag(.data$ticks) %&gt;% {\n        .[1] = 0\n        .\n      }) %&gt;%\n      dplyr::ungroup() %&gt;%\n      dplyr::select(-!!cols_to_remove)\n  }\n\n\n\n\nShow the code\n# load system midi file to start\nmidi_file_string &lt;- system.file(\"extdata\", \"test_midi_file.mid\", package = \"pyramidi\")\n\nmfr &lt;- MidiFramer$new(midi_file_string)\n\n# set tempo\nBPM_to_microsecond_tempo &lt;- function(BPM) {\n  return(60000000/BPM)\n}\n\nmidi_tempo &lt;- BPM_to_microsecond_tempo(120)\n\nmfr$df_meta &lt;- mfr$df_meta %&gt;%\n  mutate(tempo = case_when(type == \"set_tempo\" ~ midi_tempo),\n         clocks_per_click = case_when(type == \"time_signature\" ~ 24),\n         b = 0) %&gt;%\n  #keep the first track\n  filter(i_track == 0)\n\n#View(mfr$df_meta)\n#mfr$ticks_per_beat\n#View(mfr$df_notes_wide)\n\n# set some timing params\nnum_bars &lt;- 1\nn_beats &lt;- num_bars*4\nsmallest_note_per_bar &lt;- 16\nmax_steps &lt;- num_bars*smallest_note_per_bar\n\nticks_per_beat &lt;- 960L\nmfr$ticks_per_beat &lt;- 960L\n\n\n# Construct a beat with hihats\ndrum &lt;- tibble(\n  i_track = 0,\n  i_note = 1:max_steps,\n  kick = rep(c(1,NA,NA,NA),4),\n  snare = rep(c(NA,NA,1,NA),4),\n  hihat_closed = rep(1,16),\n  meta = FALSE,\n  channel = 9,\n  velocity_note_on = 100,\n  velocity_note_off = 0,\n  b_note_on = 0:(max_steps - 1),\n  b_note_off = b_note_on + 1 / 2\n) %&gt;%\n  mutate(kick = kick*36,\n         snare = snare*38,\n         hihat_closed = hihat_closed*42) %&gt;%\n  pivot_longer(cols = c(\"kick\",\"snare\",\"hihat_closed\"), \n               values_to = \"note\",\n               values_drop_na = T) %&gt;%\n  mutate(b_note_on = b_note_on/(2),\n         b_note_off = b_note_off/(2))\n\ndrum$i_note &lt;- 1:dim(drum)[1]\n\n# helper function\nbeats_to_ticks &lt;- function(notes_wide) {\n  notes_wide %&gt;%\n    mutate(\n      ticks_note_on  = b_note_on  * ticks_per_beat,\n      ticks_note_off = b_note_off * ticks_per_beat\n    )\n}\n\n# pass the tibble back to the midi df\nmfr$update_notes_wide(beats_to_ticks(drum))\n\ndf_notes_long &lt;- pivot_long_notes(mfr$df_notes_wide)\ndf_not_notes &lt;- mfr$df_not_notes\n\n# do pyramidi stuff to get the tibble back into the format it needs to be\n#mfr$df_notes_long &lt;- pivot_long_notes(mfr$df_notes_wide)\n\ndf_meta &lt;- mfr$df_meta %&gt;%\n  mutate(ticks = case_when(type == \"end_of_track\" ~ (max(df_notes_long$ticks)+ticks_per_beat),\n                           type != \"end_of_track\" ~ 0))\n\n\n#mfr$df_meta &lt;- mfr$df_meta %&gt;%\n#  mutate(ticks = case_when(type == \"end_of_track\" ~ (max(mfr$df_notes_long$ticks)+ticks_per_beat)))\n\ndf_midi_out &lt;- merge_midi_frames_matt(df_meta, df_notes_long, df_not_notes)\n\ndfc2 &lt;- df_midi_out %&gt;%\n        miditapyr$nest_midi(repair_reticulate_conversion = TRUE)\n\n#########\n# bounce \n\ntrack_name &lt;- \"try_Write\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# write the midi file to disk\nmiditapyr$write_midi(dfc2, ticks_per_beat, midi_name)\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n\n[1] \"/Users/mattcrump/Github/homophony.github.io/blog/32_1_30_24_R_synth/try_Write.mp3\"\n\n\nShow the code\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\n\n\n[1] TRUE\n\n\n  \nThis works sort of. MIDI fried my brain here. For some reason the file has extra bars of silence, can’t figure out why right now. I’m missing something here about how midi time works.\n\nloaded better in musescore.\nmusescore shows two bars, but it plays through 4 bars…not sure what is going on."
  },
  {
    "objectID": "blog/32_1_30_24_R_synth/index.html#piano-keyboard-plot-with-ggplot2",
    "href": "blog/32_1_30_24_R_synth/index.html#piano-keyboard-plot-with-ggplot2",
    "title": "Midi and synthesis in R",
    "section": "Piano keyboard plot with ggplot2",
    "text": "Piano keyboard plot with ggplot2\nNeat, this is from pyramidi.\n\n\nShow the code\nlibrary(ggplot2)\n\npiano_keys_coordinates %&gt;%\n  # plot white keys first that they don't cover half of the black keys:\n  dplyr::arrange(layer) %&gt;%\n  ggplot(aes(\n    ymin = ymin,\n    ymax = ymax,\n    xmin = xmin,\n    xmax = xmax,\n    fill = factor(layer)\n  )) +\n  geom_rect(color = \"black\", show.legend = FALSE) +\n  scale_fill_manual(values = c(\"#ffffdd\", \"#113300\")) +\n  coord_fixed(ratio = 10)\n\n\n\n\n\n\n\n\n\nLet’s try plotting a chord, C7. Cool!\n\n\nShow the code\n# get midi notes from table look up\nnote_numbers &lt;- midi_defs %&gt;%\n  filter(note_name %in% c(\"C4\",\"E4\",\"G4\",\"Bb4\") == TRUE) %&gt;%\n  pull(note)\n\n# plot\npiano_keys_coordinates %&gt;%\n  # make black keys 3rd order for printing\n  mutate(layer = case_when(layer == 2 ~ 3,\n                           layer == 1 ~ 1)) %&gt;%\n  # set played keys to layer 2\n  mutate(layer = case_when(midi %in% note_numbers == TRUE ~ 2,\n                           midi %in% note_numbers == FALSE ~ layer)) %&gt;%\n  # plot white keys first that they don't cover half of the black keys:\n  dplyr::arrange(layer) %&gt;%\n  ggplot(aes(\n    ymin = ymin,\n    ymax = ymax,\n    xmin = xmin,\n    xmax = xmax,\n    fill = factor(layer)\n  )) +\n  geom_rect(color = \"black\", show.legend = FALSE) +\n  scale_fill_manual(values = c(\"#ffffdd\", \"pink\",\"#113300\")) +\n  coord_fixed(ratio = 10)"
  },
  {
    "objectID": "blog/33_2_1_24_rand_mario/index.html",
    "href": "blog/33_2_1_24_rand_mario/index.html",
    "title": "Systematically randomizing Super Mario brothers with R",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"Super mario brothers. Random, random pixels. pixel art. chaos.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nNOTE: I took some of the mp3s off because the page was loading really slowly with too many.\nIn the pursuit of MIDI knowledge I have achieved my goal of randomizing the overworld music from Super Mario Brothers.\nThis one randomly shuffles 50% of the note ON messages.\nShow the code\nlibrary(pyramidi)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\n\n#import midi using miditapyr\ntest_midi &lt;- pyramidi::miditapyr$MidiFrames(\"overworld.mid\")\n\n#import using mido\nmido_import &lt;- pyramidi::mido$MidiFile(\"overworld.mid\")\n\n# to R dataframe\ndfc &lt;- pyramidi::miditapyr$frame_midi(mido_import)\nticks_per_beat &lt;- mido_import$ticks_per_beat\n\n# unnest the dataframe\ndf &lt;- pyramidi::miditapyr$unnest_midi(dfc)\n\n# achieve master plan to systematically randomize mario brothers\n# modify midi file\n# get ids for note on messages\nids &lt;- df %&gt;%\n  mutate(add_row_id = 1:dim(df)[1]) %&gt;%\n  filter(i_track &lt; 4,\n         type == \"note_on\") %&gt;%\n  pull(add_row_id)\n\nall_notes &lt;- 1:length(ids)\nrand_proportion &lt;- .5\nids_to_swap &lt;- sample(all_notes,round(length(all_notes)*rand_proportion))\n\nfrom_id &lt;- ids[ids_to_swap]\nto_id &lt;- sample(from_id)\n\n# swap the notes\ndf[from_id,]$note &lt;- df[to_id,]$note\n\nmod_df &lt;- df\n\n# update df\ntest_midi$midi_frame_unnested$update_unnested_mf(mod_df)\n\n# write midi file\n\ntest_midi$write_file(\"mario.mid\")\n\n#########\n# bounce \n\ntrack_name &lt;- \"mario\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# write the midi file to disk\n#miditapyr$write_midi(dfc, ticks_per_beat, midi_name)\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\nNow I just need to get a nintendo soundfont and do even more weird stuff."
  },
  {
    "objectID": "blog/32_1_30_24_R_synth/index.html#miditapyr",
    "href": "blog/32_1_30_24_R_synth/index.html#miditapyr",
    "title": "Midi and synthesis in R",
    "section": "miditapyr",
    "text": "miditapyr\nhttps://miditapyr.readthedocs.io/en/latest/notebooks/midi_frame_usage.html\nLearning more about the miditapyr workflow.\n\n\nShow the code\n#import midi using miditapyr\ntest_midi &lt;- pyramidi::miditapyr$MidiFrames(\"ableton.mid\")\n\n#View(test_midi$midi_frame_unnested$df)\n\n# modify something\nmod_df &lt;- test_midi$midi_frame_unnested$df %&gt;%\n  mutate(note = case_when(note == 72 ~ 63,\n                          TRUE ~ note))\n\n# update df\ntest_midi$midi_frame_unnested$update_unnested_mf(mod_df)\n\n# write midi file\n\ntest_midi$write_file(\"ableton2.mid\")\n\n\nI made a 1 bar drum loop in ableton, using the clip view. Apparently this kind of midi file does not contain the meta set_tempo message…\n\n\nShow the code\n#import midi using miditapyr\ntest_midi &lt;- pyramidi::miditapyr$MidiFrames(\"ableton.mid\")\n\n#import using mido\nmido_import &lt;- pyramidi::mido$MidiFile(\"ableton.mid\")\n\n# to R dataframe\ndfc &lt;- pyramidi::miditapyr$frame_midi(mido_import)\nticks_per_beat &lt;- mido_import$ticks_per_beat\n\n# unnest the dataframe\ndf &lt;- pyramidi::miditapyr$unnest_midi(dfc)\n\n# add set_tempo message\n# missing from ableton midi clip\n\ndf &lt;- df %&gt;%\n  mutate(tempo = NaN) %&gt;%\n  add_row(\n    i_track = 0,\n    time = 0,\n    meta = TRUE,\n    type = \"set_tempo\",\n    tempo = 500000,\n    .before = 2\n  )\n\n\nBefore I go on, I need to do some timing tests to make sure I understand a few things.\nThe beat is 1 bar. There are 4 beats per bar. The hi hats are in 16th notes, and there are 16 hi hats. The tempo is set to 120. What happens if I play with the time column.\n\n\nShow the code\n# reload df\ndf &lt;- pyramidi::miditapyr$unnest_midi(dfc)\n\n# add set_tempo message\n# missing from ableton midi clip\n\ndf &lt;- df %&gt;%\n  mutate(tempo = NaN) %&gt;%\n  add_row(\n    i_track = 0,\n    time = 0,\n    meta = TRUE,\n    type = \"set_tempo\",\n    tempo = 500000,\n    .before = 2\n  )\n\n# futz with the time column\n# behaves as expected, yah!\ndf &lt;- df %&gt;%\n  mutate(time = time/2)\n\n# update df\ntest_midi$midi_frame_unnested$update_unnested_mf(df)\n\n# write midi file\n\ntest_midi$write_file(\"ableton2.mid\")\n\n\nThe pyramidi package has a tab_measures() function that adds additional timing information, for later use in composition and modification.\n\n\nShow the code\n# add more time info\n\ndfm &lt;- tab_measures(df = df, ticks_per_beat = ticks_per_beat)"
  },
  {
    "objectID": "blog/33_2_1_24_rand_mario/index.html#note-on-and-off",
    "href": "blog/33_2_1_24_rand_mario/index.html#note-on-and-off",
    "title": "Systematically randomizing Super Mario brothers with R",
    "section": "Note on and off",
    "text": "Note on and off\nRows in the midi file either contain note on or note off messages. There is no grouping variable for a particular note’s ON and OFF message. In the above, I only shuffled the note numbers for Note ON messages. This results in some notes having a long sustain because they are not turned OFF. Some of the notes may never be turned off, OMG that poor midi file.\nNext goal is, can I pair up the on and off messages, and re-randomize to make everything sound tighter.\nNew Function to assign a unique id to each note\n\n\nShow the code\n# A function to create unique ids for each note\n# this helps group note on and off messages\n\nget_unique_note_id &lt;- function(note_column){\n  \n  unique_note_id &lt;- rep(NaN,length(note_column))\n  unique_counter &lt;- 0\n  \n  notes_that_are_on &lt;- tibble(note_number = NA,\n                              note_id = NA)\n  \n  for (i in 1:length(note_column)){\n    \n    if(is.nan(note_column[i]) == T) {\n    # catch NaNs don't modify  \n    }\n    \n    if(is.nan(note_column[i]) == F) {\n      # get current note\n      current_note &lt;- note_column[i]\n      \n      do_once_toggle &lt;- 0\n      # if current note is in the note_on buffer, remove it\n      if(current_note %in% notes_that_are_on$note_number == TRUE){\n        \n        # assign id to column vector\n        unique_note_id[i]&lt;- notes_that_are_on %&gt;%\n          filter(note_number == current_note) %&gt;%\n          pull(note_id)\n        \n        # remove note from buffer\n        notes_that_are_on &lt;- notes_that_are_on %&gt;%\n          filter(note_number != current_note)\n        \n        do_once_toggle &lt;- 1\n      }\n      \n      # if it isn't in the note_on buffer, put it in there\n      if(do_once_toggle == 0){\n        if(current_note %in% notes_that_are_on$note_number == FALSE){\n          # increment unique note counter\n          unique_counter &lt;- unique_counter+1\n          \n          # add note on info to a new row in the buffer\n          notes_that_are_on &lt;- notes_that_are_on %&gt;%\n            add_row(note_number = current_note,\n                                 note_id = unique_counter)\n          # assign id to column vector\n          unique_note_id[i] &lt;- unique_counter\n        }\n      }\n      \n    }\n  \n  }\n  \n  return(unique_note_id)\n  \n}\n\n\nThat works and assigns a new column (to be removed later)\nNow, I should be able to shuffle note objects, including their on and off messages. This should get rid of those sustained notes from the first randomization.\nThe midi file has four tracks. The fourth track is the drum track. The first three parts are essentially mono-voice harmony. The code below takes 50% of the notes in the first track and randomly shuffles them around so they appear in different random spots in the song. It still sounds very recognizable.\n\n\nShow the code\n#import midi using miditapyr\ntest_midi &lt;- pyramidi::miditapyr$MidiFrames(\"overworld.mid\")\n\n#import using mido\nmido_import &lt;- pyramidi::mido$MidiFile(\"overworld.mid\")\n\n# to R dataframe\ndfc &lt;- pyramidi::miditapyr$frame_midi(mido_import)\nticks_per_beat &lt;- mido_import$ticks_per_beat\n\n# unnest the dataframe\ndf &lt;- pyramidi::miditapyr$unnest_midi(dfc)\n\n# isolate track 1 and add the unique note id vector\ntrack_1 &lt;- df %&gt;%\n  filter(i_track == 1) %&gt;%\n  mutate(row_id = 1:n()) %&gt;%\n  mutate(note_id = get_unique_note_id(note))\n\n# get all unique notes\npossible_notes &lt;- unique(track_1$note_id[is.nan(track_1$note_id) == F])\n\n# randomly select some proportion to shuffle\nrand_proportion &lt;- .5\nshuffled_ids &lt;- sample(possible_notes, round(length(possible_notes)*rand_proportion))\n\n# get the note names that will be shuffled\nnote_names &lt;- track_1[track_1$note_id %in% shuffled_ids == T &\n          track_1$type == 'note_on',]$note\n\n# shuffle\nnote_names &lt;- sample(note_names)\n\n# assign shuffled notes back to df for note and note off messages\ntrack_1[track_1$note_id %in% shuffled_ids == T &\n          track_1$type == 'note_on',]$note &lt;- note_names\n\ntrack_1[track_1$note_id %in% shuffled_ids == T &\n          track_1$type == 'note_off',]$note &lt;- note_names\n\ntrack_1 &lt;- track_1 %&gt;%\n  select(-row_id,-note_id)\n\n# put track 1 back into df\n\ndf[df$i_track == 1,] &lt;- track_1\n\nmod_df &lt;- df\n\n# update df\ntest_midi$midi_frame_unnested$update_unnested_mf(mod_df)\n\n# write midi file\n\ntest_midi$write_file(\"mario_B.mid\")\n\n#########\n# bounce \n\ntrack_name &lt;- \"mario_B\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# write the midi file to disk\n#miditapyr$write_midi(dfc, ticks_per_beat, midi_name)\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}"
  },
  {
    "objectID": "blog/33_2_1_24_rand_mario/index.html#randomizing-mario-even-more",
    "href": "blog/33_2_1_24_rand_mario/index.html#randomizing-mario-even-more",
    "title": "Systematically randomizing Super Mario brothers with R",
    "section": "Randomizing Mario even more",
    "text": "Randomizing Mario even more\nIt would only be fair to randomize the song in steps of equal proportion from .1 to 1.\nI’m only randomizing the note numbers within voices, which preserves the timing of all of the notes. As the shuffling proportion increases the song becomes stranger. Still sounds like mario, even with full entropy.\n\n\nShow the code\nfor(p in c(.1,.2,.3,.4,.5,.6,.7,.8,.9,1)){\n    #import midi using miditapyr\n  test_midi &lt;- pyramidi::miditapyr$MidiFrames(\"overworld.mid\")\n  \n  #import using mido\n  mido_import &lt;- pyramidi::mido$MidiFile(\"overworld.mid\")\n  \n  # to R dataframe\n  dfc &lt;- pyramidi::miditapyr$frame_midi(mido_import)\n  ticks_per_beat &lt;- mido_import$ticks_per_beat\n  \n  # unnest the dataframe\n  df &lt;- pyramidi::miditapyr$unnest_midi(dfc)\n  \n  # do shuffling for all tracks\n  for(t in c(1,2,3)){\n      # isolate track 1 and add the unique note id vector\n    track &lt;- df %&gt;%\n      filter(i_track == t) %&gt;%\n      mutate(row_id = 1:n()) %&gt;%\n      mutate(note_id = get_unique_note_id(note))\n    \n    # get all unique notes\n    possible_notes &lt;- unique(track$note_id[is.nan(track$note_id) == F])\n    \n    # randomly select some proportion to shuffle\n    rand_proportion &lt;- p\n    shuffled_ids &lt;- sample(possible_notes, round(length(possible_notes)*rand_proportion))\n    \n    # get the note names that will be shuffled\n    note_names &lt;- track[track$note_id %in% shuffled_ids == T &\n              track$type == 'note_on',]$note\n    \n    # shuffle\n    note_names &lt;- sample(note_names)\n    \n    # assign shuffled notes back to df for note and note off messages\n    track[track$note_id %in% shuffled_ids == T &\n              track$type == 'note_on',]$note &lt;- note_names\n    \n    track[track$note_id %in% shuffled_ids == T &\n              track$type == 'note_off',]$note &lt;- note_names\n    \n    track &lt;- track %&gt;%\n      select(-row_id,-note_id)\n    \n    # put track 1 back into df\n    \n    df[df$i_track == t,] &lt;- track\n  }\n  \n  \n  mod_df &lt;- df\n  \n  # update df\n  test_midi$midi_frame_unnested$update_unnested_mf(mod_df)\n  \n  # write midi file\n  iter &lt;- p*10\n \n  track_name &lt;- paste0(\"mario_\",iter)\n  midi_name &lt;- paste0(track_name,\".mid\")\n   \n  test_midi$write_file(midi_name)\n  \n  #########\n  # bounce \n  \n \n  \n  wav_name &lt;- paste0(track_name,\".wav\")\n  mp3_name &lt;- paste0(track_name,\".mp3\")\n  \n  # write the midi file to disk\n  #miditapyr$write_midi(dfc, ticks_per_beat, midi_name)\n  \n  # synthesize midi file to wav with fluid synth\n  system_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 {midi_name}')\n  system(system_command)\n  \n  # convert wav to mp3\n  av::av_audio_convert(wav_name,mp3_name)\n  \n  # clean up and delete wav\n  if(file.exists(wav_name)){\n    file.remove(wav_name)\n  }\n}\n\n\n10% shuffled notes   \n50% shuffled notes   \n100% shuffled notes"
  },
  {
    "objectID": "blog/33_2_1_24_rand_mario/index.html#really-really-random",
    "href": "blog/33_2_1_24_rand_mario/index.html#really-really-random",
    "title": "Systematically randomizing Super Mario brothers with R",
    "section": "Really Really Random",
    "text": "Really Really Random\nLet’s go a bit further into that green pipe of entropy. Now, instead of shuffling notes within each track by some proportion, let’s just choose any random note (not necessarily from the song or the key). I used the note range 30 - 90 (midi notes), and sampled from this distribution uniformly. Mario get’s even more weird now. Still even at 100% it’s still recognizably Mario…as if Mario was trying to dial into the internet from a landline.\n\n\nShow the code\nfor(p in c(.1,.5,1)){\n    #import midi using miditapyr\n  test_midi &lt;- pyramidi::miditapyr$MidiFrames(\"overworld.mid\")\n  \n  #import using mido\n  mido_import &lt;- pyramidi::mido$MidiFile(\"overworld.mid\")\n  \n  # to R dataframe\n  dfc &lt;- pyramidi::miditapyr$frame_midi(mido_import)\n  ticks_per_beat &lt;- mido_import$ticks_per_beat\n  \n  # unnest the dataframe\n  df &lt;- pyramidi::miditapyr$unnest_midi(dfc)\n  \n  # do shuffling for all tracks\n  for(t in c(1,2,3)){\n      # isolate track 1 and add the unique note id vector\n    track &lt;- df %&gt;%\n      filter(i_track == t) %&gt;%\n      mutate(row_id = 1:n()) %&gt;%\n      mutate(note_id = get_unique_note_id(note))\n    \n    # get all unique notes\n    possible_notes &lt;- unique(track$note_id[is.nan(track$note_id) == F])\n    \n    # randomly select some proportion to shuffle\n    rand_proportion &lt;- p\n    shuffled_ids &lt;- sample(possible_notes, round(length(possible_notes)*rand_proportion))\n    \n    # get the note names that will be shuffled\n    note_names &lt;- track[track$note_id %in% shuffled_ids == T &\n              track$type == 'note_on',]$note\n    \n    # shuffle\n    note_names &lt;- sample(30:90,length(note_names), replace = T)\n    \n    # assign shuffled notes back to df for note and note off messages\n    track[track$note_id %in% shuffled_ids == T &\n              track$type == 'note_on',]$note &lt;- note_names\n    \n    track[track$note_id %in% shuffled_ids == T &\n              track$type == 'note_off',]$note &lt;- note_names\n    \n    track &lt;- track %&gt;%\n      select(-row_id,-note_id)\n    \n    # put track 1 back into df\n    \n    df[df$i_track == t,] &lt;- track\n  }\n  \n  \n  mod_df &lt;- df\n  \n  # update df\n  test_midi$midi_frame_unnested$update_unnested_mf(mod_df)\n  \n  # write midi file\n  iter &lt;- p*10\n \n  track_name &lt;- paste0(\"mario_R\",iter)\n  midi_name &lt;- paste0(track_name,\".mid\")\n   \n  test_midi$write_file(midi_name)\n  \n  #########\n  # bounce \n  \n \n  \n  wav_name &lt;- paste0(track_name,\".wav\")\n  mp3_name &lt;- paste0(track_name,\".mp3\")\n  \n  # write the midi file to disk\n  #miditapyr$write_midi(dfc, ticks_per_beat, midi_name)\n  \n  # synthesize midi file to wav with fluid synth\n  system_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 {midi_name}')\n  system(system_command)\n  \n  # convert wav to mp3\n  av::av_audio_convert(wav_name,mp3_name)\n  \n  # clean up and delete wav\n  if(file.exists(wav_name)){\n    file.remove(wav_name)\n  }\n}\n\n\n10% random notes   \n50% random notes   \n100% random notes"
  },
  {
    "objectID": "blog/33_2_1_24_rand_mario/index.html#trying-another-soundfont",
    "href": "blog/33_2_1_24_rand_mario/index.html#trying-another-soundfont",
    "title": "Systematically randomizing Super Mario brothers with R",
    "section": "Trying another soundfont",
    "text": "Trying another soundfont\nSo far I have only used a general style soundfont. Let’s see how easy it is to get a nintendo sound.\nhttps://musical-artifacts.com/artifacts/610\nPretty easy, nice.\n\n\nShow the code\n  track_name &lt;- \"overworld\"\n  midi_name &lt;- paste0(track_name,\".mid\")\n   \n  #########\n  # bounce \n\n  wav_name &lt;- paste0(track_name,\".wav\")\n  mp3_name &lt;- paste0(track_name,\".mp3\")\n  \n  # write the midi file to disk\n  #miditapyr$write_midi(dfc, ticks_per_beat, midi_name)\n  \n  # synthesize midi file to wav with fluid synth\n  system_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/nintendo_soundfont.sf2 {midi_name}')\n  system(system_command)\n  \n  # convert wav to mp3\n  av::av_audio_convert(wav_name,mp3_name)\n  \n  # clean up and delete wav\n  if(file.exists(wav_name)){\n    file.remove(wav_name)\n  }"
  },
  {
    "objectID": "blog/33_2_1_24_rand_mario/index.html#more-random-notes-with-the-mario-sound",
    "href": "blog/33_2_1_24_rand_mario/index.html#more-random-notes-with-the-mario-sound",
    "title": "Systematically randomizing Super Mario brothers with R",
    "section": "More random notes with the mario sound",
    "text": "More random notes with the mario sound\n\n\nShow the code\nfor(p in c(.1,.5,1)){\n    #import midi using miditapyr\n  test_midi &lt;- pyramidi::miditapyr$MidiFrames(\"overworld.mid\")\n  \n  #import using mido\n  mido_import &lt;- pyramidi::mido$MidiFile(\"overworld.mid\")\n  \n  # to R dataframe\n  dfc &lt;- pyramidi::miditapyr$frame_midi(mido_import)\n  ticks_per_beat &lt;- mido_import$ticks_per_beat\n  \n  # unnest the dataframe\n  df &lt;- pyramidi::miditapyr$unnest_midi(dfc)\n  \n  # do shuffling for all tracks\n  for(t in c(1,2,3)){\n      # isolate track 1 and add the unique note id vector\n    track &lt;- df %&gt;%\n      filter(i_track == t) %&gt;%\n      mutate(row_id = 1:n()) %&gt;%\n      mutate(note_id = get_unique_note_id(note))\n    \n    # get all unique notes\n    possible_notes &lt;- unique(track$note_id[is.nan(track$note_id) == F])\n    \n    # randomly select some proportion to shuffle\n    rand_proportion &lt;- p\n    shuffled_ids &lt;- sample(possible_notes, round(length(possible_notes)*rand_proportion))\n    \n    # get the note names that will be shuffled\n    note_names &lt;- track[track$note_id %in% shuffled_ids == T &\n              track$type == 'note_on',]$note\n    \n    # shuffle\n    note_names &lt;- sample(30:90,length(note_names), replace = T)\n    \n    # assign shuffled notes back to df for note and note off messages\n    track[track$note_id %in% shuffled_ids == T &\n              track$type == 'note_on',]$note &lt;- note_names\n    \n    track[track$note_id %in% shuffled_ids == T &\n              track$type == 'note_off',]$note &lt;- note_names\n    \n    track &lt;- track %&gt;%\n      select(-row_id,-note_id)\n    \n    # put track 1 back into df\n    \n    df[df$i_track == t,] &lt;- track\n  }\n  \n  \n  mod_df &lt;- df\n  \n  # update df\n  test_midi$midi_frame_unnested$update_unnested_mf(mod_df)\n  \n  # write midi file\n  iter &lt;- p*10\n \n  track_name &lt;- paste0(\"mario_NES\",iter)\n  midi_name &lt;- paste0(track_name,\".mid\")\n   \n  test_midi$write_file(midi_name)\n  \n  #########\n  # bounce \n  \n \n  \n  wav_name &lt;- paste0(track_name,\".wav\")\n  mp3_name &lt;- paste0(track_name,\".mp3\")\n  \n  # write the midi file to disk\n  #miditapyr$write_midi(dfc, ticks_per_beat, midi_name)\n  \n  # synthesize midi file to wav with fluid synth\n  system_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/nintendo_soundfont.sf2 {midi_name}')\n  system(system_command)\n  \n  # convert wav to mp3\n  av::av_audio_convert(wav_name,mp3_name)\n  \n  # clean up and delete wav\n  if(file.exists(wav_name)){\n    file.remove(wav_name)\n  }\n}\n\n\n  \n  \n  \nNow my princess really is in another castle."
  },
  {
    "objectID": "blog/35_2_3_24_matrix_midi/index.html",
    "href": "blog/35_2_3_24_matrix_midi/index.html",
    "title": "Midi to matrix representation and probabilistic super mario music with R",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"super mario enters the matrix. mario matrix. feature vector encoding. binary representation. pixel art.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nFor various reasons I’d like to convert midi into a matrix format and back again. In this example, I represent each bar in the midi file in a note x time step matrix. The matrix for each bar is concatenated into a long vector, and the whole song is represented as a bar x feature vector matrix. I then calculate the column sums, and divide by the total to get probabilities for each note x time cell. These probabilities can be used to generate new bars, by randomly creating notes in time as a function of those probabilities. After making new bars, I work backwards to generate midi files from the matrix representations to listen to everything.\nI used the mario overworld midi file, and this yields some probabilistic version of the overworld theme.\nHere’s a picture of roughly what’s happening:"
  },
  {
    "objectID": "blog/35_2_3_24_matrix_midi/index.html#all-voices",
    "href": "blog/35_2_3_24_matrix_midi/index.html#all-voices",
    "title": "Midi to matrix representation and probabilistic super mario music with R",
    "section": "all voices",
    "text": "all voices\n\n\nShow the code\nlibrary(pyramidi)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\n\n\n\n\nShow the code\n#import midi using miditapyr\ntest_midi &lt;- pyramidi::miditapyr$MidiFrames(\"all_overworld.mid\")\n\n#import using mido\nmido_import &lt;- pyramidi::mido$MidiFile(\"all_overworld.mid\")\n\n# to R dataframe\ndfc &lt;- pyramidi::miditapyr$frame_midi(mido_import)\nticks_per_beat &lt;- mido_import$ticks_per_beat\n\n# unnest the dataframe\ndf &lt;- pyramidi::miditapyr$unnest_midi(dfc)\n\n##############################################\n# convert to matrix\n\n# grab track 1\ntrack_1 &lt;- df %&gt;%\n  filter(i_track == 0,\n         type %in% c(\"note_on\",\"note_off\") ==TRUE) %&gt;%\n  mutate(total_time = cumsum(time)) %&gt;%\n  filter(type == \"note_on\")\n\ntime_steps &lt;- seq(0,max(track_1$total_time),8)\ntotal_bars &lt;- round(length(time_steps)/48)+1\nbars &lt;- rep(1:total_bars,each=48)\nbar_steps &lt;- rep(1:48,total_bars)\n\nmetric_tibble &lt;- tibble(time_steps = time_steps,\n                        bars = bars[1:length(time_steps)],\n                        bar_steps =bar_steps[1:length(time_steps)])\n\ntrack_1 &lt;- track_1 %&gt;%\n  mutate(time_markers = 0,\n         bars = 0,\n         bar_steps = 0) \n\nfor(i in 1:dim(track_1)[1]){\n  track_1$time_markers[i] &lt;- which(time_steps %in% track_1$total_time[i])\n}\n\n# get bar divisions, add them to track_1\n\nfor(i in 1:dim(track_1)[1]){\n  get_timestep &lt;- time_steps[track_1$time_markers[i]]\n  track_1$bars[i] &lt;- metric_tibble %&gt;%\n    filter(time_steps == get_timestep) %&gt;%\n    pull(bars)\n  track_1$bar_steps[i] &lt;- metric_tibble %&gt;%\n    filter(time_steps == get_timestep) %&gt;%\n    pull(bar_steps)\n}\n\n# assign intervals to the bars\n\nmusic_matrix &lt;- matrix(0,\n                       ncol = (48+128+(48*128)),\n                       nrow = max(track_1$bars)\n)\n\nfor(i in 1:max(track_1$bars)){\n  \n  bar_midi &lt;- track_1%&gt;%\n    filter(bars == i)\n  \n  one_bar &lt;- matrix(0,\n                  nrow=dim(pyramidi::midi_defs)[1],\n                  ncol=48)\n  \n  for(j in 1:dim(one_bar)[1]){\n    one_bar[bar_midi$note[j],bar_midi$bar_steps[j]] &lt;- 1\n  }\n  \n  \n  pitch_vector &lt;- rowSums(one_bar)\n  time_vector &lt;- colSums(one_bar)\n  pitch_by_time &lt;- c(one_bar)\n\n  #concatenate_vector\n  music_vector &lt;- c(pitch_vector,time_vector,pitch_by_time)\n  \n  music_matrix[i,] &lt;- music_vector\n  \n}\n\n#########################################\n\n# compose midi tibble\nall_midi_bars &lt;- df[1,]\nall_midi_bars &lt;- all_midi_bars[-1,]\n\nfor(i in 1:32){\n\nsum_music &lt;- colSums(music_matrix)\nsum_music &lt;- sum_music[(48+128+1):length(sum_music)]\nprob_music &lt;- sum_music/sum(sum_music)\n\n# figure out average number of notes\n#mean(rowSums(music_matrix[,(48+128+1):dim(music_matrix)[2]]))\n\n# note size parameter controls note density in the bar\n# 24 is about the average from the song\nprob_bar &lt;- rbinom(length(sum_music),size = 48,prob_music)\nprob_bar[prob_bar &gt; 1] &lt;- 1\n#sum(prob_bar)\n#plot(prob_bar)\n\n#reconstitute matrix\none_bar_matrix &lt;- matrix(prob_bar,\n                         nrow=128,\n                         ncol=48,\n                         byrow=F)\n\n#filter for notes and times\nnote_times &lt;- which(one_bar_matrix == 1, arr.ind=T)\ncolnames(note_times) &lt;- c(\"note_num\",\"bar_step\")\n\n# convert back to midi time in ticks\nnote_times &lt;- as_tibble(note_times)\nnote_times &lt;- note_times %&gt;%\n  mutate(ticks = (bar_step *8) -8,\n         note_id = 1:n(),\n         note_on = ticks,\n         note_off = ticks+8) %&gt;%\n  pivot_longer(cols = c(\"note_on\",\"note_off\"), \n               names_to = \"type\",\n               values_to = \"cumulative_ticks\") %&gt;%\n  arrange(cumulative_ticks) %&gt;%\n  mutate(time = cumulative_ticks - lag(cumulative_ticks, default = 0))\n\nmidi_bar &lt;- df[1,]\nmidi_bar &lt;- midi_bar[-1,]\n\nmidi_bar &lt;- midi_bar %&gt;%\n  add_row(type = note_times$type,\n          time = note_times$time,\n          note = note_times$note_num) %&gt;%\n  mutate(\n    i_track = 0,\n    meta = FALSE,\n    numerator = NaN,\n    denominator = NaN,\n    clocks_per_click = NaN,\n    notated_32nd_notes_per_beat = NaN,\n    tempo = NaN,\n    name = NA,\n    control = NaN,\n    value = NaN,\n    channel = 0,\n    program = NaN,\n    velocity = 64\n         )\n\nall_midi_bars &lt;- rbind(all_midi_bars,midi_bar)\n\n}\n\n# get meta messages\nmeta_midi  &lt;- df %&gt;% \n  filter(meta ==TRUE,\n         i_track == 0) %&gt;%\n  mutate(tempo = NaN) %&gt;%\n  add_row(\n    i_track = 0,\n    time = 0,\n    meta = TRUE,\n    type = \"set_tempo\",\n    tempo = 500000,\n    .before = 2\n  )\n\n# merge bar midi into full midi file\nmidi_track &lt;- meta_midi %&gt;%\n  add_row(all_midi_bars,.after = 3)\n\n##########################\n# bounce\n\nmod_df &lt;- midi_track\n\n# update df\ntest_midi$midi_frame_unnested$update_unnested_mf(mod_df)\n\n# write midi file\n\ntest_midi$write_file(\"mario_P48_allbar.mid\")\n\n#########\n# bounce \n\ntrack_name &lt;- \"mario_P48_allbar\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# write the midi file to disk\n#miditapyr$write_midi(dfc, ticks_per_beat, midi_name)\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/nintendo_soundfont.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\n\n\nI used the rbinom() function to randomly generate notes based on estimates of note x time feature probabilities. It’s possible to control the amount of notes generated using the size parameter. The following examples have three levels of note density, from sparse to thick.\nAbout 12 notes/bar:   \nAbout 24 notes/bar:   \nAbout 48 notes/bar:"
  },
  {
    "objectID": "blog/35_2_3_24_matrix_midi/index.html#to-do",
    "href": "blog/35_2_3_24_matrix_midi/index.html#to-do",
    "title": "Midi to matrix representation and probabilistic super mario music with R",
    "section": "to do",
    "text": "to do\nlots of stuff to clean up here. currently bespoke."
  },
  {
    "objectID": "blog/36_2_4_24_heart_soul/index.html",
    "href": "blog/36_2_4_24_heart_soul/index.html",
    "title": "Messing around with heart and soul",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"heart and soul piano duet. probabilities. linear algebra. markov chain. thundercats cartoon.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nI’m following similar steps from this post, but with my own noodling around playing heart and soul on piano.\nNotes:"
  },
  {
    "objectID": "blog/36_2_4_24_heart_soul/index.html#try-stuff",
    "href": "blog/36_2_4_24_heart_soul/index.html#try-stuff",
    "title": "Messing around with heart and soul",
    "section": "Try stuff",
    "text": "Try stuff\n\nticks per beat is 96\ngoing with temporal units down to smallest tick (1).\ngot it working, ya\ngenerate 1 minute of heart and soul nonsense with nintendo synth sound for fun\n\n\n\nShow the code\nlibrary(pyramidi)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\n\n\n\n\nShow the code\n#import midi using miditapyr\ntest_midi &lt;- pyramidi::miditapyr$MidiFrames(\"heart_soul_103_185b.mid\")\n\n#import using mido\nmido_import &lt;- pyramidi::mido$MidiFile(\"heart_soul_103_185b.mid\")\n\n# to R dataframe\ndfc &lt;- pyramidi::miditapyr$frame_midi(mido_import)\nticks_per_beat &lt;- mido_import$ticks_per_beat\n\n# unnest the dataframe\ndf &lt;- pyramidi::miditapyr$unnest_midi(dfc)\n\n##############################################\n# convert to matrix\n\n# grab track 1\ntrack_1 &lt;- df %&gt;%\n  filter(i_track == 0,\n         type %in% c(\"note_on\",\"note_off\") ==TRUE) %&gt;%\n  mutate(total_time = cumsum(time)) %&gt;%\n  filter(type == \"note_on\")\n\ntime_steps &lt;- seq(0,max(track_1$total_time),1)\ntotal_bars &lt;- round(length(time_steps)/(96*4))+1\nbars &lt;- rep(1:total_bars,each=(96*4))\nbar_steps &lt;- rep(1:(96*4),total_bars)\n\nmetric_tibble &lt;- tibble(time_steps = time_steps,\n                        bars = bars[1:length(time_steps)],\n                        bar_steps =bar_steps[1:length(time_steps)])\n\ntrack_1 &lt;- track_1 %&gt;%\n  mutate(time_markers = 0,\n         bars = 0,\n         bar_steps = 0) \n\nfor(i in 1:dim(track_1)[1]){\n  track_1$time_markers[i] &lt;- which(time_steps %in% track_1$total_time[i])\n}\n\n# get bar divisions, add them to track_1\n\nfor(i in 1:dim(track_1)[1]){\n  get_timestep &lt;- time_steps[track_1$time_markers[i]]\n  track_1$bars[i] &lt;- metric_tibble %&gt;%\n    filter(time_steps == get_timestep) %&gt;%\n    pull(bars)\n  track_1$bar_steps[i] &lt;- metric_tibble %&gt;%\n    filter(time_steps == get_timestep) %&gt;%\n    pull(bar_steps)\n}\n\n# assign intervals to the bars\n\nmusic_matrix &lt;- matrix(0,\n                       ncol = ((96*4)+128+((96*4)*128)),\n                       nrow = max(track_1$bars)\n)\n\nfor(i in 1:max(track_1$bars)){\n  \n  bar_midi &lt;- track_1%&gt;%\n    filter(bars == i)\n  \n  one_bar &lt;- matrix(0,\n                  nrow=dim(pyramidi::midi_defs)[1],\n                  ncol=(96*4))\n  \n  for(j in 1:dim(one_bar)[1]){\n    one_bar[bar_midi$note[j],bar_midi$bar_steps[j]] &lt;- 1\n  }\n  \n  \n  pitch_vector &lt;- rowSums(one_bar)\n  time_vector &lt;- colSums(one_bar)\n  pitch_by_time &lt;- c(one_bar)\n\n  #concatenate_vector\n  music_vector &lt;- c(pitch_vector,time_vector,pitch_by_time)\n  \n  music_matrix[i,] &lt;- music_vector\n  \n}\n\n#########################################\n\n# compose midi tibble\nall_midi_bars &lt;- df[1,]\nall_midi_bars &lt;- all_midi_bars[-1,]\n\nfor(i in 1:32){\n\nsum_music &lt;- colSums(music_matrix)\nsum_music &lt;- sum_music[((96*4)+128+1):length(sum_music)]\nprob_music &lt;- sum_music/sum(sum_music)\n\n# figure out average number of notes\n#mean(rowSums(music_matrix[,((96*4)+128+1):dim(music_matrix)[2]]))\n\n# note size parameter controls note density in the bar\n# 16 is about the average from the song\nprob_bar &lt;- rbinom(length(sum_music),size = 32,prob_music)\nprob_bar[prob_bar &gt; 1] &lt;- 1\n#sum(prob_bar)\n#plot(prob_bar)\n\n#reconstitute matrix\none_bar_matrix &lt;- matrix(prob_bar,\n                         nrow=128,\n                         ncol=(96*4),\n                         byrow=F)\n\n#filter for notes and times\nnote_times &lt;- which(one_bar_matrix == 1, arr.ind=T)\ncolnames(note_times) &lt;- c(\"note_num\",\"bar_step\")\n\n# convert back to midi time in ticks\nnote_times &lt;- as_tibble(note_times)\nnote_times &lt;- note_times %&gt;%\n  mutate(ticks = (bar_step *1) -1,\n         note_id = 1:n(),\n         note_on = ticks,\n         note_off = ticks+32) %&gt;%\n  pivot_longer(cols = c(\"note_on\",\"note_off\"), \n               names_to = \"type\",\n               values_to = \"cumulative_ticks\") %&gt;%\n  arrange(cumulative_ticks) %&gt;%\n  mutate(time = cumulative_ticks - lag(cumulative_ticks, default = 0))\n\nmidi_bar &lt;- df[1,]\nmidi_bar &lt;- midi_bar[-1,]\n\nmidi_bar &lt;- midi_bar %&gt;%\n  add_row(type = note_times$type,\n          time = note_times$time,\n          note = note_times$note_num) %&gt;%\n  mutate(\n    i_track = 0,\n    meta = FALSE,\n    numerator = NaN,\n    denominator = NaN,\n    clocks_per_click = NaN,\n    notated_32nd_notes_per_beat = NaN,\n    tempo = NaN,\n    name = NA,\n    control = NaN,\n    value = NaN,\n    channel = 0,\n    velocity = 64\n         )\n\nall_midi_bars &lt;- rbind(all_midi_bars,midi_bar)\n\n}\n\n# get meta messages\nmeta_midi  &lt;- df %&gt;% \n  filter(meta ==TRUE,\n         i_track == 0) %&gt;%\n  mutate(tempo = NaN) %&gt;%\n  add_row(\n    i_track = 0,\n    time = 0,\n    meta = TRUE,\n    type = \"set_tempo\",\n    tempo = 500000,\n    .before = 2\n  )\n\n# merge bar midi into full midi file\nmidi_track &lt;- meta_midi %&gt;%\n  add_row(all_midi_bars,.after = 3)\n\n##########################\n# bounce\n\nmod_df &lt;- midi_track\n\n# update df\ntest_midi$midi_frame_unnested$update_unnested_mf(mod_df)\n\n# write midi file\n\ntest_midi$write_file(\"heart_soul_A.mid\")\n\n#########\n# bounce \n\ntrack_name &lt;- \"heart_soul_A\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# write the midi file to disk\n#miditapyr$write_midi(dfc, ticks_per_beat, midi_name)\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/nintendo_soundfont.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\n\n\n  \nrendered using a nintendo soundfont to increase the ridiculousness of this. It is approaching “modem-connecting-to-the-internet-sounds”, but too melodic."
  },
  {
    "objectID": "blog/36_2_4_24_heart_soul/index.html#thoughts",
    "href": "blog/36_2_4_24_heart_soul/index.html#thoughts",
    "title": "Messing around with heart and soul",
    "section": "thoughts",
    "text": "thoughts\nThe main goal for today was to find out how generalizable my previous code would be to a midi file that I recorded myself playing piano.\nI learned that ableton produced midi files with no tempo, so I need to set that myself. Ableton’s midi files have 96 ticks per beat, at least mine do, I wonder if I can change that.\nI played the piano duet heart and soul to a metronome set at 103 BPM. I was pretty messy with the timing, but played stuff all over the keyboard for about 184 bars.\nI did not quantize the midi file. The 96 ticks per bar resolution is not great at all, especially in terms of capturing live music performance, so a lot of the rhythm is really messed up (also sloppy playing, whatever). Nevertheless, I ended up with a matrix of 128 notes by (96x4) ticks, for each bar of music.\nThis is all very cool in terms of getting some of my own playing into a matrix form.\nThe mp3 above is generated randomly from the note x tick probabilities that I calculated from my overall corpus of bars of notes that I played. It sounds like clown music, ha. At the present time, what is happening is that every note is treated completely independently from each other in terms of probabilistic sampling. It’s very busy, and not particularly musical, still mildly interesting. I wonder what this would sound like with way more notes, until it approaches something like white noise. hmmm.\nThere are a whole bunch more things to try to wrestle these probabilities into something more interesting. But, that’s for another day."
  },
  {
    "objectID": "blog/37_2_5_24_matrix_analysis/index.html",
    "href": "blog/37_2_5_24_matrix_analysis/index.html",
    "title": "MIDI analysis, bags of notes, and probabilistic generation",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"cartoon musical notes in bags. Scrooge mcduck hoarding bags of musical notes. A huge room with a pile of musical notes. ducktales.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nOver the past few posts I’ve been exploring probabilistic MIDI generation derived from note probabilities in existing MIDI files, using R.\nAside from having a bit of fun with this, I have a musical interest in finding out whether this approach can be inspiring to my own playing somehow, and in terms of my day job as a cognition researcher, I have an interest in pushing my particular method of choice into this arena as a way to understand the method from a new perspective. This post is mainly notes to self along with some R code."
  },
  {
    "objectID": "blog/37_2_5_24_matrix_analysis/index.html#bags-of-notes",
    "href": "blog/37_2_5_24_matrix_analysis/index.html#bags-of-notes",
    "title": "MIDI analysis, bags of notes, and probabilistic generation",
    "section": "Bags of notes",
    "text": "Bags of notes\nI’m representing midi files in one bar increments using a technique borrowed from language processing and semantic cognition research, sometimes called “bags of words” (Landauer and Dumais 1997). The technique represents chunks of written language as an n-gram frequency vector. For example, take a book and find all of the unique words in the book. Create a matrix with the same number of columns as unique words in the book. Split the book up into “bags”, which is a collection of words of some size. For example, the bag size could be at the sentence level or paragraph level. Let’s say paragraphs. For each paragraph count how many times each word appears in the paragraph. Finally, create a frequency count vector for the paragraph that has zeros for all words not found in the paragraph, and the individual frequency counts for all words in the paragraph. Repeat as necessary for all bags of words. This procedure creates a document x term matrix that represents aspects of how words co-occur with other words in natural language. I’ve been doing something similar with midi files.\nAs this isn’t an academic paper, I won’t review much more literature, but suffice it to say, several variations on the “bags of words” approach have been used for MIDI representation and analysis, especially to aid MIDI similarity analysis.\nMy current ’bags of notes” representation is as follows.\n\nSplit a midi file into bars\nChoose a unit for temporal resolution (e.g., quarter notes, eighth notes, down to the smallest possible representation, midi ticks).\nCreate a note by time matrix. If the midi file has 96 ticks per beat, then the matrix will have 128 rows (for each possible midi note 0-127), and 96 x 4 = 384 columns.\nFor each note occurrence in a bar, assign a 1 to the “note on” location in the matrix. Otherwise, assign a 0.\n\nThis allows a midi file to be represented as “bags of notes” in successive note x time frequency matrices. Although I am presently disregarding note length, the resulting matrix contains all of the information necessary to reconstruct the note on messages back into MIDI format.\nI have some background modeling aims that involve training theories of learning and memory on patterns from MIDI files, so I’m also choosing representations that would work for those models. In this case, I am most interested in training a MINERVA-II style model on MIDI patterns (Hintzman 1984). That model represents patterns in individual memory trace as feature vectors. I’d like to put bars of music as individual patterns into the model, and to do that I concatenate the note x time matrix into a single feature vector.\nI’ve decided to proceed by examples instead."
  },
  {
    "objectID": "blog/37_2_5_24_matrix_analysis/index.html#mario-example",
    "href": "blog/37_2_5_24_matrix_analysis/index.html#mario-example",
    "title": "MIDI analysis, bags of notes, and probabilistic generation",
    "section": "Mario example",
    "text": "Mario example\nIn [this previous post](https://homophony.quest/blog/35_2_3_24_matrix_midi/) I imported the Super Mario overworld music, represented it in the above matrix format, and then generated probabilistic versions of the music. My plan here is to walk through the example in a little bit more detail, and then consider methods for potentially improving the musicality of generated sequences.\nThe following code reads in the overworld.mid file, and converts it to a matrix. Let’s take a closer look.\n\n\nShow the code\nlibrary(pyramidi)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\n\n\n\n\nShow the code\n#import midi using miditapyr\ntest_midi &lt;- pyramidi::miditapyr$MidiFrames(\"all_overworld.mid\")\n\n#import using mido\nmido_import &lt;- pyramidi::mido$MidiFile(\"all_overworld.mid\")\n\n# to R dataframe\ndfc &lt;- pyramidi::miditapyr$frame_midi(mido_import)\nticks_per_beat &lt;- mido_import$ticks_per_beat\n\n# unnest the dataframe\ndf &lt;- pyramidi::miditapyr$unnest_midi(dfc)\n\n##############################################\n# convert to matrix\n\n# grab track 1\ntrack_1 &lt;- df %&gt;%\n  filter(i_track == 0,\n         type %in% c(\"note_on\",\"note_off\") ==TRUE) %&gt;%\n  mutate(total_time = cumsum(time)) %&gt;%\n  filter(type == \"note_on\")\n\ntime_steps &lt;- seq(0,max(track_1$total_time),8)\ntotal_bars &lt;- round(length(time_steps)/48)+1\nbars &lt;- rep(1:total_bars,each=48)\nbar_steps &lt;- rep(1:48,total_bars)\n\nmetric_tibble &lt;- tibble(time_steps = time_steps,\n                        bars = bars[1:length(time_steps)],\n                        bar_steps =bar_steps[1:length(time_steps)])\n\ntrack_1 &lt;- track_1 %&gt;%\n  mutate(time_markers = 0,\n         bars = 0,\n         bar_steps = 0) \n\nfor(i in 1:dim(track_1)[1]){\n  track_1$time_markers[i] &lt;- which(time_steps %in% track_1$total_time[i])\n}\n\n# get bar divisions, add them to track_1\n\nfor(i in 1:dim(track_1)[1]){\n  get_timestep &lt;- time_steps[track_1$time_markers[i]]\n  track_1$bars[i] &lt;- metric_tibble %&gt;%\n    filter(time_steps == get_timestep) %&gt;%\n    pull(bars)\n  track_1$bar_steps[i] &lt;- metric_tibble %&gt;%\n    filter(time_steps == get_timestep) %&gt;%\n    pull(bar_steps)\n}\n\n# assign intervals to the bars\n\nmusic_matrix &lt;- matrix(0,\n                       ncol = (48+128+(48*128)),\n                       nrow = max(track_1$bars)\n)\n\nfor(i in 1:max(track_1$bars)){\n  \n  bar_midi &lt;- track_1%&gt;%\n    filter(bars == i)\n  \n  one_bar &lt;- matrix(0,\n                  nrow=dim(pyramidi::midi_defs)[1],\n                  ncol=48)\n  \n  for(j in 1:dim(one_bar)[1]){\n    one_bar[bar_midi$note[j],bar_midi$bar_steps[j]] &lt;- 1\n  }\n  \n  \n  pitch_vector &lt;- rowSums(one_bar)\n  time_vector &lt;- colSums(one_bar)\n  pitch_by_time &lt;- c(one_bar)\n\n  #concatenate_vector\n  music_vector &lt;- c(pitch_vector,time_vector,pitch_by_time)\n  \n  music_matrix[i,] &lt;- music_vector\n  \n}\n\n\n\nThe first bar of the midi file\nThe table shows a tibble of the first bar. I’ve added a few columns to code which bars the notes are occurring (not represented in a midi file), along with when each note occurs inside the bar. In this case, I’m using 8 ticks as the smallest unit of time inside a bar.\n\n\nShow the code\nfirst_bar &lt;- track_1 %&gt;%\n  filter(bars == 1)\nknitr::kable(first_bar)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ni_track\nmeta\ntype\nname\ntime\nnumerator\ndenominator\nclocks_per_click\nnotated_32nd_notes_per_beat\nprogram\nchannel\ncontrol\nvalue\nnote\nvelocity\ntotal_time\ntime_markers\nbars\nbar_steps\n\n\n\n\n0\nFALSE\nnote_on\nNA\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n50\n64\n0\n1\n1\n1\n\n\n0\nFALSE\nnote_on\nNA\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n66\n64\n0\n1\n1\n1\n\n\n0\nFALSE\nnote_on\nNA\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n76\n64\n0\n1\n1\n1\n\n\n0\nFALSE\nnote_on\nNA\n8\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n50\n64\n24\n4\n1\n4\n\n\n0\nFALSE\nnote_on\nNA\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n66\n64\n24\n4\n1\n4\n\n\n0\nFALSE\nnote_on\nNA\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n76\n64\n24\n4\n1\n4\n\n\n0\nFALSE\nnote_on\nNA\n32\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n50\n64\n72\n10\n1\n10\n\n\n0\nFALSE\nnote_on\nNA\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n66\n64\n72\n10\n1\n10\n\n\n0\nFALSE\nnote_on\nNA\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n76\n64\n72\n10\n1\n10\n\n\n0\nFALSE\nnote_on\nNA\n32\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n50\n64\n120\n16\n1\n16\n\n\n0\nFALSE\nnote_on\nNA\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n66\n64\n120\n16\n1\n16\n\n\n0\nFALSE\nnote_on\nNA\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n72\n64\n120\n16\n1\n16\n\n\n0\nFALSE\nnote_on\nNA\n8\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n50\n64\n144\n19\n1\n19\n\n\n0\nFALSE\nnote_on\nNA\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n66\n64\n144\n19\n1\n19\n\n\n0\nFALSE\nnote_on\nNA\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n76\n64\n144\n19\n1\n19\n\n\n0\nFALSE\nnote_on\nNA\n32\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n67\n64\n192\n25\n1\n25\n\n\n0\nFALSE\nnote_on\nNA\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n71\n64\n192\n25\n1\n25\n\n\n0\nFALSE\nnote_on\nNA\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n79\n64\n192\n25\n1\n25\n\n\n0\nFALSE\nnote_on\nNA\n80\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n55\n64\n288\n37\n1\n37\n\n\n0\nFALSE\nnote_on\nNA\n0\nNaN\nNaN\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n67\n64\n288\n37\n1\n37\n\n\n\n\n\n\n\nBags of notes representation\nThe big code chunk has a loop that processes all bars into a vector representation. Below, I unpack this and describe each step.\nThis code chunk takes the notes from a midi tibble that pertain to a single bar (e.g., the table above), creates an empty note by time interval matrix, and then assigns the notes in the bar to their respective intervals.\n\n\nShow the code\n# take a bar from the tibble\nbar_midi &lt;- first_bar\n\n# make a note by time unit matrix\none_bar &lt;- matrix(0,\n                  nrow = dim(pyramidi::midi_defs)[1],\n                  ncol = 48)\n\n# assign 1s to note locations in time\nfor (j in 1:dim(one_bar)[1]) {\n  one_bar[bar_midi$note[j], bar_midi$bar_steps[j]] &lt;- 1\n}\n\n\nThe note x time matrix is 128 rows by 48 columns. There are 0-127 possible notes in MIDI, and I divided one bar (4/4 time) into 48 individual slices of time. The note occurrences in the first bar of mario are coded as 1s in the matrix.\nHere is a plot of the matrix showing note locations. It looks like a typical piano roll.\n\n\nShow the code\nlibrary(plot.matrix)\nplot(one_bar)\n\n\n\n\n\n\n\n\n\n\n\nCounting and concatenating\nI create three vectors that will be appended together as the final feature vector for a bar of music.\nThe first two are frequency summarizers. The pitch vector takes the sum across the rows of the matrix. This yields a single vector with 128 elements corresponding to each note. The values are simply the counts of each note in the bar. The time vector takes the column sums, and provides a count of each temporal unit occurrence in the matrix. These summary representations will be useful later on for conditionalizing sequence generation based on notes and time in general.\nThe last vector concatenates the matrix into a single vector by starting at the first column, and then appending column after column, until the matrix is one long vector.\nFinally, I append the pitch vector, the time vector, and the pitch x time vector into a single vector, called the music vector.\n\n\nShow the code\npitch_vector &lt;- rowSums(one_bar)\ntime_vector &lt;- colSums(one_bar)\npitch_by_time &lt;- c(one_bar)\n\n#concatenate_vector\nmusic_vector &lt;- c(pitch_vector, time_vector, pitch_by_time)\n\n\n\n\nThe music matrix\nThe above steps are applied to all of the bars in a midi file, and the resulting music feature vector is added to a matrix. For mario, I get a 37 (bars) by 6320 (features) matrix.\nOooh, and it can be plotted! Hurrah for the plot.matrix library!\n\n\nShow the code\nlibrary(plot.matrix)\nplot(music_matrix, border=NA, breaks=c(0,1,10),col=c(\"white\",\"red\",\"black\"))"
  },
  {
    "objectID": "blog/37_2_5_24_matrix_analysis/index.html#probabilistic-generation",
    "href": "blog/37_2_5_24_matrix_analysis/index.html#probabilistic-generation",
    "title": "MIDI analysis, bags of notes, and probabilistic generation",
    "section": "Probabilistic generation",
    "text": "Probabilistic generation\nThe above matrix has a few different properties, but the main one is that it counts how often particular notes occur in particular units of time across all of the bars of music. And, aside from the fact that I threw out the the note off information, this matrix representation is pretty much the same as the original MIDI file, so it is possible to convert back to a MIDI file from this matrix representation.\nHowever, the matrix representation provides interesting opportunities for manipulating MIDI. Today I am focusing on generating sequences from the statistical structure of a musical corpus. I’ll take the above matrix as a small corpus of musical bars.\n\nNote x time probabilities\nIn previous posts where I generated some “mario-esque” sequences, I only got as far as generating them based on fairly simple, unconditionalized probabilities.\nFor example, columns 177 to 6320 in the music matrix contain the note x time frequency vectors for each bar. We can compute the total number of occurrences of all individual features by summing that whole part of the matrix. The answer is 901 note x time features occurred across all of mario.\n\n\nShow the code\ntotal_sum &lt;- sum(music_matrix[,(128+48+1):dim(music_matrix)[2]])\ntotal_sum\n\n\n[1] 901\n\n\nWhat about individual note x time features, how often did they occur? We can compute the column sums. This vector has too many numbers to print, but here’s a sense of how it looks (y-axis is frequency counts).\n\n\nShow the code\nnote_times_sum &lt;- colSums(music_matrix[,(128+48+1):dim(music_matrix)[2]])\nplot(note_times_sum)\n\n\n\n\n\n\n\n\n\nDividing the column sums by the total sum gives a vector of proportions that sum to 1.\n\n\nShow the code\nnote_times_prob &lt;- note_times_sum/total_sum\nsum(note_times_prob)\n\n\n[1] 1\n\n\nThe vector of proportions can be treated as probabilities and used to generate new notes in time based on this statistical structure. R has a convenient function for this called rbinom(). The goal is to produce a vector with the same length as the probability vector, but populated with 1s and 0s, where the 1 is a note occurrence and a 0 is the absence of a note.\nThis code chunk generates a new vector of note x time features based on the probabilities, restores the vector to a matrix format, and then plots the matrix of notes as a piano roll.\n\n\nShow the code\n# generate new vector based on probabilities\nnew_sequence &lt;- rbinom(n = length(note_times_prob),\n                       size = 12,\n                       prob = note_times_prob)\n\n# ensure any element greater than 1 is set to 1\nnew_sequence[new_sequence &gt; 1] &lt;- 1\n\n#\nnew_sequence_matrix &lt;- matrix(\n  new_sequence,\n  ncol = 48,\n  nrow = 128,\n  byrow = F\n)\n\nlibrary(plot.matrix)\nplot(new_sequence_matrix)\n\n\n\n\n\n\n\n\n\nThe new sequence is generated from independent probabilities. Essentially every note is sampled based on the probability that it appeared in a particular time slot across the whole corpus of bars. This creates a sonic mishmash where very global statistics of how notes appear in time across bars determine which notes get sampled and when they are placed in time.\nThe size parameter of the rbinom() function controls how many trials are simulated by the binomial sampling process, which controls the total number of notes sampled into a new bar.\nOne option is to set the size parameter to the mean number of notes that are found per bar in the corpus. I get about 24.\n\n\nShow the code\nmean(rowSums(music_matrix[,(48+128+1):dim(music_matrix)[2]]))\n\n\n[1] 24.35135\n\n\nAnd, using 24 as the size parameter generates new bars that should have some more notes:\n\n\nShow the code\n# generate new vector based on probabilities\nnew_sequence &lt;- rbinom(n = length(note_times_prob),\n                       size = 24,\n                       prob = note_times_prob)\n\n# ensure any element greater than 1 is set to 1\nnew_sequence[new_sequence &gt; 1] &lt;- 1\n\n#\nnew_sequence_matrix &lt;- matrix(\n  new_sequence,\n  ncol = 48,\n  nrow = 128,\n  byrow = F\n)\n\nlibrary(plot.matrix)\nplot(new_sequence_matrix)\n\n\n\n\n\n\n\n\n\nOr, this one has a size of 48, and starts looking even more busy:\n\n\nShow the code\n# generate new vector based on probabilities\nnew_sequence &lt;- rbinom(n = length(note_times_prob),\n                       size = 48,\n                       prob = note_times_prob)\n\n# ensure any element greater than 1 is set to 1\nnew_sequence[new_sequence &gt; 1] &lt;- 1\n\n#\nnew_sequence_matrix &lt;- matrix(\n  new_sequence,\n  ncol = 48,\n  nrow = 128,\n  byrow = F\n)\n\nlibrary(plot.matrix)\nplot(new_sequence_matrix)\n\n\n\n\n\n\n\n\n\n\n\nA eurorack digression\nAs a quick side note, in terms of sonic exploration, it would be so fun to get matrix manipulation of midi for probabilistic sequence generation into a eurorack format.\nOne of my favorite modules to play with is Marbles by Mutable Instruments.\n\nThis module is a random CV generator for time and pitch, and it can do some things in the direction I’m headed. For example, you can play in a series of notes, and it will calculate pitch frequencies, and then playback notes based on the sampled frequencies. That’s pretty cool.\nThe documentation for Marbles even hints at a secret “markov” mode which is more similar to what I’m doing here:\nhttps://pichenettes.github.io/mutable-instruments-documentation/modules/marbles/secrets/\nOnce I figure out what I’m doing in R, I wonder how easy it would be to get these algorithms working in a eurorack module…hmmm."
  },
  {
    "objectID": "blog/37_2_5_24_matrix_analysis/index.html#alternative-note-and-time-feature-generation",
    "href": "blog/37_2_5_24_matrix_analysis/index.html#alternative-note-and-time-feature-generation",
    "title": "MIDI analysis, bags of notes, and probabilistic generation",
    "section": "Alternative note and time feature generation",
    "text": "Alternative note and time feature generation\nThe matrix of bars also has pitch vectors and time vectors that summarize counts within their respective dimensions. These vectors can be turned into probabilistic generators as well.\n\n\nShow the code\npitch_probabilities &lt;- colSums(music_matrix[,1:128])/sum(music_matrix[,1:128])\n\ntime_probabilities &lt;- colSums(music_matrix[,129:(128+48)])/sum(music_matrix[,129:(128+48)])\n\n# get new pitches\nnew_pitches &lt;- rbinom(n = length(pitch_probabilities),\n       size = 24,\n       prob = pitch_probabilities)\nnew_pitches[new_pitches &gt; 1] &lt;-1\n\n# get new times\nnew_times &lt;- rbinom(n = length(time_probabilities),\n       size = 24,\n       prob = time_probabilities)\nnew_times[new_times &gt; 1] &lt;-1\n\n# get row column ids\nsampled_notes &lt;- which(new_pitches == 1)\nsampled_times &lt;- which(new_times == 1)\n\n# combine, make sure equal length\nif(length(sampled_notes) &gt;= length(sampled_times)){\n  sampled_ids &lt;- tibble(notes = sampled_notes[1:length(sampled_times)],\n                        times = sampled_times)\n} else {\n  sampled_ids &lt;- tibble(notes = sampled_notes,\n                        times = sampled_times[1:length(sampled_notes)])\n}\n\n# shuffle the notes across the times so the sampling is uniform\nsampled_ids$notes &lt;- sample(sampled_ids$notes)\n\n# make a note by time unit matrix\none_bar &lt;- matrix(0,\n                  nrow = dim(pyramidi::midi_defs)[1],\n                  ncol = 48)\n\n# assign 1s to note locations in time\nfor (i in 1:dim(sampled_ids)[1]) {\n  one_bar[sampled_ids$notes[i], sampled_ids$times[i]] &lt;- 1\n}\n\nlibrary(plot.matrix)\nplot(one_bar)\n\n\n\n\n\n\n\n\n\nThis should produce sequences that reflect even more general statistical structure from the corpus. I haven’t tried listening to lots of bars generated this way. Something to look forward to :)"
  },
  {
    "objectID": "blog/37_2_5_24_matrix_analysis/index.html#conditionalized-sampling",
    "href": "blog/37_2_5_24_matrix_analysis/index.html#conditionalized-sampling",
    "title": "MIDI analysis, bags of notes, and probabilistic generation",
    "section": "Conditionalized sampling",
    "text": "Conditionalized sampling\nSo far I’ve looked at fairly global note and time probabilities. It’s possible to conditionalize these probabilities by other events. For example, given that the first note in a bar is in the first time step, and that the first note is MIDI note 50, what are the probabilities of other notes in time?\nOne way to do this is to first filter the music matrix for only rows containing features to conditionalize on (e.g., only rows where the first note is MIDI note 50 in the first time slot.), and then compute the probabilities based on the filter matrix. In the case of mario, we only have 30+ bars of music, which doesn’t give a great estimate of the probabilities.\nAt the same time, I could change the size of matrix and ask different questions. Currently, the matrix is set up to ask questions about a whole bar of notes. For example, I could ask the question, given the first note is MIDI 50, what are the probabilities for all of the remaining notes in the whole bar.\nThe matrix could be re-arranged in larger or smaller collections of time. Let’s try re-arranging the matrix in slices of 1 beat instead of 1 bar. This will produce many more “bags” of notes than we currently have, and should do a better job of capturing local statistics for note-to-note transitions.\n\n\nShow the code\n# this can be improved, but works for now\n\nfor(i in 1:dim(music_matrix)[1]) {\n  extract_bar &lt;- music_matrix[i, (128 + 48 + 1):dim(music_matrix)[2]]\n  \n  bar_to_matrix &lt;- matrix(extract_bar,\n                          ncol = 48,\n                          nrow = 128,\n                          byrow = F)\n  \n  divisions &lt;- seq(1, 48, 12)\n  \n  for (j in 1:length(divisions)) {\n    beat_vector &lt;- bar_to_matrix[, divisions[j]:(divisions[j] + 12 - 1)]\n    beat_vector &lt;- c(beat_vector)\n    if (i == 1 && j ==1) {\n      beat_matrix &lt;- t(as.matrix(beat_vector))\n    } else {\n      beat_matrix &lt;- rbind(beat_matrix, beat_vector)\n    }\n  }\n  \n}\n\n\nA plot of the new beats matrix, with 4 times as many rows as before.\n\n\nShow the code\nlibrary(plot.matrix)\nplot(beat_matrix, border=NA, breaks=c(0,1,10),col=c(\"white\",\"red\",\"black\"))\n\n\n\n\n\n\n\n\n\nI’d like to contrast probabilistic playback from this matrix in general versus condtionalized on some starting notes.\n\nGeneral probabilistic sampling from beat matrix\nI’m going to repeat some steps from before. This is where I wish I had some functions to do this quickly. However, this lengthy note-book style approach is also helping me understand what I might functionalize later.\nThis code chunk should generate a new “beats” worth of notes every time it is run.\n\n\nShow the code\ntotal_sum &lt;- sum(beat_matrix[,1:dim(beat_matrix)[2]])\nnote_times_sum &lt;- colSums(beat_matrix[,1:dim(beat_matrix)[2]])\nnote_times_prob &lt;- note_times_sum/total_sum\n\n#mean(rowSums(beat_matrix[,1:dim(beat_matrix)[2]]))\n\n# generate new vector based on probabilities\nnew_sequence &lt;- rbinom(n = length(note_times_prob),\n                       size = 6,\n                       prob = note_times_prob)\n\n# ensure any element greater than 1 is set to 1\nnew_sequence[new_sequence &gt; 1] &lt;- 1\n\n#\nnew_sequence_matrix &lt;- matrix(\n  new_sequence,\n  ncol = 48,\n  nrow = 128,\n  byrow = F\n)\n\nplot(new_sequence_matrix)\n\n\n\n\n\n\n\n\n\n\n\nConditionalized sampling on starting notes\nThis example conditions on some starting notes.\nThe first three notes in mario overworld are a chord composed of MIDI notes 50, 66, and 70.\nThe following code grabs only the rows in the beat matrix that have these exact notes in the first time unit. The, new note x time probabilities are calculated from the reduced matrix, and then used to generate a new “beats” worth of notes. This type of sequence should be much more similar to the rows from which it was composed, compared to all rows in general.\n\n\nShow the code\n# create conditional vector\nconditional_vector &lt;- rep(0,dim(beat_matrix)[2])\nconditional_vector[c(50,66,70)] &lt;- 1 \n\n# compute cosine to find same items\nsimilarities &lt;- RsemanticLibrarian::cosine_x_to_m(conditional_vector,beat_matrix)\n\n# choose only rows that have the conditional vector in them\nconditionalized_matrix &lt;- beat_matrix[which(similarities &gt; 0),]\n\ntotal_sum &lt;- sum(conditionalized_matrix[,1:dim(conditionalized_matrix)[2]])\nnote_times_sum &lt;- colSums(conditionalized_matrix[,1:dim(conditionalized_matrix)[2]])\nnote_times_prob &lt;- note_times_sum/total_sum\n\n#mean(rowSums(beat_matrix[,1:dim(beat_matrix)[2]]))\n\n# generate new vector based on probabilities\nnew_sequence &lt;- rbinom(n = length(note_times_prob),\n                       size = 6,\n                       prob = note_times_prob)\n\n# ensure any element greater than 1 is set to 1\nnew_sequence[new_sequence &gt; 1] &lt;- 1\n\n#\nnew_sequence_matrix &lt;- matrix(\n  new_sequence,\n  ncol = 48,\n  nrow = 128,\n  byrow = F\n)\n\nplot(new_sequence_matrix)"
  },
  {
    "objectID": "blog/37_2_5_24_matrix_analysis/index.html#conditionalized-sampling-1",
    "href": "blog/37_2_5_24_matrix_analysis/index.html#conditionalized-sampling-1",
    "title": "MIDI analysis, bags of notes, and probabilistic generation",
    "section": "Conditionalized sampling",
    "text": "Conditionalized sampling\nThis example conditions on some starting notes.\nThe first three notes in mario overworld are a chord composed of MIDI notes 50, 66, and 70.\nThe following code grabs only the rows in the beat matrix that have these exact notes in the first time unit. The, new note x time probabilities are calculated from the reduced matrix, and then used to generate a new “beats” worth of notes. This type of sequence should be much more similar to the rows from which it was composed, compared to all rows in general.\n\n\nShow the code\n# create conditional vector\nconditional_vector &lt;- rep(0,dim(beat_matrix)[2])\nconditional_vector[c(50,66,70)] &lt;- 1 \n\n# compute cosine to find same items\nsimilarities &lt;- RsemanticLibrarian::cosine_x_to_m(conditional_vector,beat_matrix)\n\n# choose only rows that have the conditional vector in them\nconditionalized_matrix &lt;- beat_matrix[which(similarities &gt; 0),]\n\ntotal_sum &lt;- sum(conditionalized_matrix[,1:dim(conditionalized_matrix)[2]])\nnote_times_sum &lt;- colSums(conditionalized_matrix[,1:dim(conditionalized_matrix)[2]])\nnote_times_prob &lt;- note_times_sum/total_sum\n\n#mean(rowSums(beat_matrix[,1:dim(beat_matrix)[2]]))\n\n# generate new vector based on probabilities\nnew_sequence &lt;- rbinom(n = length(note_times_prob),\n                       size = 6,\n                       prob = note_times_prob)\n\n# ensure any element greater than 1 is set to 1\nnew_sequence[new_sequence &gt; 1] &lt;- 1\n\n#\nnew_sequence_matrix &lt;- matrix(\n  new_sequence,\n  ncol = 48,\n  nrow = 128,\n  byrow = F\n)\n\nplot(new_sequence_matrix)"
  },
  {
    "objectID": "blog/37_2_5_24_matrix_analysis/index.html#the-end",
    "href": "blog/37_2_5_24_matrix_analysis/index.html#the-end",
    "title": "MIDI analysis, bags of notes, and probabilistic generation",
    "section": "The end",
    "text": "The end\nI’d like to start listening to these sequences. But, right now the code is pretty clunky to start generating lots of things to listen to. It’s probably time to refactor the code into functions so it’s all more generalizable, clear, and easier to use.\n\nAnd hello again"
  },
  {
    "objectID": "blog/37_2_5_24_matrix_analysis/index.html#writing-functions",
    "href": "blog/37_2_5_24_matrix_analysis/index.html#writing-functions",
    "title": "MIDI analysis, bags of notes, and probabilistic generation",
    "section": "Writing functions",
    "text": "Writing functions\nI’m at the point where note-book style code is getting frustrating, and I would like to have some functions at my disposal. I’m note quite ready to jump into developing an R package, so I’ll try a few things out down here for now.\n\nimporting midi\nI would just use pyramidi::MidiFramer$new(path) but some of my midi files don’t have set_tempo, so that fails. Need to wrap a few things and then return them to the global environment.\nNot sure it is a good or bad idea to put R6 objects into a list? So far it’s working.\n\n\nShow the code\nmidi_to_object &lt;- function(file_path) {\n  #import midi using miditapyr\n  miditapyr_object &lt;- pyramidi::miditapyr$MidiFrames(file_path)\n  \n  #import using mido\n  mido_object &lt;- pyramidi::mido$MidiFile(file_path)\n  \n  # to R dataframe\n  message_list_df &lt;- pyramidi::miditapyr$frame_midi(mido_object)\n  ticks_per_beat &lt;- mido_object$ticks_per_beat\n  \n  # unnest the dataframe\n  midi_df &lt;- pyramidi::miditapyr$unnest_midi(message_list_df)\n  \n  return(\n    midi_import_objects &lt;- list(\n      miditapyr_object = miditapyr_object,\n      mido_object = mido_object,\n      message_list_df = message_list_df,\n      ticks_per_beat = ticks_per_beat,\n      midi_df = midi_df\n    )\n  )\n}\n\n# test function\nmidi_import_objects &lt;- midi_to_object(\"all_overworld.mid\")\n\n# import objects in list to global environment\n# bad form to put this in the above function?\nlist2env(midi_import_objects, .GlobalEnv)\n\n\n&lt;environment: R_GlobalEnv&gt;\n\n\n\n\nconverting midi to a feature vector matrix.\nI’m not confident that I have processed enough midi files to know how well any of this will generalize. I might start with small functions…\n\n\nShow the code\ncopy_and_extend_midi_df &lt;- function(midi_df, track_num = 0) {\n  # Select a particular track from the midi file\n  copy_df &lt;- midi_df %&gt;%\n    filter(i_track == track_num,\n           type %in% c(\"note_on\", \"note_off\") == TRUE) %&gt;%\n    # add column to sum cumulative time\n    mutate(total_time = cumsum(time)) %&gt;%\n    # return only note_on\n    filter(type == \"note_on\")\n  \n}\n\ntrack_1 &lt;- copy_and_extend_midi_df(midi_df, 0)\n\n########################################\n\n# work out new time divisions\n\nmake_metric_tibble &lt;- function(df, ticks_per_beat, bars=NULL, smallest_tick=NULL ) {\n  \n  if(is.null(smallest_tick) == TRUE){\n    # if not set by user\n    # set smallest temporal unit to smallest observed unit in time column\n    smallest_tick &lt;- min(df$time[df$time &gt; 0])\n  }\n  \n  time_steps &lt;- seq(0, max(track_1$total_time), smallest_tick)\n  \n  if(is.null(bars) == TRUE){\n    # only 4/4 for now\n    bars &lt;- max(df$total_time)/(ticks_per_beat*4)\n  }\n  \n  total_bars &lt;- round(length(time_steps) / bars) + 1\n  bar_vector &lt;- rep(1:total_bars, each = bars)\n  bar_steps &lt;- rep(1:bars, total_bars)\n  \n  metric_tibble &lt;- tibble(time_steps = time_steps,\n                          bars = bar_vector[1:length(time_steps)],\n                          bar_steps = bar_steps[1:length(time_steps)])\n  return(metric_tibble)\n}\n\nmetric_tibble &lt;- make_metric_tibble(track_1,\n                                    ticks_per_beat = 96,\n                                    bars = 48,\n                                    smallest_tick = 8)\n\n##########################################\n\nadd_bars_to_copy_df &lt;- function(df, metric_tibble) {\n  \n  # add new columns\n  df &lt;- df %&gt;%\n    mutate(time_markers = 0,\n           bars = 0,\n           bar_steps = 0)\n  \n  for (i in 1:dim(df)[1]) {\n    df$time_markers[i] &lt;-\n      which(metric_tibble$time_steps %in% df$total_time[i])\n  }\n  \n  # get bar divisions, add them to df\n  \n  for (i in 1:dim(df)[1]) {\n    get_timestep &lt;- metric_tibble$time_steps[df$time_markers[i]]\n    \n    df$bars[i] &lt;- metric_tibble %&gt;%\n      filter(time_steps == get_timestep) %&gt;%\n      pull(bars)\n    \n    df$bar_steps[i] &lt;- metric_tibble %&gt;%\n      filter(time_steps == get_timestep) %&gt;%\n      pull(bar_steps)\n  }\n  \n  return(df)\n}\n\ntrack_1 &lt;- add_bars_to_copy_df(track_1,metric_tibble)\n\n##########################################\n\n# create the matrix\n\ncreate_midi_matrix &lt;- function(df, num_notes = 128, intervals_per_bar = 48){\n  \n  #initialize matrix\n  music_matrix &lt;- matrix(0,\n                       ncol = (intervals_per_bar+num_notes+(intervals_per_bar*num_notes)),\n                       nrow = max(df$bars)\n                       )\n\n  #loop to assign note_ons to \n  for(i in 1:max(df$bars)) {\n    \n    # get the bar \n    bar_midi &lt;- df %&gt;%\n      filter(bars == i)\n    \n    # make a temporary little matrix\n    one_bar &lt;- matrix(0,\n                      nrow = num_notes,\n                      ncol = intervals_per_bar)\n    \n    # assign 1s to note positions\n    for (j in 1:dim(one_bar)[1]) {\n      one_bar[bar_midi$note[j], bar_midi$bar_steps[j]] &lt;- 1\n    }\n    \n    # get summary vectors\n    pitch_vector &lt;- rowSums(one_bar)\n    time_vector &lt;- colSums(one_bar)\n    pitch_by_time &lt;- c(one_bar)\n    \n    #concatenate_vector\n    music_vector &lt;- c(pitch_vector, time_vector, pitch_by_time)\n    \n    # add to matrix\n    music_matrix[i, ] &lt;- music_vector\n    \n  }\n  \n  return(music_matrix)\n  \n}\n\nmusic_matrix &lt;- create_midi_matrix(track_1,128,48)\n\n\nStarting to look more manageable. Testing the functions so far.\n\n\nShow the code\n#import\nmidi_import_objects &lt;- midi_to_object(\"all_overworld.mid\")\nlist2env(midi_import_objects, .GlobalEnv) # add to global env\n\n\n&lt;environment: R_GlobalEnv&gt;\n\n\nShow the code\n# extract midi df and add new timing information\ntrack_1 &lt;- copy_and_extend_midi_df(midi_df, 0)\n\nmetric_tibble &lt;- make_metric_tibble(track_1,\n                                    ticks_per_beat = 96,\n                                    bars = 48,\n                                    smallest_tick = 8)\n\ntrack_1 &lt;- add_bars_to_copy_df(track_1,metric_tibble)\n\n# convert all bars to a feature vector matrix, with one bar per row.\nmusic_matrix &lt;- create_midi_matrix(track_1,128,48)"
  },
  {
    "objectID": "blog/38_2_6_24_midiblender/index.html",
    "href": "blog/38_2_6_24_midiblender/index.html",
    "title": "midiblender: working on an experimental R package",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"blender on white background. blender blending musical notes. cartoon. 80s retro.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\n\n\n\n\n\n\n\n\n\nblender on white background. blender blending musical notes. cartoon. 80s retro.”- Dreamshaper v7\nI decided to have a go at turning my midi code into a set of functions. So, I spent the day working on a new R package called {midiblender}. Because, will it blend? and R.\nI’m not quite ready to share the package on github yet, because I suspect it will undergo many changes before I settle on useful patterns. Nevertheless, I have it working on my machine, and I’m doing a quick test here. Will write more about this later, and in the package documentation.\n\n\nShow the code\nlibrary(midiblender)\n\n# load a midi file\nmario &lt;- midi_to_object(\"all_overworld.mid\")\nlist2env(mario, .GlobalEnv) # send objects to global environment\n\n# copy a particular track for further processing\ntrack_0 &lt;- copy_midi_df_track(midi_df = midi_df,track_num = 0)\n\n# separate copy to extend with additional timing information\ncopy_track &lt;- copy_and_extend_midi_df(midi_df = midi_df,\n                                      track_num = 0)\n\n# get timing information\nmetric_tibble &lt;- make_metric_tibble(copy_track, bars = 48, smallest_tick = 8,ticks_per_beat = 96)\n\n# add timing to copy\ncopy_track &lt;- add_bars_to_copy_df(copy_track, metric_tibble)\n\n# convert to a matrix, each row is a bar.\nmusic_matrix &lt;- create_midi_matrix(\n  df = copy_track,\n  num_notes = 128,\n  intervals_per_bar = 48,\n  separate = TRUE\n)\n\n#### probabilistic sampling\nfeature_probs &lt;- get_feature_probs(midi_matrix = music_matrix$pitch_by_time_matrix)\n\nmean_note_density &lt;- get_mean_note_density(midi_matrix = music_matrix$pitch_by_time_matrix)\n\nnew_features &lt;- new_features_from_probs(probs = feature_probs,\n                                        density = mean_note_density,\n                                        examples = 10)\n\nnew_matrix &lt;- feature_vector_to_matrix(vec = new_features,\n                                       num_notes = 128)\n\n#### transform for export\n\nmidi_time_df &lt;- matrix_to_midi_time(midi_matrix = new_matrix,\n                                    smallest_time_unit = 8,\n                                    note_off_length = 32)\n\nmeta_messages_df &lt;- get_midi_meta_df(track_0)\n\nmeta_messages_df &lt;- set_midi_tempo_meta(meta_messages_df,update_tempo = 500000)\n\nsplit_meta_messages_df &lt;- split_meta_df(meta_messages_df)\n\nnew_midi_df &lt;- matrix_to_midi_track(midi_time_df = midi_time_df,\n                                    split_meta_list = split_meta_messages_df,\n                                    channel = 0,\n                                    velocity = 64)\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(new_midi_df)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"try_write.mid\")"
  },
  {
    "objectID": "blog/39_2_7_24_midiblender_mario/index.html",
    "href": "blog/39_2_7_24_midiblender_mario/index.html",
    "title": "WIP: Endless probabilistically generated mario music with midiblender",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"blender on white background. blender blending musical notes. cartoon. 80s retro.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\n\n\n\n\n\n\n\n\n\nThe current logo for my experimental r package\nAnother quick test post, but I am very excited about this one.\nIn past posts I’ve been doing things like importing midi files to R, such as the overworld music from the NES version of super mario brothers, and then randomizing the notes, or generating new sequences based off of probabilities computed from the music.\nSome of the ideas are minimally explained here, and in particular I had a goal of sampling new sequences based on conditionalized starting conditions. But, I didn’t have the code base in proper order to crunch those sequences.\nSo, yesterday I started on {midiblender}, an un-released R package, that now has the necessary code to accomplish my goal. You see, the previous ways of mangling mario were fun and interesting, and I approve of them. But, they got pretty stochastic sounding.\nI don’t have time right now to really explain what I’m trying to do in detail, but it is basically this:\n\ncreate a corpus (matrix) of feature vectors representing the note and time information for some temporal interval from the mario midi file. I’ve set it to 2 beats.\nPick a starting vector, such as the first chord in mario\nfilter the matrix for rows that have some similarity to the starting vector.\nCalculate a probability vector for the notes and times from the filtered matrix.\nGenerate a new feature vector from the probabilities.\nI have this set up in pairs, so I take the generated feature vector for the second beat, and use it as the next starting vector.\nrepeat to generate as many beats worth as you want.\n\nThe result should still be fairly probabilistic sounding, but more mario sequency sounding than before.\n\n\nShow the code\nlibrary(midiblender)\n#import midi\nmario &lt;- midi_to_object(\"all_overworld.mid\")\nlist2env(mario, .GlobalEnv) # send objects to global environment\ntrack_0 &lt;- copy_midi_df_track(midi_df = midi_df,track_num = 0)\n\n# convert to a piano roll matrix\nall_midi &lt;- midi_df_to_matrix(midi_df,track=0)\n\n# reshape to desired interval\nmusic_matrix &lt;- reshape_piano_roll(all_midi,48*2)\n\n# probabilistic sampling\n\n# create conditional vector\nconditional_vector &lt;- rep(0,dim(music_matrix)[2])\nconditional_vector[(c(50,66,76)+1)] &lt;- 1 \ndot_products &lt;- conditional_vector %*% t(music_matrix)\npositive_similarity &lt;- which(dot_products &gt; 0)\n\n\nfeature_probs &lt;- get_feature_probs(midi_matrix = music_matrix[positive_similarity,])\n\nmean_note_density &lt;- get_mean_note_density(midi_matrix = music_matrix[positive_similarity,])\n\nnew_features &lt;- new_features_from_probs(probs = feature_probs,\n                                        density = mean_note_density,\n                                        examples = 1)\n# loop\nnew_feature_vectors &lt;- new_features\n\nfor(i in 1:200) {\n  # put second half into conditional vector\n  conditional_vector &lt;- c(new_features[((length(new_features)/2)+1):length(new_features)],\n                          rep(0,length(new_features)/2))\n  dot_products &lt;- conditional_vector %*% t(music_matrix)\n  positive_similarity &lt;- which(dot_products &gt; 0)\n  \n  if(length(positive_similarity) == 0){\n    # sample some random stuff\n    positive_similarity &lt;- sample(1:dim(music_matrix)[1],25)\n    print(i)\n  }\n  \n  \n  feature_probs &lt;-\n    get_feature_probs(midi_matrix = music_matrix[positive_similarity, ])\n  \n  mean_note_density &lt;-\n    get_mean_note_density(midi_matrix = music_matrix[positive_similarity, ])\n  \n  new_features &lt;- new_features_from_probs(probs = feature_probs,\n                                          density = mean_note_density,\n                                          examples = 1)\n  new_feature_vectors &lt;- rbind(new_feature_vectors,new_features)\n}\n\nnew_matrix &lt;- feature_vector_to_matrix(vec = new_feature_vectors,\n                                       num_notes = 128)\n\n# transform back to midi\nmidi_time_df &lt;- matrix_to_midi_time(midi_matrix = new_matrix,\n                                    smallest_time_unit = 1,\n                                    note_off_length = 8)\n\nmeta_messages_df &lt;- get_midi_meta_df(track_0)\n\nmeta_messages_df &lt;- set_midi_tempo_meta(meta_messages_df,update_tempo = 600000)\n\nsplit_meta_messages_df &lt;- split_meta_df(meta_messages_df)\n\nnew_midi_df &lt;- matrix_to_midi_track(midi_time_df = midi_time_df,\n                                    split_meta_list = split_meta_messages_df,\n                                    channel = 0,\n                                    velocity = 64)\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(new_midi_df)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"endless_mario_1.mid\")\n\n#########\n# bounce to mp3 with fluid synth\n\ntrack_name &lt;- \"endless_mario_1\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/nintendo_soundfont.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\n\n\n  \nIt worked!\nIt’s slightly more musical than the other methods, and suggest lots of other fun to stuff to try.\nI really wish I had this in a eurorack module."
  },
  {
    "objectID": "blog/40_2_7_24_midiblender_canon/index.html",
    "href": "blog/40_2_7_24_midiblender_canon/index.html",
    "title": "Midi blending Canon in D probabilistically",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"blender on white background. blender blending musical notes. cartoon. 80s retro.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nStill working on {midiblender} here. Check out past posts for more info.\nAnother quick post with some code and mp3 examples. I’m waffling on whether to share {midiblender} on github sooner than later. It’s in a personally usable state, which is my intended use case. I guess my inclination is to share in case others find the code helpful, with a big warning that “it might not blend”. I’m too new to processing midi files, and the code is not general enough to handle lots of aspects of midi files. Anyway, on with the post.\nThis one is basically an extension of what I was doing with the mario overworld theme in the last post.\nInstead of mario, this time I’m importing Pachelbel’s Canon in D. It’s not the most well-articulated midi file, but it is recognizable as the Canon in D. One goal is make sure my code will mangle other midi files, so I’m kicking the tires here."
  },
  {
    "objectID": "blog/40_2_7_24_midiblender_canon/index.html#endless-canon-in-probabilid",
    "href": "blog/40_2_7_24_midiblender_canon/index.html#endless-canon-in-probabilid",
    "title": "Midi blending Canon in D probabilistically",
    "section": "Endless Canon in ProbabiliD",
    "text": "Endless Canon in ProbabiliD\nIn this example I slice up the midi file into small temporal intervals, like 2 beats of a bar, and represent each of them in a pitch x midi ticks matrix. Using this matrix I compute probabilities of each note in time within the bar, and then generate new notes based on those probabilities. There is a little bit more going on to make it slightly more musically. I start with the first two beats in the song and compute a measure of similarity between them, and all other 2 beat slices. I take the slices that have some positive similarity and compute the note and time probabilities from this smaller set. I have it set up so that when I generate notes from these probabilities (by sampling from binomial distributions), I also sample notes from the probabilities for the next beat. I then use those probabilities in the next step to filter the matrix for most similar bars, and so on.\nIt’s endless in the sense that I could run the loop for much longer and it would keep going, but I stopped it to get about 3 minutes out.\n\n\nShow the code\nlibrary(midiblender)\n#import midi\nmario &lt;- midi_to_object(\"canon.mid\")\nlist2env(mario, .GlobalEnv) # send objects to global environment\ntrack_0 &lt;- copy_midi_df_track(midi_df = midi_df,track_num = 0)\n\n# convert to a piano roll matrix\nall_midi &lt;- midi_df_to_matrix(midi_df,track=0)\n\n# reshape to desired interval\nmusic_matrix &lt;- reshape_piano_roll(all_midi,96*2)\n\n# probabilistic sampling\n\n# create conditional vector\nconditional_vector &lt;- rep(0,dim(music_matrix)[2])\nconditional_vector &lt;- music_matrix[1,]\ndot_products &lt;- conditional_vector %*% t(music_matrix)\npositive_similarity &lt;- which(dot_products &gt; 0)\n\n\nfeature_probs &lt;- get_feature_probs(midi_matrix = music_matrix[positive_similarity,])\n\nmean_note_density &lt;- get_mean_note_density(midi_matrix = music_matrix[positive_similarity,])\n\nnew_features &lt;- new_features_from_probs(probs = feature_probs,\n                                        density = mean_note_density,\n                                        examples = 1)\n# loop\nnew_feature_vectors &lt;- new_features\n\nfor(i in 1:100) {\n  # put second half into conditional vector\n  conditional_vector &lt;- c(new_features[((length(new_features)/2)+1):length(new_features)],\n                          rep(0,length(new_features)/2))\n  dot_products &lt;- conditional_vector %*% t(music_matrix)\n  positive_similarity &lt;- which(dot_products &gt; 0)\n  \n  if(length(positive_similarity)  &lt; 2){\n    # sample some random stuff\n    positive_similarity &lt;- sample(1:dim(music_matrix)[1],25)\n    print(i)\n  }\n  \n  \n  feature_probs &lt;-\n    get_feature_probs(midi_matrix = music_matrix[positive_similarity, ])\n  \n  mean_note_density &lt;-\n    get_mean_note_density(midi_matrix = music_matrix[positive_similarity, ])\n  \n  new_features &lt;- new_features_from_probs(probs = feature_probs,\n                                          density = mean_note_density,\n                                          examples = 1)\n  new_feature_vectors &lt;- rbind(new_feature_vectors,new_features)\n}\n\nnew_matrix &lt;- feature_vector_to_matrix(vec = new_feature_vectors,\n                                       num_notes = 128)\n\n# transform back to midi\nmidi_time_df &lt;- matrix_to_midi_time(midi_matrix = new_matrix,\n                                    smallest_time_unit = 1,\n                                    note_off_length = 8)\n\nmeta_messages_df &lt;- get_midi_meta_df(track_0)\n\nmeta_messages_df &lt;- set_midi_tempo_meta(meta_messages_df,update_tempo = 1000000)\n\nsplit_meta_messages_df &lt;- split_meta_df(meta_messages_df)\n\nnew_midi_df &lt;- matrix_to_midi_track(midi_time_df = midi_time_df,\n                                    split_meta_list = split_meta_messages_df,\n                                    channel = 0,\n                                    velocity = 100)\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(new_midi_df)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"endless_canon.mid\")\n\n#########\n# bounce to mp3 with fluid synth\n\ntrack_name &lt;- \"endless_canon\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\n\n\n  \nIt sounds like a probabilistic mess, but there are moments with some resemblance to Canon in D. The fluid synth piano sound helps with the cheese factor. Also, my code ignores note length, so everything sounds a bit choppy."
  },
  {
    "objectID": "blog/40_2_7_24_midiblender_canon/index.html#blending-the-mess",
    "href": "blog/40_2_7_24_midiblender_canon/index.html#blending-the-mess",
    "title": "Midi blending Canon in D probabilistically",
    "section": "Blending the mess",
    "text": "Blending the mess\nThe next example offers a blending parameter. I split the original into 2 beat sections. Then I loop through each 2 beat slice. For each slice I compute note in time probabilities and then generate a 2 beat probabilistic sequence. At the end, I have all of the original 2 beat sections, and another set of probabilistically generated ones. To construct the final tune, I blend them together in some proportion. In this case the proportion is 50/50.\n\n\nShow the code\nlibrary(midiblender)\n#import midi\nmario &lt;- midi_to_object(\"canon.mid\")\nlist2env(mario, .GlobalEnv) # send objects to global environment\ntrack_0 &lt;- copy_midi_df_track(midi_df = midi_df,track_num = 0)\n\n# convert to a piano roll matrix\nall_midi &lt;- midi_df_to_matrix(midi_df,track=0)\n\n# reshape to desired interval\nmusic_matrix &lt;- reshape_piano_roll(all_midi,96*2)\nprobabilistic_matrix &lt;- music_matrix\n\n# probabilistic sampling and blending\n\nfor( i in 1: dim(music_matrix)[1]){\n\n  conditional_vector &lt;- music_matrix[i,]\n  dot_products &lt;- conditional_vector %*% t(music_matrix)\n  positive_similarity &lt;- which(dot_products &gt; 0)\n  \n  if(length(positive_similarity)  &lt; 2){\n    # sample some random stuff\n    positive_similarity &lt;- sample(1:dim(music_matrix)[1],25)\n    print(i)\n  }\n  \n  \n  feature_probs &lt;- get_feature_probs(midi_matrix = music_matrix[positive_similarity,])\n  \n  mean_note_density &lt;- get_mean_note_density(midi_matrix = music_matrix[positive_similarity,])\n  \n  new_features &lt;- new_features_from_probs(probs = feature_probs,\n                                          density = mean_note_density,\n                                          examples = 1)\n  probabilistic_matrix[i,] &lt;- new_features\n}\n\n## blend\nblend_matrix &lt;- music_matrix\nfor(i in 1:dim(music_matrix)[1]){\n  sample_ids &lt;- sample(1:dim(music_matrix)[2])  \n  first_half &lt;- round(length(sample_ids)/2)\n  blend_vector &lt;- rep(0,length(sample_ids))\n  blend_vector[sample_ids[1:first_half]] &lt;- music_matrix[i,sample_ids[1:first_half]]\n  blend_vector[sample_ids[(first_half+1):length(sample_ids)]] &lt;- probabilistic_matrix[i,sample_ids[(first_half+1):length(sample_ids)]]\n  blend_matrix[i,] &lt;- blend_vector\n}\n\n\n\nnew_matrix &lt;- feature_vector_to_matrix(vec = blend_matrix,\n                                       num_notes = 128)\n\n# transform back to midi\nmidi_time_df &lt;- matrix_to_midi_time(midi_matrix = new_matrix,\n                                    smallest_time_unit = 1,\n                                    note_off_length = 8)\n\nmeta_messages_df &lt;- get_midi_meta_df(track_0)\n\nmeta_messages_df &lt;- set_midi_tempo_meta(meta_messages_df,update_tempo = 1000000)\n\nsplit_meta_messages_df &lt;- split_meta_df(meta_messages_df)\n\nnew_midi_df &lt;- matrix_to_midi_track(midi_time_df = midi_time_df,\n                                    split_meta_list = split_meta_messages_df,\n                                    channel = 0,\n                                    velocity = 100)\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(new_midi_df)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"endless_canon_2.mid\")\n\n#########\n# bounce to mp3 with fluid synth\n\ntrack_name &lt;- \"endless_canon_2\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\n\n\n  \nMuch more recognizable now, with 50% less probabilistic mess. Still sounds ridiculous. Nevertheless, I’m expecting some fun to start happening later, especially when running more interesting midi files through mr. modular synthesizer."
  },
  {
    "objectID": "blog/41_2_8_24_midiblender_alive/index.html",
    "href": "blog/41_2_8_24_midiblender_alive/index.html",
    "title": "{midiblender} is alive!",
    "section": "",
    "text": "Show the code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"blender on white background. blender blending musical notes. cartoon. 80s retro.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\n\n\n\n\n\n\n\n\n\nR stats package logo\nA super quick post to say that midiblender is alive on github.\nThe github repo is: https://github.com/CrumpLab/midiblender\nThis is the beginnings of an #rstats package for experimental mangling of #MIDI files. TBH, it’s my personal experimental hacky-code base wrapped in R package clothing. I’m messing with it constantly, and sharing in case others are interested.\nI wrote a getting starting vignette that could be helpful for others to try stuff out.\nOtherwise, I’m planning to keep posting examples and experiments on this blog. So, more {midiblender} to come."
  },
  {
    "objectID": "blog/42_2_8_24_prob_filter/index.html",
    "href": "blog/42_2_8_24_prob_filter/index.html",
    "title": "Analyzing and filtering note occurence by point estimation",
    "section": "",
    "text": "Code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"futuristic vacuum cleaner on white background. sucking up probabilities of musical notes. cartoon. 80s retro.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nThere’s many ideas swirling around leading me to this MIDI mangling experiment. My preference would be to discuss them in advance of the code, but I’m going to mostly resist that urge. Perhaps another time.\nGetting right into then, let’s use midiblender to import the mario music into a matrix.\nCode\nlibrary(midiblender)\n\n#import midi\nmario &lt;- midi_to_object(\"all_overworld.mid\")\nlist2env(mario, .GlobalEnv) # send objects to global environment\n\n\n&lt;environment: R_GlobalEnv&gt;\n\n\nCode\n# convert midi to matrix\npiano_roll &lt;- midi_df_to_matrix(midi_df,track=0)\ndim(piano_roll)\n\n\n[1]   128 14113\nThe piano_roll matrix has 128 rows for each possible midi note, and 14113 columns for each midi time tick. All of the notes are coded as 1s in this matrix. All other cells are set to 0."
  },
  {
    "objectID": "blog/42_2_8_24_prob_filter/index.html#point-estimates-of-note-occurrence-probability",
    "href": "blog/42_2_8_24_prob_filter/index.html#point-estimates-of-note-occurrence-probability",
    "title": "Analyzing and filtering note occurence by point estimation",
    "section": "Point estimates of note occurrence probability",
    "text": "Point estimates of note occurrence probability\nLet’s do a bit of data analysis. How many notes occur in this midi file?\n\n\nCode\nsum(piano_roll)\n\n\n[1] 901\n\n\nWhat are the frequencies of each note across the song?\n\n\nCode\nnote_frequencies &lt;- rowSums(piano_roll)\nplot(note_frequencies)\n\n\n\n\n\n\n\n\n\nMost of the notes are between MIDI note 40ish and 85ish. The most frequent note is clocking in around 90 occurences.\nDividing the vector of note frequencies by the total sum gives point estimates of the probabilities for each note.\n\n\nCode\nnote_probabilities &lt;- note_frequencies/sum(note_frequencies)\nplot(sort(note_probabilities))\n\n\n\n\n\n\n\n\n\nThere a quick sorted plot, most notes have 0 probability of occurrence, and then it swings up to a max of 0.1054384. Neato.\nIn previous posts I’ve been taking point estimation much further in terms of reshaping the matrix and estimating probabilities for notes in particular time slices. I haven’t yet tried doing something simple like the above, and using only these probabilities to generate new sequences. And, I’m not going to do that here either."
  },
  {
    "objectID": "blog/42_2_8_24_prob_filter/index.html#filtering-note-occurrences-by-point-estimates.",
    "href": "blog/42_2_8_24_prob_filter/index.html#filtering-note-occurrences-by-point-estimates.",
    "title": "Analyzing and filtering note occurence by point estimation",
    "section": "Filtering note occurrences by point estimates.",
    "text": "Filtering note occurrences by point estimates.\nI have a bunch of things I want to try with this, but in this post I’ll keep it simple to get the idea down.\nHere’s a simple filtering idea. Remove notes from the original piece depending on their point estimates. We could listen to mario with all of the high probability notes removed, or all of the low probability notes removed, or whatever combo one desires.\nThe more complicated plans involve deriving point estimates for conditional probabilities based on prior note occurrence, and/or note x time probabilities, and then filtering notes on that basis.\nBefore I do that, here’s a short digression. Consider listening to note sequences as an exercise in dynamic expectation. This is a pretty common notion in music and music cognition. You hear notes over time as they repeat and change, and have feelings of expectation about what comes next.\nLet’s say you are a note accumulator matrix, like the piano roll. As you accumulate note occurrences, some happen more often than others. If the music has repetitive structure, then there could be a sense of expectation. The new notes might have the same frequency profiles as the old notes, giving a sense of congruency…this is what the matrix was expecting. The new notes might have different frequencies, violating the expectation of the matrix, but nevertheless adding to the total accumulation of notes…the matrix would be less surprised about similar deviations the next time. Apologies for over-anthropomorphizing the matrix.\nThe point estimates above are very global point estimates of note probability taken over all local contexts. After listening to mario music on repeat, as I did for who knows how long as a kid, these probabilities represent a kind of mean expectation about what notes I should be hearing in general.\nI don’t know the music analysis literature well enough to know what this would be called, but it would be pretty straightforward to look at notes in a musical piece in terms of how they deviate from their point estimates. And, there are lots of interesting questions here about which point estimates to choose. Ones from the song, or set of songs like it, or a massive corpus of midi files, or a person’s entire listening experience. But, the analysis stuff is for another day.\nGotta reign in this digression. One point is there seems to be some interesting applied value in being able to quickly code notes in terms of their point estimates. It’ll be super easy to do this in R. For example, we would just replace all the 1s in each row of the piano matrix with their respective point estimates.\nFortunately, R does the hadamard product, so it’s just this.\n\n\nCode\npoint_estimate_matrix &lt;- piano_roll * note_probabilities\n\n\nGreat, so what do we do with the new matrix? Lots of possibilities come to mind. This could be a quick way to edit for wrong notes. If this was a recorded performance, and you accidentally flubbed a couple notes, these could be low frequency occurrences with very low probability of occurrence. Just delete the really low probability notes and re-constitute the midi file from the matrix.\nNeed to be delicate with the low probability notes, they might be really surprising and special. Maybe there needs to be more low probability notes.\nThe goal of this post was to listen to mario with high probability notes deleted, or with low probability notes deleted. We’ll do both. I suspect the technique could be interesting also with respect to probabilistically generated notes, many of which sound awful. Maybe tracking them and deleting them by their point estimates could make the probabilistic sequences sound “better”. If I think about this in terms of a eurorack module, I don’t really care how musical it sounds, I just want a knob to turn and mangle the probabilities in any direction for fun timez.\nNo more digression, let’s do this thing."
  },
  {
    "objectID": "blog/42_2_8_24_prob_filter/index.html#high-pass-probability-filtering-mario-without-high-probability-notes",
    "href": "blog/42_2_8_24_prob_filter/index.html#high-pass-probability-filtering-mario-without-high-probability-notes",
    "title": "Analyzing and filtering note occurence by point estimation",
    "section": "High pass probability filtering: Mario without high probability notes",
    "text": "High pass probability filtering: Mario without high probability notes\nDid a quick check and notes with a higher than 5% point estimate account for about 56% of all of the note occurrences. These notes get played a lot in mario.\n\n\nCode\nsum(note_probabilities[note_probabilities &gt; .05])\n\n\n[1] 0.5615982\n\n\nOK, bye bye high frequency notes. Setting those ones to 0. Setting all other values to 1, converting this back into a note occurrence matrix.\n\n\nCode\nmario_no_high &lt;- point_estimate_matrix\nmario_no_high[mario_no_high &gt; .05] &lt;- 0\nmario_no_high[mario_no_high &gt; 0] &lt;- 1 \n\n\nNow, a bit of {midiblender} magic, and let’s turn this matrix back into a midi file and listen to it.\n\n\nCode\ntrack_0 &lt;- copy_midi_df_track(midi_df,track_num = 0)\n\n# transform back to midi\nmidi_time_df &lt;- matrix_to_midi_time(midi_matrix = mario_no_high,\n                                    smallest_time_unit = 1,\n                                    note_off_length = 8)\n\nmeta_messages_df &lt;- get_midi_meta_df(track_0)\n\nmeta_messages_df &lt;- set_midi_tempo_meta(meta_messages_df,update_tempo = 500000)\n\nsplit_meta_messages_df &lt;- split_meta_df(meta_messages_df)\n\nnew_midi_df &lt;- matrix_to_midi_track(midi_time_df = midi_time_df,\n                                    split_meta_list = split_meta_messages_df,\n                                    channel = 0,\n                                    velocity = 100)\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(new_midi_df)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"mario_no_high.mid\")\n\n#########\n# bounce to mp3 with fluid synth\n\ntrack_name &lt;- \"mario_no_high\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/nintendo_soundfont.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\n\n\n  \nWell, half of the notes are gone, and it sounds pretty sparse. Very cool function though."
  },
  {
    "objectID": "blog/42_2_8_24_prob_filter/index.html#low-pass-probability-filtering-mario-without-low-probability-notes",
    "href": "blog/42_2_8_24_prob_filter/index.html#low-pass-probability-filtering-mario-without-low-probability-notes",
    "title": "Analyzing and filtering note occurence by point estimation",
    "section": "Low pass probability filtering: Mario without low probability notes",
    "text": "Low pass probability filtering: Mario without low probability notes\nLet’s do the opposite, and take out the low probability notes. It will be even more sparse using .05 as the cutoff. But it must be done.\n\n\nCode\nsum(note_probabilities[note_probabilities &lt; .05])\n\n\n[1] 0.4384018\n\n\nOK, bye bye high frequency notes. Setting those ones to 0. Setting all other values to 1, converting this back into a note occurrence matrix.\n\n\nCode\nmario_no_low &lt;- point_estimate_matrix\nmario_no_low[mario_no_low &lt; .05] &lt;- 0\nmario_no_low[mario_no_low &gt; 0] &lt;- 1 \n\n\nNow, a bit of {midiblender} magic, and let’s turn this matrix back into a midi file and listen to it.\n\n\nCode\ntrack_0 &lt;- copy_midi_df_track(midi_df,track_num = 0)\n\n# transform back to midi\nmidi_time_df &lt;- matrix_to_midi_time(midi_matrix = mario_no_low,\n                                    smallest_time_unit = 1,\n                                    note_off_length = 8)\n\nmeta_messages_df &lt;- get_midi_meta_df(track_0)\n\nmeta_messages_df &lt;- set_midi_tempo_meta(meta_messages_df,update_tempo = 500000)\n\nsplit_meta_messages_df &lt;- split_meta_df(meta_messages_df)\n\nnew_midi_df &lt;- matrix_to_midi_track(midi_time_df = midi_time_df,\n                                    split_meta_list = split_meta_messages_df,\n                                    channel = 0,\n                                    velocity = 100)\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(new_midi_df)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"mario_no_low.mid\")\n\n#########\n# bounce to mp3 with fluid synth\n\ntrack_name &lt;- \"mario_no_low\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/nintendo_soundfont.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\n\n\n  \nAgain, pretty sparse, but emphasizing different aspects of mario."
  },
  {
    "objectID": "blog/42_2_8_24_prob_filter/index.html#conclusions-i-want-a-eurorack-module-for-this",
    "href": "blog/42_2_8_24_prob_filter/index.html#conclusions-i-want-a-eurorack-module-for-this",
    "title": "Analyzing and filtering note occurence by point estimation",
    "section": "Conclusions: I want a eurorack module for this",
    "text": "Conclusions: I want a eurorack module for this\nI wish I could just dial around a knob, like a filter knob, that did this to a midi file while it was playing. Maybe someday I’ll figure out how to do that for a eurorack module.\nThese examples didn’t sound like much, but whatever. I could imagine this being very useful in general. For example, it would be nice to step back and look some composition and quickly appreciate what the spread of point estimates are. Maybe it is too regular, so you filter down the high probability stuff, or the opposite. It would be fun to have an equalizer like situation, where instead of emphasizing different wave frequencies, it allowed one to bring up or down different note probabilities. I think this would have really interesting effects, especially on super dense improvisational stuff, where I would probably want to delete things, and being able to do that on the basis of probabilities seems promising.\nThis is just a quick and dirty example, will fool around with this more for sure."
  },
  {
    "objectID": "blog/43_2_9_24_max_random/index.html",
    "href": "blog/43_2_9_24_max_random/index.html",
    "title": "Turning up the randomness to 11 for MAXIMUM MIDI MADNESS",
    "section": "",
    "text": "Code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"Random MIDI randomness. Explosive torrent of musical notes. Exploding the universe of music. Music everywhere. Neon super colorful sonic cartoon.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nIf I’m going to mangle MIDI, I should at least stuff it with as many notes from a uniform distribution as I care to.\nI will now attempt to randomize the pancakes out of a MIDI file until they splatter into a polyphonic volcano of mangled bliss (that’s the dream scenario).\nAccording to some quick research, it seems that fluid synth (that I’m using to quickly render MIDI files with premium cheese factor) has at least 128 voices of polyphony, which is enough to play all of the midi notes at the same time…potentially all in randomized voices.\nHere’s the plan. Will it blend? I don’t know. I’m pretty sure the first part of the plan will blend quickly.\nLet the abomination begin.\nCode\nlibrary(midiblender)\n\n#import midi\n#using mario to get the midi headers\nmario &lt;- midi_to_object(\"all_overworld.mid\")\nlist2env(mario, .GlobalEnv) # send objects to global environment\n\n# convert midi to matrix\nn_notes &lt;- 128\nn_ticks &lt;- 96*4*15 #15 bars at 120BPM of this should be enough\n\nfeature_vector &lt;- rep(1,n_notes*n_ticks)\nprob_vector &lt;- feature_vector/sum(feature_vector)\n\n# ensures uniform sampling of notes into tick intervals\n# Number of notes is approximately  = density\n# my note density goes to 1100\nrando_vector &lt;- midiblender::new_features_from_probs(prob_vector,density = 1100)\n\n# convert back piano roll matrix\nrando_matrix &lt;- feature_vector_to_matrix(rando_vector, num_notes=128)\n\n###############\n# turn it back into midi and render to mp3\n\ntrack_0 &lt;- copy_midi_df_track(midi_df,track_num = 0)\n\n# transform back to midi\nmidi_time_df &lt;- matrix_to_midi_time(midi_matrix = rando_matrix,\n                                    smallest_time_unit = 1,\n                                    note_off_length = 8)\n\nmeta_messages_df &lt;- get_midi_meta_df(track_0)\n\nmeta_messages_df &lt;- set_midi_tempo_meta(meta_messages_df,update_tempo = 500000)\n\nsplit_meta_messages_df &lt;- split_meta_df(meta_messages_df)\n\nnew_midi_df &lt;- matrix_to_midi_track(midi_time_df = midi_time_df,\n                                    split_meta_list = split_meta_messages_df,\n                                    channel = 0,\n                                    velocity = 100)\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(new_midi_df)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"rando_1100.mid\")\n\n#########\n# bounce to mp3 with fluid synth\n\ntrack_name &lt;- \"rando_1100\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/nintendo_soundfont.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\nI used a nintendo sound font sound here. It’s nice and random, too pretty though. Not nearly enough notes.\nThere are 128 possible notes and 15 bars. Each bar has four beats of 96 ticks. In total, there is 128 x 96 x 4 x 15 = 737280 cells where a note could be. I sampled about 1100 notes uniformly into those cells. That’s just not enough.\nIt should at least go to 11 percent, or about 81000 notes in 30 seconds. That seems like so many…oooooeeee.\nCode\n#import midi\n#using mario to get the midi headers\nmario &lt;- midi_to_object(\"all_overworld.mid\")\nlist2env(mario, .GlobalEnv) # send objects to global environment\n\n# convert midi to matrix\nn_notes &lt;- 128\nn_ticks &lt;- 96*4*15 #15 bars at 120BPM of this should be enough\n\nfeature_vector &lt;- rep(1,n_notes*n_ticks)\nprob_vector &lt;- feature_vector/sum(feature_vector)\n\n# ensures uniform sampling of notes into tick intervals\n# Number of notes is approximately  = density\n# my note density goes to 11%\nrando_vector &lt;- midiblender::new_features_from_probs(prob_vector,density = 81100)\n\n# convert back piano roll matrix\nrando_matrix &lt;- feature_vector_to_matrix(rando_vector, num_notes=128)\n\n###############\n# turn it back into midi and render to mp3\n\ntrack_0 &lt;- copy_midi_df_track(midi_df,track_num = 0)\n\n# transform back to midi\nmidi_time_df &lt;- matrix_to_midi_time(midi_matrix = rando_matrix,\n                                    smallest_time_unit = 1,\n                                    note_off_length = 8)\n\nmeta_messages_df &lt;- get_midi_meta_df(track_0)\n\nmeta_messages_df &lt;- set_midi_tempo_meta(meta_messages_df,update_tempo = 500000)\n\nsplit_meta_messages_df &lt;- split_meta_df(meta_messages_df)\n\nnew_midi_df &lt;- matrix_to_midi_track(midi_time_df = midi_time_df,\n                                    split_meta_list = split_meta_messages_df,\n                                    channel = 0,\n                                    velocity = 100)\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(new_midi_df)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"rando_B.mid\")\n\n#########\n# bounce to mp3 with fluid synth\n\ntrack_name &lt;- \"rando_B\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/nintendo_soundfont.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\nHAHAHHAHAHA. Oh MY. Giving my eurorack a run on thick noise.\nMuseScore4 would not render the sheet music for this. But, fluid synth didn’t even blink.\nIt’s very loud, so beware.\nChunky compared to white noise, and whiffs of crystal rain."
  },
  {
    "objectID": "blog/43_2_9_24_max_random/index.html#notes",
    "href": "blog/43_2_9_24_max_random/index.html#notes",
    "title": "Turning up the randomness to 11 for MAXIMUM MIDI MADNESS",
    "section": "Notes",
    "text": "Notes\nThat was fun. Had to try it. Even though this is basically ridiculous, I might return here one day with some FX pedals.\n\nI couldn’t stay away. Too many questions. And, it seems like I could add program change notes to change the synth voice, so I’m going to try that. And, I wondered about ramping the note density up and down over time, and will try that too. Maybe some other stuff."
  },
  {
    "objectID": "blog/43_2_9_24_max_random/index.html#randomizing-notes-and-program-changes",
    "href": "blog/43_2_9_24_max_random/index.html#randomizing-notes-and-program-changes",
    "title": "Turning up the randomness to 11 for MAXIMUM MIDI MADNESS",
    "section": "Randomizing notes and program changes",
    "text": "Randomizing notes and program changes\nBased ona few checks of MIDI files, it seems I could insert a program change message anywhere in a track, and this ought to change the synth voice. Gotta try it out.\nNotes:\n\nhow many patches are in the nintendo sound font? need to find out.\n\nMaybe run this? Seems to work and shows a long enough list to get me started.\necho \"inst 1\" | fluidsynth ~/Library/Audio/Sounds/Banks/nintendo_soundfont.sf2\nHad to break out a while loop for this one!\nNeed to keep the program change between 0-127. I guess there would be other stuff to switch banks. Leaving that for now.\n\n\nCode\nlibrary(midiblender)\n\n#import midi\n#using mario to get the midi headers\nmario &lt;- midi_to_object(\"all_overworld.mid\")\nlist2env(mario, .GlobalEnv) # send objects to global environment\n\n# convert midi to matrix\nn_notes &lt;- 128\nn_ticks &lt;- 96*4*15 #15 bars at 120BPM of this should be enough\n\nfeature_vector &lt;- rep(1,n_notes*n_ticks)\nprob_vector &lt;- feature_vector/sum(feature_vector)\n\n# ensures uniform sampling of notes into tick intervals\n# Number of notes is approximately  = density\n# my note density goes to 1100\nrando_vector &lt;- midiblender::new_features_from_probs(prob_vector,density = 1100)\n\n# convert back piano roll matrix\nrando_matrix &lt;- feature_vector_to_matrix(rando_vector, num_notes=128)\n\n###############\n# turn it back into midi and render to mp3\n\ntrack_0 &lt;- copy_midi_df_track(midi_df,track_num = 0)\n\n# transform back to midi\nmidi_time_df &lt;- matrix_to_midi_time(midi_matrix = rando_matrix,\n                                    smallest_time_unit = 1,\n                                    note_off_length = 16)\n\nmeta_messages_df &lt;- get_midi_meta_df(track_0)\n\nmeta_messages_df &lt;- set_midi_tempo_meta(meta_messages_df,update_tempo = 500000)\n\nsplit_meta_messages_df &lt;- split_meta_df(meta_messages_df)\n\nnew_midi_df &lt;- matrix_to_midi_track(midi_time_df = midi_time_df,\n                                    split_meta_list = split_meta_messages_df,\n                                    channel = 0,\n                                    velocity = 100)\n\n###################\n# add random program changes before each note on\n\ni &lt;- 0\nwhile(i &lt; dim(new_midi_df)[1]){\n  i &lt;- i+1\n  if(new_midi_df$type[i] == \"note_on\"){\n   new_midi_df &lt;- new_midi_df %&gt;%\n      dplyr::add_row(\n        i_track = 0,\n        meta = FALSE,\n        time = 0,\n        program = sample(0:127,1),\n        channel = 0,\n        type = \"program_change\",\n        .before = i)\n   i &lt;- i+1\n  }\n  \n}\n\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(new_midi_df)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"rando_1100_PC_B.mid\")\n\n#########\n# bounce to mp3 with fluid synth\n\ntrack_name &lt;- \"rando_1100_PC\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/nintendo_soundfont.sf2 {midi_name}')\nsystem(system_command)\n\n#system_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 {midi_name}')\n#system(system_command)\n\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\n\n\n  \nI got a lot of fluidsynth warnings for instrument not found when the PC value was 120 or greater. Oh well. This sounds like a breathy R2D2.\nHere’s another one using the FluidR3_GM.sf2 soundfont, which is a General MIDI kind of thing. I set the note length to a bit longer.\n  \nHa, I like it."
  },
  {
    "objectID": "blog/43_2_9_24_max_random/index.html#mario-with-every-note-played-by-a-random-instrument",
    "href": "blog/43_2_9_24_max_random/index.html#mario-with-every-note-played-by-a-random-instrument",
    "title": "Turning up the randomness to 11 for MAXIMUM MIDI MADNESS",
    "section": "Mario with every note played by a random instrument",
    "text": "Mario with every note played by a random instrument\nI gotta one more thing with this. What does the Mario music sound like when every note is played by a different instrument?\n\n\nCode\nlibrary(midiblender)\n\n#import midi\n#using mario to get the midi headers\nmario &lt;- midi_to_object(\"all_overworld.mid\")\nlist2env(mario, .GlobalEnv) # send objects to global environment\n\nnew_midi_df &lt;- midi_df\n\n###################\n# add random program changes before each note on\n\ni &lt;- 0\nwhile(i &lt; dim(new_midi_df)[1]){\n  i &lt;- i+1\n  if(new_midi_df$type[i] == \"note_on\"){\n   new_midi_df &lt;- new_midi_df %&gt;%\n      dplyr::add_row(\n        i_track = 0,\n        meta = FALSE,\n        time = 0,\n        program = sample(0:127,1),\n        channel = 0,\n        type = \"program_change\",\n        .before = i)\n   i &lt;- i+1\n  }\n  \n}\n\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(new_midi_df)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"mario_rand_voice.mid\")\n\n#########\n# bounce to mp3 with fluid synth\n\ntrack_name &lt;- \"mario_rand_voice\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# synthesize midi file to wav with fluid synth\n#system_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/nintendo_soundfont.sf2 {midi_name}')\n#system(system_command)\n\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 {midi_name}')\nsystem(system_command)\n\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\n\n\n  \nMeh…If I had a better handle on which patches I was randomizing over I bet that could sound more interesting.\nMaybe I should write a utility function to play through the sounds really fast and check them out."
  },
  {
    "objectID": "blog/43_2_9_24_max_random/index.html#sweeping-randomness-up-and-down",
    "href": "blog/43_2_9_24_max_random/index.html#sweeping-randomness-up-and-down",
    "title": "Turning up the randomness to 11 for MAXIMUM MIDI MADNESS",
    "section": "Sweeping randomness up and down",
    "text": "Sweeping randomness up and down\nIf this was a eurorack module there would be a whole bunch of knobs to turn. I would turn a knob for note density, so let’s see how easy it would be to have the randomness start with few notes, go up to ludicrous speed, and then come back down again.\nAnd, program change all the notes, because why not.\n\n\nCode\nlibrary(midiblender)\n\n#import midi\n#using mario to get the midi headers\nmario &lt;- midi_to_object(\"all_overworld.mid\")\nlist2env(mario, .GlobalEnv) # send objects to global environment\n\n# convert midi to matrix\nn_notes &lt;- 128\nn_ticks &lt;- 96*4*15 #15 bars at 120BPM of this should be enough\npoints &lt;- n_notes*n_ticks\n\nmid_point &lt;- round(points / 2)\nramp_up &lt;- round(seq(1, 100, length.out = mid_point))\nramp_down &lt;- round(seq(100, 1, length.out = (points - mid_point)))\nfeature_vector &lt;- c(ramp_up,ramp_down)\nprob_vector &lt;- feature_vector/sum(feature_vector)\n\n# ensures uniform sampling of notes into tick intervals\n# Number of notes is approximately  = density\n# my note density goes to 1100\nrando_vector &lt;- midiblender::new_features_from_probs(prob_vector,density = 4000)\n\n# convert back piano roll matrix\nrando_matrix &lt;- feature_vector_to_matrix(rando_vector, num_notes=128)\n\n###############\n# turn it back into midi and render to mp3\n\ntrack_0 &lt;- copy_midi_df_track(midi_df,track_num = 0)\n\n# transform back to midi\nmidi_time_df &lt;- matrix_to_midi_time(midi_matrix = rando_matrix,\n                                    smallest_time_unit = 1,\n                                    note_off_length = 8)\n\nmeta_messages_df &lt;- get_midi_meta_df(track_0)\n\nmeta_messages_df &lt;- set_midi_tempo_meta(meta_messages_df,update_tempo = 500000)\n\nsplit_meta_messages_df &lt;- split_meta_df(meta_messages_df)\n\nnew_midi_df &lt;- matrix_to_midi_track(midi_time_df = midi_time_df,\n                                    split_meta_list = split_meta_messages_df,\n                                    channel = 0,\n                                    velocity = 100)\n\n###################\n# add random program changes before each note on\n\ni &lt;- 0\nwhile(i &lt; dim(new_midi_df)[1]){\n  i &lt;- i+1\n  if(new_midi_df$type[i] == \"note_on\"){\n   new_midi_df &lt;- new_midi_df %&gt;%\n      dplyr::add_row(\n        i_track = 0,\n        meta = FALSE,\n        time = 0,\n        program = sample(0:127,1),\n        channel = 0,\n        type = \"program_change\",\n        .before = i)\n   i &lt;- i+1\n  }\n  \n}\n\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(new_midi_df)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"rando_ramp.mid\")\n\n#########\n# bounce to mp3 with fluid synth\n\ntrack_name &lt;- \"rando_ramp\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# synthesize midi file to wav with fluid synth\n#system_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/nintendo_soundfont.sf2 {midi_name}')\n#system(system_command)\n\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\n\n\n  \nA ramp up and down of sorts was added. Didn’t achieve ludicrous levels so much in the middle. Clocking out on this."
  },
  {
    "objectID": "blog/44_2_10_24_euclid/index.html",
    "href": "blog/44_2_10_24_euclid/index.html",
    "title": "Euclidean Rhythms and circulating sequences with information theory concepts",
    "section": "",
    "text": "Code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"A computer drumming. synthesizer drum. Drum beats. probabilistic drums. information theory. Euclid. Math. Drum machines everywhere. background is a room full of drum machines. Rhythm. drums. Transformers cartoon. retro 80s.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nThis is another exploratory post. I’m planning to use midiblender for a few things in the future. One of them is to generate stimuli for musical recognition tasks. The others are mostly fooling around. This here is a bit of both.\nI’ve been practicing the following kinds of patterns on piano, and I’ve been thinking about coding them as a generative sequence algorithm. I’ll start playing around on a single note, like C major, and then slowly add notes from the circle of fifths around the note I’m playing. I’ll bring in F and G, which gives a very Csus feel. And, then I’ll go out even more, add a Bb and D. These fifths are symmetrical about C. Sometimes I’ll go a bit lopsided, maybe add an A into the mix, and feel it out as I play. Another way to look at this is that I’m playing some group of notes from the circle of fifths, like {Bb, F, C, G, D}. I could expand the collection by adding more notes, or reduce the collection by taking some away. Sometimes I rotate around the circle, etc. It’s fun to play on piano.\nOne goal for this post is to get an algorithm that does something like the above. Just wanders around the cycle of fifths, expands and contracts the collection of notes to play, and then spits them out in an interesting rhythm.\nBut, what rhythm should I code in here? Euclidean Rhythms to the rescue. They mostly sound fine. Not the most super interesting rhythms ever, but very fine. If you don’t know what they are, this was a good post describing them, and it also had some code I modified to make a Euclidean Rhythms algorithm for R. I use these rhythms all the time in modular synthesis. So, part of this post is about getting them into {midiblender}.\nIdeally I would have a bunch of midi notes spread about in euclidean rhythms, and I could assign pitches to those notes using some kind of circle of fifths algorithm. Enter the last part of the equation, concepts from information theory.\nInformation theory provides an equation to measure the variance of a discrete probability distribution. It’s called Shannon entropy, and I won’t go into the details here too much. Actually, I doubt I will use the equation for this. It’s the concept of variation in a probability distribution that I’m interested in.\nA uniform probability distribution has no variance. All of the elements in the distribution have an equal likelihood of occurrence. I could use a uniform distribution to randomly pick notes from a collection like {Bb, F, C, G, D}, and sequences produced from that distribution would have 100% entropy. Another option is to bias the probabilities so that some of the notes are more probable than others in the set. These kinds of sequences have less entropy and are more predictable.\nMusical sequences usually don’t look like they come from a uniform distribution, often times some notes repeat more often than others. In terms of generating sequences with {midiblender}, I’d like some control over the variance of the probability distribution that is controlling note occurrences. Ideally, I would have an entropy knob on my eurorack somewhere that did all of this.\nThose are the basic ideas. I’m going to code some stuff and see what happens."
  },
  {
    "objectID": "blog/44_2_10_24_euclid/index.html#euclidean-rhythms-in-r",
    "href": "blog/44_2_10_24_euclid/index.html#euclidean-rhythms-in-r",
    "title": "Euclidean Rhythms and circulating sequences with information theory concepts",
    "section": "Euclidean Rhythms in R",
    "text": "Euclidean Rhythms in R\nHere’s a function that creates Euclidean rhythms in R. Again, thanks to Jeff Holtzkener for a great run down on this algorithm. To give some more credit, Euclidean rhythms were coined by Godfried Toussaint (Toussaint 2005), and his paper is available here.\nThe basic idea for the algorithm is to find a way to spread beats evenly through a set of time steps. 4 beats goes into 16 steps very evenly, 1 beat every 4 steps. But, how does one space 5 beats into 13 steps. The algorithm finds some nice solutions that happen to also sound good as rhythms. I learned from the Holtzkener post that Bresenham’s line algorithm (for raster scans), also works to compute Euclidean Rhythms, so I use that equation here.\n\n\nCode\nbresenham_euclidean &lt;- function(beats, steps, start = 1) {\n  previous &lt;- start\n  pattern &lt;- c()\n  \n  for (i in 0:(steps-1)) {\n    xVal &lt;- floor((beats / steps) * (i))\n    pattern &lt;- c(pattern, ifelse(xVal == previous, 0, 1))\n    previous &lt;- xVal\n  }\n  \n  return(pattern)\n}\n\n\nHere’s a 4 on the floor:\n\n\nCode\nbresenham_euclidean(beats = 4, steps = 16, start = 1) \n\n\n [1] 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0\n\n\nHere’s 7 beats over 16 steps:\n\n\nCode\nbresenham_euclidean(beats = 7, steps = 16, start = 1) \n\n\n [1] 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0\n\n\nGreat it seems to work. I’m tempted to go in a completely different direction and do some killer beats with {midiblender}, but that is too exciting. I will be back one day for the killer beats.\nScrew it, what is this blog if not a series of digressions. I think I’ve got enough breadcrumbs to lay down a beat quickly (fingers crossed).\n\n\n\n\n\n\nNote\n\n\n\nThe mp3s sometimes take a bit to load, especially the longer ones."
  },
  {
    "objectID": "blog/44_2_10_24_euclid/index.html#a-basic-drum-beat",
    "href": "blog/44_2_10_24_euclid/index.html#a-basic-drum-beat",
    "title": "Euclidean Rhythms and circulating sequences with information theory concepts",
    "section": "A basic drum beat",
    "text": "A basic drum beat\nThe metric structure for the beat should be easy. I just need to figure out which midi notes are which for the drum sounds.\nnotes:\n\nchannel 9 from FluidR3_GM.sf2\npercussion map https://usermanuals.finalemusic.com/SongWriter2012Win/Content/PercussionMaps.htm\n\nkick = 36 snare = 38 hihats = 42\nLet’s do 4 bars or something like that.\n\n\nCode\nlibrary(midiblender)\n# import midi\n# using mario to get the midi headers\n# need to add a vanilla midi file to this package for easier importing\nmario &lt;- midi_to_object(\"all_overworld.mid\")\nlist2env(mario, .GlobalEnv) # send objects to global environment\n\nbars &lt;- 8\nnotes &lt;- 128\nticks &lt;- 96*4*bars\n\n# empty beat matrix\nbeat_matrix &lt;- matrix(0,nrow=notes,ncol=ticks)\n\n#assign kick beats\nkick &lt;- bresenham_euclidean(4*bars,ticks, start = 0)\nsnare &lt;- bresenham_euclidean(2*bars,ticks, start = 0)\nhihats &lt;- bresenham_euclidean(16*bars,ticks, start = 0)\n\n# assign to matrix\nbeat_matrix[(36+1),] &lt;- kick\nbeat_matrix[(38+1),] &lt;- snare\nbeat_matrix[(42+1),] &lt;- hihats\n\n\n# transform back to midi\ntrack_0 &lt;- copy_midi_df_track(midi_df,track_num = 0)\n\nmidi_time_df &lt;- matrix_to_midi_time(midi_matrix = beat_matrix,\n                                    smallest_time_unit = 1,\n                                    note_off_length = 8)\n\nmeta_messages_df &lt;- get_midi_meta_df(track_0)\n\nmeta_messages_df &lt;- set_midi_tempo_meta(meta_messages_df,update_tempo = 500000)\n\nsplit_meta_messages_df &lt;- split_meta_df(meta_messages_df)\n\nnew_midi_df &lt;- matrix_to_midi_track(midi_time_df = midi_time_df,\n                                    split_meta_list = split_meta_messages_df,\n                                    channel = 9,\n                                    velocity = 100)\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(new_midi_df)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"beat.mid\")\n\n#########\n# bounce to mp3 with fluid synth\n\ntrack_name &lt;- \"beat\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\n\n\n  \nA basic rock beat.\n\nSpent some time fiddling with this to see if I can get the beats to sounds like my euclidean circles eurorack module. Whatever I was doing wasn’t working, so I’m trying some other stuff.\n\n\nCode\nlibrary(midiblender)\n# import midi\n# using mario to get the midi headers\n# need to add a vanilla midi file to this package for easier importing\nmario &lt;- midi_to_object(\"all_overworld.mid\")\nlist2env(mario, .GlobalEnv) # send objects to global environment\n\nbars &lt;- 1\nnotes &lt;- 128\nticks &lt;- 96*4*bars\n\n# empty beat matrix\nbeat_matrix &lt;- matrix(0,nrow=notes,ncol=ticks)\n\n#assign kick beats\nkick &lt;- bresenham_euclidean(5, 16, start = 1)\nsnare &lt;- bresenham_euclidean(2, 16, start = 1)\nhihats &lt;- bresenham_euclidean(13, 16, start = 1)\n\n# assign to matrix\nbeat_matrix[(36+1),seq(1,ticks,ticks/16)] &lt;- kick\nbeat_matrix[(38+1),seq(1,ticks,ticks/16)] &lt;- snare\nbeat_matrix[(42+1),seq(1,ticks,ticks/16)] &lt;- hihats\n\nbeat_matrix &lt;- cbind(beat_matrix,\n                     beat_matrix,\n                     beat_matrix,\n                     beat_matrix)\n\n# transform back to midi\ntrack_0 &lt;- copy_midi_df_track(midi_df,track_num = 0)\n\nmidi_time_df &lt;- matrix_to_midi_time(midi_matrix = beat_matrix,\n                                    smallest_time_unit = 1,\n                                    note_off_length = 8)\n\nmeta_messages_df &lt;- get_midi_meta_df(track_0)\n\nmeta_messages_df &lt;- set_midi_tempo_meta(meta_messages_df,update_tempo = 500000)\n\nsplit_meta_messages_df &lt;- split_meta_df(meta_messages_df)\n\nnew_midi_df &lt;- matrix_to_midi_track(midi_time_df = midi_time_df,\n                                    split_meta_list = split_meta_messages_df,\n                                    channel = 9,\n                                    velocity = 100)\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(new_midi_df)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"beat_2.mid\")\n\n#########\n# bounce to mp3 with fluid synth\n\ntrack_name &lt;- \"beat_2\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\n\n\n  \nNice, a little more intrigue, thanks to the euclidean rhythms. I need to spend way more time thinking about better ways to code this. Hmmm…so tempting to do one more.\n\n\nCode\nlibrary(midiblender)\n# import midi\n# using mario to get the midi headers\n# need to add a vanilla midi file to this package for easier importing\nmario &lt;- midi_to_object(\"all_overworld.mid\")\nlist2env(mario, .GlobalEnv) # send objects to global environment\n\nbars &lt;- 1\nnotes &lt;- 128\nticks &lt;- 96*4*bars\n\nall_beat_matrix &lt;- matrix(0,nrow=notes,ncol=96)\n\nfor(i in 1:8) {\n  # empty beat matrix\n  beat_matrix &lt;- matrix(0, nrow = notes, ncol = ticks)\n  \n  #assign kick beats\n  kick &lt;- bresenham_euclidean(sample(2:7,1), 16, start = 1)\n  snare &lt;- bresenham_euclidean(sample(2:7,1), 16, start = 1)\n  hihats &lt;- bresenham_euclidean(sample(8:14,1), 16, start = 1)\n  \n  # assign to matrix\n  beat_matrix[(36 + 1), seq(1, ticks, ticks / 16)] &lt;- kick\n  beat_matrix[(38 + 1), seq(1, ticks, ticks / 16)] &lt;- snare\n  beat_matrix[(42 + 1), seq(1, ticks, ticks / 16)] &lt;- hihats\n  \n  all_beat_matrix &lt;- cbind(all_beat_matrix, beat_matrix)\n}\n\n# transform back to midi\ntrack_0 &lt;- copy_midi_df_track(midi_df,track_num = 0)\n\nmidi_time_df &lt;- matrix_to_midi_time(midi_matrix = all_beat_matrix,\n                                    smallest_time_unit = 1,\n                                    note_off_length = 8)\n\nmeta_messages_df &lt;- get_midi_meta_df(track_0)\n\nmeta_messages_df &lt;- set_midi_tempo_meta(meta_messages_df,update_tempo = 500000)\n\nsplit_meta_messages_df &lt;- split_meta_df(meta_messages_df)\n\nnew_midi_df &lt;- matrix_to_midi_track(midi_time_df = midi_time_df,\n                                    split_meta_list = split_meta_messages_df,\n                                    channel = 9,\n                                    velocity = 100)\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(new_midi_df)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"beat_3.mid\")\n\n#########\n# bounce to mp3 with fluid synth\n\ntrack_name &lt;- \"beat_3\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\n\n\n  \nCool, for this one I randomly chose the paramaters for the euclidean sources every bar. Not super groovy, but shows some of the variation you can get from euclidean rhythms. I’ll continue to mess with drums some other time."
  },
  {
    "objectID": "blog/44_2_10_24_euclid/index.html#setting-notes-to-euclidean-rhythms",
    "href": "blog/44_2_10_24_euclid/index.html#setting-notes-to-euclidean-rhythms",
    "title": "Euclidean Rhythms and circulating sequences with information theory concepts",
    "section": "Setting notes to euclidean rhythms",
    "text": "Setting notes to euclidean rhythms\nBefore I try any of that information theory stuff, let’s see if I can set some notes to the euclidean rhythms.\nI have a roundabout strategy to do this and a sense it will blend. I’m going to take some inspiration from this code, that I used to generate new sequences from separate vectors for note probability and time probability.\n\nUse euclidean rhythms to generate a bunch of possible time stamps, that should have some mildly OK rhythm.\nGet the point estimates for time steps to probabilistically generate rhythm later.\nMake a vector of note probabilities\nPut them together and listen to it.\n\nnotes:\n\nrealizing I don’t have a favorite way of picking individual notes\n\n\ntest for one track 8 bars.\n\n\nCode\nlibrary(midiblender)\nlibrary(dplyr)\nlibrary(tibble)\n# import midi\n# using mario to get the midi headers\n# need to add a vanilla midi file to this package for easier importing\nmario &lt;- midi_to_object(\"all_overworld.mid\")\nlist2env(mario, .GlobalEnv) # send objects to global environment\n\n\n# Sample some beats for timesteps into the matrix\nbars &lt;- 1\nnotes &lt;- 128\nticks &lt;- 96*4*bars\n\nall_beat_matrix &lt;- matrix(0,nrow=notes,ncol=96*4)\n\nfor(i in 1:8) {\n  # empty beat matrix\n  beat_matrix &lt;- matrix(0, nrow = notes, ncol = ticks)\n  \n  #assign kick beats\n  kick &lt;- bresenham_euclidean(sample(2:7,1), 16, start = 1)\n  snare &lt;- bresenham_euclidean(sample(2:7,1), 16, start = 1)\n  hihats &lt;- bresenham_euclidean(sample(8:16,1), 16, start = 1)\n  \n  # assign to matrix\n  beat_matrix[(36 + 1), seq(1, ticks, ticks / 16)] &lt;- kick\n  beat_matrix[(38 + 1), seq(1, ticks, ticks / 16)] &lt;- snare\n  beat_matrix[(42 + 1), seq(1, ticks, ticks / 16)] &lt;- hihats\n  \n  all_beat_matrix &lt;- cbind(all_beat_matrix, beat_matrix)\n}\n\n# get the point estimates\ntime_probabilities &lt;- colSums(all_beat_matrix)/sum(all_beat_matrix)\n\nbar_probabilities &lt;- matrix(time_probabilities,\n                            ncol=(96*4),\n                            byrow = T)\n\nbar_probabilities &lt;- colMeans(bar_probabilities)\n\n# make my own midi notes df\n# add this to midiblender later\nmidi_notes &lt;- pyramidi::midi_defs %&gt;%\n  rowwise() %&gt;%\n  mutate(note_letter = unlist(strsplit(as.character(note_name),\"-\"))[1],\n         octave = unlist(strsplit(as.character(note_name),\"-\"))[2])\n\nmidi_notes &lt;- tibble(notes = rep(midi_notes[1:12,]$note_letter,11)[1:128],\n                     octaves = rep(-1:9, each = 12)[1:128],\n                     midi_number = 0:127\n)\n\nsong &lt;- matrix(0,nrow=notes,ncol=96*4)\n\n# begin bar by bar composition loop\n\nfor(i in 1:8){\n\n  # get some notes to sample from fifths around a starting note\n  starting_note &lt;- 60 # C4\n  fifth_intervals &lt;- c(0, 7, 14, -7, -14)\n  notes_to_choose &lt;- starting_note + fifth_intervals\n  octave_range &lt;- 2:5\n  \n  note_names &lt;- midi_notes %&gt;%\n    filter(midi_number %in% notes_to_choose) %&gt;%\n    pull(notes)\n  \n  possible_notes &lt;- midi_notes %&gt;%\n    filter(notes %in% note_names == TRUE,\n           octaves %in% octave_range == TRUE)\n  \n  possible_notes &lt;- possible_notes %&gt;%\n    mutate(probs = 1 / dim(possible_notes)[1])\n  \n  # create probability vector\n  pitch_probabilities &lt;- rep(0, 128)\n  pitch_probabilities[possible_notes$midi_number + 1] &lt;-\n    possible_notes$probs\n  \n  # get new pitches\n  new_pitches &lt;- rbinom(n = length(pitch_probabilities),\n                        size = 32,\n                        prob = pitch_probabilities)\n  new_pitches[new_pitches &gt; 1] &lt;- 1\n  \n  # get new times\n  new_times &lt;- rbinom(n = length(bar_probabilities),\n                      size = 256,\n                      prob = bar_probabilities)\n  # To Do: come back here and make it ok to have more than 1\n  new_times[new_times &gt; 1] &lt;- 1\n  \n  # get row column ids\n  sampled_notes &lt;- which(new_pitches == 1)\n  sampled_times &lt;- which(new_times == 1)\n  \n  # combine, make sure equal length\n  if (length(sampled_notes) &gt;= length(sampled_times)) {\n    sampled_ids &lt;-\n      tibble(notes = sampled_notes[1:length(sampled_times)],\n             times = sampled_times)\n  } else {\n    sampled_ids &lt;- tibble(notes = sampled_notes,\n                          times = sampled_times[1:length(sampled_notes)])\n  }\n  \n  # shuffle the notes across the times so the sampling is uniform\n  sampled_ids$notes &lt;- sample(sampled_ids$notes)\n  \n  # make a note by time unit matrix\n  one_bar &lt;- matrix(0,\n                    nrow = 128,\n                    ncol = 96 * 4)\n  \n  # assign 1s to note locations in time\n  for (i in 1:dim(sampled_ids)[1]) {\n    one_bar[sampled_ids$notes[i], sampled_ids$times[i]] &lt;- 1\n  }\n  \n  song &lt;- cbind(song,one_bar)\n}\n\n#####################\n# transform back to midi\ntrack_0 &lt;- copy_midi_df_track(midi_df,track_num = 0)\n\nmidi_time_df &lt;- matrix_to_midi_time(midi_matrix = song,\n                                    smallest_time_unit = 4,\n                                    note_off_length = 32)\n\nmeta_messages_df &lt;- get_midi_meta_df(track_0)\n\nmeta_messages_df &lt;- set_midi_tempo_meta(meta_messages_df,update_tempo = 500000)\n\nsplit_meta_messages_df &lt;- split_meta_df(meta_messages_df)\n\nnew_midi_df &lt;- matrix_to_midi_track(midi_time_df = midi_time_df,\n                                    split_meta_list = split_meta_messages_df,\n                                    channel = 0,\n                                    velocity = 100)\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(new_midi_df)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"fifths.mid\")\n\n#########\n# bounce to mp3 with fluid synth\n\ntrack_name &lt;- \"fifths\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\n\n\nThat worked, but it’s sounds like piano popcorn. Not worth sharing.\n\nI’ve increased number of loops here for multiple tracks. Each track has different note densities. The results are almost listenable, even more so if I put the midi tracks into ableton and give them some more interesting synth voices.\n\n\nCode\nlibrary(midiblender)\nlibrary(dplyr)\nlibrary(tibble)\n# import midi\n# using mario to get the midi headers\n# need to add a vanilla midi file to this package for easier importing\nmario &lt;- midi_to_object(\"all_overworld.mid\")\nlist2env(mario, .GlobalEnv) # send objects to global environment\n\n\n# Sample some beats for timesteps into the matrix\nbars &lt;- 1\nnotes &lt;- 128\nticks &lt;- 96*4*bars\n\nall_beat_matrix &lt;- matrix(0,nrow=notes,ncol=96*4)\n\nfor(i in 1:8) {\n  # empty beat matrix\n  beat_matrix &lt;- matrix(0, nrow = notes, ncol = ticks)\n  \n  #assign kick beats\n  kick &lt;- bresenham_euclidean(sample(2:7,1), 16, start = 1)\n  snare &lt;- bresenham_euclidean(sample(2:7,1), 16, start = 1)\n  hihats &lt;- bresenham_euclidean(sample(8:16,1), 16, start = 1)\n  \n  # assign to matrix\n  beat_matrix[(36 + 1), seq(1, ticks, ticks / 16)] &lt;- kick\n  beat_matrix[(38 + 1), seq(1, ticks, ticks / 16)] &lt;- snare\n  beat_matrix[(42 + 1), seq(1, ticks, ticks / 16)] &lt;- hihats\n  \n  all_beat_matrix &lt;- cbind(all_beat_matrix, beat_matrix)\n}\n\n# get the point estimates\ntime_probabilities &lt;- colSums(all_beat_matrix)/sum(all_beat_matrix)\n\nbar_probabilities &lt;- matrix(time_probabilities,\n                            ncol=(96*4),\n                            byrow = T)\n\nbar_probabilities &lt;- colMeans(bar_probabilities)\n\n# make my own midi notes df\n# add this to midiblender later\nmidi_notes &lt;- pyramidi::midi_defs %&gt;%\n  rowwise() %&gt;%\n  mutate(note_letter = unlist(strsplit(as.character(note_name),\"-\"))[1],\n         octave = unlist(strsplit(as.character(note_name),\"-\"))[2])\n\nmidi_notes &lt;- tibble(notes = rep(midi_notes[1:12,]$note_letter,11)[1:128],\n                     octaves = rep(-1:9, each = 12)[1:128],\n                     midi_number = 0:127\n)\n\nall_track_midi &lt;- data.frame()\n\ntime_densities &lt;- c(256,128,64)\n\n# track loop\nfor(t in 1:3){\n\nsong &lt;- matrix(0,nrow=notes,ncol=1)\n\n# begin bar by bar composition loop\n\nfor(i in 1:16){\n\n  # get some notes to sample from fifths around a starting note\n  starting_note &lt;- 60 # C4\n  fifth_intervals &lt;- c(0, 7, 14, -7, -14)\n  notes_to_choose &lt;- starting_note + fifth_intervals\n  octave_range &lt;- 3:6\n  \n  note_names &lt;- midi_notes %&gt;%\n    filter(midi_number %in% notes_to_choose) %&gt;%\n    pull(notes)\n  \n  possible_notes &lt;- midi_notes %&gt;%\n    filter(notes %in% note_names == TRUE,\n           octaves %in% octave_range == TRUE)\n  \n  possible_notes &lt;- possible_notes %&gt;%\n    mutate(probs = 1 / dim(possible_notes)[1])\n  \n  # create probability vector\n  pitch_probabilities &lt;- rep(0, 128)\n  pitch_probabilities[possible_notes$midi_number + 1] &lt;-\n    possible_notes$probs\n  \n  # get new pitches\n  new_pitches &lt;- rbinom(n = length(pitch_probabilities),\n                        size = 32,\n                        prob = pitch_probabilities)\n  new_pitches[new_pitches &gt; 1] &lt;- 1\n  \n  # get new times\n  new_times &lt;- rbinom(n = length(bar_probabilities),\n                      size = time_densities[t],\n                      prob = bar_probabilities)\n  # To Do: come back here and make it ok to have more than 1\n  new_times[new_times &gt; 1] &lt;- 1\n  \n  # get row column ids\n  sampled_notes &lt;- which(new_pitches == 1)\n  sampled_times &lt;- which(new_times == 1)\n  \n  # combine, make sure equal length\n  if (length(sampled_notes) &gt;= length(sampled_times)) {\n    sampled_ids &lt;-\n      tibble(notes = sampled_notes[1:length(sampled_times)],\n             times = sampled_times)\n  } else {\n    sampled_ids &lt;- tibble(notes = sampled_notes,\n                          times = sampled_times[1:length(sampled_notes)])\n  }\n  \n  # shuffle the notes across the times so the sampling is uniform\n  sampled_ids$notes &lt;- sample(sampled_ids$notes)\n  \n  # make a note by time unit matrix\n  one_bar &lt;- matrix(0,\n                    nrow = 128,\n                    ncol = 96 * 4)\n  \n  # assign 1s to note locations in time\n  for (r in 1:dim(sampled_ids)[1]) {\n    one_bar[sampled_ids$notes[r], sampled_ids$times[r]] &lt;- 1\n  }\n  \n  song &lt;- cbind(song,one_bar)\n}\n\n#####################\n# transform back to midi\ntrack_0 &lt;- copy_midi_df_track(midi_df,track_num = 0)\n\nmidi_time_df &lt;- matrix_to_midi_time(midi_matrix = song,\n                                    smallest_time_unit = 1,\n                                    note_off_length = 32)\n\nmeta_messages_df &lt;- get_midi_meta_df(track_0)\n\nmeta_messages_df &lt;- set_midi_tempo_meta(meta_messages_df,update_tempo = 500000)\n\nsplit_meta_messages_df &lt;- split_meta_df(meta_messages_df)\n\nnew_midi_df &lt;- matrix_to_midi_track(midi_time_df = midi_time_df,\n                                    split_meta_list = split_meta_messages_df,\n                                    channel = 0,\n                                    velocity = 100)\nnew_midi_df &lt;- new_midi_df %&gt;%\n  mutate(i_track = t)\n\nall_track_midi &lt;- rbind(all_track_midi,\n                        new_midi_df)\n}\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(all_track_midi)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"fifths_tracks.mid\")\n\n#########\n# bounce to mp3 with fluid synth\n\ntrack_name &lt;- \"fifths\"\n\nwav_name &lt;- paste0(track_name,\".wav\")\nmidi_name &lt;- paste0(track_name,\".mid\")\nmp3_name &lt;- paste0(track_name,\".mp3\")\n\n# synthesize midi file to wav with fluid synth\nsystem_command &lt;- glue::glue('fluidsynth -F {wav_name} ~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2 {midi_name}')\nsystem(system_command)\n\n# convert wav to mp3\nav::av_audio_convert(wav_name,mp3_name)\n\n# clean up and delete wav\nif(file.exists(wav_name)){\n  file.remove(wav_name)\n}\n\n\nThis was fun, even if my eyes are bleeding from the wall of code. I put the three tracks generated here into Ableton, assigned them some synth voices, added a couple drum beats, slowed down the tempo, and it sounds like this:\nThis one is louder than the others, not super loud, but louder\n  \nI didn’t get to all of my goals. I still need to come back and change the probability of different notes so they aren’t uniform. Right now all the pitches were uniformly sampled each time. The result is OK, but I want to hear other versions sampled from non-uniform distributions.\nAt this point I think I’ve basically recreated something like what Marbles by Mutable Instruments is doing. I could have probably done this in eurorack. And, that would have been fun too.\nTime to take a break.\n\nPseudo Composition code\nThe above is what I would consider my first attempt at composing generative sequences in R. Taking a couple minutes here to write pseudo code on what it was that happened in the code.\n\nImport a MIDI file\nRun euclidean rhythms into a matrix for a while to get point estimates for rhythm time steps. Turn this into a vector of time probabilities.\nBegin a Track Composition loop\n\n\nLoop for each of 3 tracks\n\nBegin a Bar composition Loop\n\nLoop for 16 bars\npick a collection of notes\nassign probability of occurrence\ngenerate time stamps, and notes from time and note probability vectors\nCombine them together\nput them in a one_bar matrix\n\nput all the bars into a midi_df, setting i_track to the track number\n\n\nIt was helpful to summarize the code this morning. It’s unlikely I will begin any re-factoring, but it’s easier to see where I could make this more compact and less of a wall of code."
  },
  {
    "objectID": "blog/45_2_11_24_eurorack/index.html",
    "href": "blog/45_2_11_24_eurorack/index.html",
    "title": "Eurorack notes to self",
    "section": "",
    "text": "Code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"Massive modular synthesizer. Eurorack modular synthesizer. Huge synthesizer room, full or modular synthesizers with patch cords connecting everywer. 80s cartoon. Linocut.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nNotes to self about eurorack stuff."
  },
  {
    "objectID": "blog/45_2_11_24_eurorack/index.html#programmable-eurorack-modules",
    "href": "blog/45_2_11_24_eurorack/index.html#programmable-eurorack-modules",
    "title": "Eurorack notes to self",
    "section": "Programmable Eurorack Modules",
    "text": "Programmable Eurorack Modules\nI very briefly tried to program ornaments and crime one time. But that’s about it. I could see becoming interested in putting ideas from {midiblender}, and other ideas, into eurorack in different ways.\n\nolder post on mod wiggler about open source programmable eurorack modules https://modwiggler.com/forum/viewtopic.php?t=220061\na useful list of programmable audio platforms https://blog.macieksypniewski.com/2019/07/08/programmable-audio-platforms/\nBela pepper https://shop.bela.io/collections/modular/products/pepper\nMonome has some interesting stuff\nEuroPi Based on raspberry pi pico I think\nEs8 or ES-9 USB-audio interface…I could use one of these in general I think…hmm."
  },
  {
    "objectID": "blog/45_2_11_24_eurorack/index.html#eurorack-composition-ideas",
    "href": "blog/45_2_11_24_eurorack/index.html#eurorack-composition-ideas",
    "title": "Eurorack notes to self",
    "section": "Eurorack composition ideas",
    "text": "Eurorack composition ideas\nI don’t really want to mess with my computer while messing with eurorack at the same time, as a general rule.\n\nEasy can do right now\n\nUse {midiblender} to compose MIDI files with some things I want, play them through hermod. This gives me 8-voices. Probably the easiest thing I can do right now. Benefits are that the midi files are pre-packaged, and I shouldn’t have to worry about any latency or anything like that.\n\n\nI like this pattern for a bunch of reasons. I focus on some compositional ideas outside of eurorack. Generate all the midi based on the compositional ideas…which basically acts as CV and gate sources. Hook it all up to voices on, and wreck the whole thing as much as possible by turning knobs during playback. Sounds fun. Feels a little bit locked in to the compositional direction from the midi files, but I guess that’s the whole point of composing something.\n\n\nThis is somewhere in the middle, but I wonder about a way to rapidly scroll through a bunch of midi files, while they are playing, kind of like how one might scroll through a wavetable and hear all of the notes. If I had banks of possible midi files to play, and could easily scroll them, that would be fun. hmmmm.\n\n\n\nlonger term can’t do right now\n\nHave a module (or computer program) that can accumulate CV or midi from a live performance, crunch the probabilities at various levels (e.g., into various markov chain ideas, or other weird stuff), and do interesting generative sequencing based off of the live performance statistics."
  },
  {
    "objectID": "blog/44_2_10_24_euclid/index.html#randomly-sampling-note-probability-distributions",
    "href": "blog/44_2_10_24_euclid/index.html#randomly-sampling-note-probability-distributions",
    "title": "Euclidean Rhythms and circulating sequences with information theory concepts",
    "section": "Randomly sampling note probability distributions",
    "text": "Randomly sampling note probability distributions\nThe above code samples notes from some collection, like {C, F, G, Bb, D} with equal proportion. I’d like to at least start with making the proportions unequal, and have this move around a bit over bars. Whatever note has the highest proportion might take on the tonal center of the evolving sequence.\nA next step would be to expand and contract the note collection, and have it rotate around the circle of fifths.\nThis seems to work. I’m grabbing a colleciton of notes, using a simple vector, like 1,2,3,4,5, to set note frequencies. That can be multiplied to change the ratio of most to least frequent. The frequency vector is converted to a probability vector later. Every bar, the ordering of the probalities as they apply to notes is randomized. That could be too fast moving to really notice differences. Need to explore this a little bit.\n\n\nCode\nlibrary(midiblender)\nlibrary(dplyr)\nlibrary(tibble)\n# import midi\n# using mario to get the midi headers\n# need to add a vanilla midi file to this package for easier importing\nmario &lt;- midi_to_object(\"all_overworld.mid\")\nlist2env(mario, .GlobalEnv) # send objects to global environment\n\n\n# Sample some beats for timesteps into the matrix\nbars &lt;- 1\nnotes &lt;- 128\nticks &lt;- 96*4*bars\n\nall_beat_matrix &lt;- matrix(0,nrow=notes,ncol=96*4)\n\nfor(i in 1:8) {\n  # empty beat matrix\n  beat_matrix &lt;- matrix(0, nrow = notes, ncol = ticks)\n  \n  #assign kick beats\n  kick &lt;- bresenham_euclidean(sample(2:7,1), 16, start = 1)\n  snare &lt;- bresenham_euclidean(sample(2:7,1), 16, start = 1)\n  hihats &lt;- bresenham_euclidean(sample(8:16,1), 16, start = 1)\n  \n  # assign to matrix\n  beat_matrix[(36 + 1), seq(1, ticks, ticks / 16)] &lt;- kick\n  beat_matrix[(38 + 1), seq(1, ticks, ticks / 16)] &lt;- snare\n  beat_matrix[(42 + 1), seq(1, ticks, ticks / 16)] &lt;- hihats\n  \n  all_beat_matrix &lt;- cbind(all_beat_matrix, beat_matrix)\n}\n\n# get the point estimates\ntime_probabilities &lt;- colSums(all_beat_matrix)/sum(all_beat_matrix)\n\nbar_probabilities &lt;- matrix(time_probabilities,\n                            ncol=(96*4),\n                            byrow = T)\n\nbar_probabilities &lt;- colMeans(bar_probabilities)\n\n# make my own midi notes df\n# add this to midiblender later\nmidi_notes &lt;- pyramidi::midi_defs %&gt;%\n  rowwise() %&gt;%\n  mutate(note_letter = unlist(strsplit(as.character(note_name),\"-\"))[1],\n         octave = unlist(strsplit(as.character(note_name),\"-\"))[2])\n\nmidi_notes &lt;- tibble(notes = rep(midi_notes[1:12,]$note_letter,11)[1:128],\n                     octaves = rep(-1:9, each = 12)[1:128],\n                     midi_number = 0:127\n)\n\n# Empty data frame to hold midi track dfs\nall_track_midi &lt;- data.frame()\n\n# paramaters that get changed for every track\n\ntime_densities &lt;- c(256,128,64) # controls number of notes per bar \n\n# track loop\nfor(t in 1:3){\n\nsong &lt;- matrix(0,nrow=notes,ncol=1)\n\n# begin bar by bar composition loop\n\nfor(i in 1:16){\n\n  # get some notes to sample from fifths around a starting note\n  starting_note &lt;- 60 # C4\n  fifth_intervals &lt;- c(0, 7, 14, -7, -14)\n  notes_to_choose &lt;- starting_note + fifth_intervals\n  octave_range &lt;- 3:6\n  # starting to bias note sampling here\n  note_frequencies &lt;- sample(1:length(fifth_intervals)*2) \n  \n  # pair the frequencies with the notes\n  note_names &lt;- midi_notes %&gt;%\n    filter(midi_number %in% notes_to_choose) %&gt;%\n    select(notes) %&gt;%\n    mutate(note_frequencies = note_frequencies)\n  \n  # get all possible notes across octaves in collection\n  possible_notes &lt;- midi_notes %&gt;%\n    filter(notes %in% note_names$notes == TRUE,\n           octaves %in% octave_range == TRUE) %&gt;%\n    # add note frequencies\n    left_join(note_names,by=\"notes\") %&gt;%\n    mutate(probs = note_frequencies/sum(note_frequencies))\n  \n  # create probability vector\n  pitch_probabilities &lt;- rep(0, 128)\n  pitch_probabilities[possible_notes$midi_number + 1] &lt;-\n    possible_notes$probs\n  \n  # get new pitches\n  new_pitches &lt;- rbinom(n = length(pitch_probabilities),\n                        size = 32,\n                        prob = pitch_probabilities)\n  new_pitches[new_pitches &gt; 1] &lt;- 1\n  \n  # get new times\n  new_times &lt;- rbinom(n = length(bar_probabilities),\n                      size = time_densities[t],\n                      prob = bar_probabilities)\n  # To Do: come back here and make it ok to have more than 1\n  new_times[new_times &gt; 1] &lt;- 1\n  \n  # get row column ids\n  sampled_notes &lt;- which(new_pitches == 1)\n  sampled_times &lt;- which(new_times == 1)\n  \n  # combine, make sure equal length\n  if (length(sampled_notes) &gt;= length(sampled_times)) {\n    sampled_ids &lt;-\n      tibble(notes = sampled_notes[1:length(sampled_times)],\n             times = sampled_times)\n  } else {\n    sampled_ids &lt;- tibble(notes = sampled_notes,\n                          times = sampled_times[1:length(sampled_notes)])\n  }\n  \n  # shuffle the notes across the times so the sampling is uniform\n  sampled_ids$notes &lt;- sample(sampled_ids$notes)\n  \n  # make a note by time unit matrix\n  one_bar &lt;- matrix(0,\n                    nrow = 128,\n                    ncol = 96 * 4)\n  \n  # assign 1s to note locations in time\n  for (r in 1:dim(sampled_ids)[1]) {\n    one_bar[sampled_ids$notes[r], sampled_ids$times[r]] &lt;- 1\n  }\n  \n  song &lt;- cbind(song,one_bar)\n}\n\n#####################\n# transform back to midi\ntrack_0 &lt;- copy_midi_df_track(midi_df,track_num = 0)\n\nmidi_time_df &lt;- matrix_to_midi_time(midi_matrix = song,\n                                    smallest_time_unit = 1,\n                                    note_off_length = 32)\n\nmeta_messages_df &lt;- get_midi_meta_df(track_0)\n\nmeta_messages_df &lt;- set_midi_tempo_meta(meta_messages_df,update_tempo = 500000)\n\nsplit_meta_messages_df &lt;- split_meta_df(meta_messages_df)\n\nnew_midi_df &lt;- matrix_to_midi_track(midi_time_df = midi_time_df,\n                                    split_meta_list = split_meta_messages_df,\n                                    channel = 0,\n                                    velocity = 100)\nnew_midi_df &lt;- new_midi_df %&gt;%\n  mutate(i_track = t)\n\nall_track_midi &lt;- rbind(all_track_midi,\n                        new_midi_df)\n}\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(all_track_midi)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"fifths_tracks_prob.mid\")\n\n######\n# using Ableton for sounds\n\n\n  \nAt some point I put three tracks into Ableton and chose some synth voices."
  },
  {
    "objectID": "blog/44_2_10_24_euclid/index.html#expandingreducing-the-note-collection",
    "href": "blog/44_2_10_24_euclid/index.html#expandingreducing-the-note-collection",
    "title": "Euclidean Rhythms and circulating sequences with information theory concepts",
    "section": "Expanding/reducing the note collection",
    "text": "Expanding/reducing the note collection\nIncreased the number of tracks to 4. Every bar the number of notes that can get sampled is randomly varied from a collection of notes around the circle of fifths. I didn’t yet get the composition spinning around the circle, but I’m skipping that for now.\n\n\nCode\nlibrary(midiblender)\nlibrary(dplyr)\nlibrary(tibble)\n# import midi\n# using mario to get the midi headers\n# need to add a vanilla midi file to this package for easier importing\nmario &lt;- midi_to_object(\"all_overworld.mid\")\nlist2env(mario, .GlobalEnv) # send objects to global environment\n\n\n# Sample some beats for timesteps into the matrix\nbars &lt;- 1\nnotes &lt;- 128\nticks &lt;- 96*4*bars\n\nall_beat_matrix &lt;- matrix(0,nrow=notes,ncol=96*4)\n\nfor(i in 1:8) {\n  # empty beat matrix\n  beat_matrix &lt;- matrix(0, nrow = notes, ncol = ticks)\n  \n  #assign kick beats\n  kick &lt;- bresenham_euclidean(sample(2:7,1), 16, start = 1)\n  snare &lt;- bresenham_euclidean(sample(2:7,1), 16, start = 1)\n  hihats &lt;- bresenham_euclidean(sample(8:16,1), 16, start = 1)\n  \n  # assign to matrix\n  beat_matrix[(36 + 1), seq(1, ticks, ticks / 16)] &lt;- kick\n  beat_matrix[(38 + 1), seq(1, ticks, ticks / 16)] &lt;- snare\n  beat_matrix[(42 + 1), seq(1, ticks, ticks / 16)] &lt;- hihats\n  \n  all_beat_matrix &lt;- cbind(all_beat_matrix, beat_matrix)\n}\n\n# get the point estimates\ntime_probabilities &lt;- colSums(all_beat_matrix)/sum(all_beat_matrix)\n\nbar_probabilities &lt;- matrix(time_probabilities,\n                            ncol=(96*4),\n                            byrow = T)\n\nbar_probabilities &lt;- colMeans(bar_probabilities)\n\n# make my own midi notes df\n# add this to midiblender later\nmidi_notes &lt;- pyramidi::midi_defs %&gt;%\n  rowwise() %&gt;%\n  mutate(note_letter = unlist(strsplit(as.character(note_name),\"-\"))[1],\n         octave = unlist(strsplit(as.character(note_name),\"-\"))[2])\n\nmidi_notes &lt;- tibble(notes = rep(midi_notes[1:12,]$note_letter,11)[1:128],\n                     octaves = rep(-1:9, each = 12)[1:128],\n                     midi_number = 0:127\n)\n\n# Empty data frame to hold midi track dfs\nall_track_midi &lt;- data.frame()\n\n# paramaters that get changed for every track\n\ntime_densities &lt;- c(512,256,128,64) # controls number of notes per bar \n\n# track loop\nfor(t in 1:4){\n\nsong &lt;- matrix(0,nrow=notes,ncol=1)\n\n# begin bar by bar composition loop\n\nfor(i in 1:32){\n\n  # get some notes to sample from fifths around a starting note\n  starting_note &lt;- 60 # C4\n  fifth_intervals &lt;- c(0, 7, 14, -7, -14, 21, -21)\n  # size of collection now changes every bar\n  notes_to_choose &lt;- starting_note + sample(fifth_intervals,sample(3:7,1))\n  octave_range &lt;- 3:6\n  # starting to bias note sampling here\n  note_frequencies &lt;- sample(1:length(notes_to_choose)*2) \n  \n  # pair the frequencies with the notes\n  note_names &lt;- midi_notes %&gt;%\n    filter(midi_number %in% notes_to_choose) %&gt;%\n    select(notes) %&gt;%\n    mutate(note_frequencies = note_frequencies)\n  \n  # get all possible notes across octaves in collection\n  possible_notes &lt;- midi_notes %&gt;%\n    filter(notes %in% note_names$notes == TRUE,\n           octaves %in% octave_range == TRUE) %&gt;%\n    # add note frequencies\n    left_join(note_names,by=\"notes\") %&gt;%\n    mutate(probs = note_frequencies/sum(note_frequencies))\n  \n  # create probability vector\n  pitch_probabilities &lt;- rep(0, 128)\n  pitch_probabilities[possible_notes$midi_number + 1] &lt;-\n    possible_notes$probs\n  \n  # get new pitches\n  new_pitches &lt;- rbinom(n = length(pitch_probabilities),\n                        size = 32,\n                        prob = pitch_probabilities)\n  new_pitches[new_pitches &gt; 1] &lt;- 1\n  \n  # get new times\n  new_times &lt;- rbinom(n = length(bar_probabilities),\n                      size = time_densities[t],\n                      prob = bar_probabilities)\n  # To Do: come back here and make it ok to have more than 1\n  new_times[new_times &gt; 1] &lt;- 1\n  \n  # make a note by time unit matrix\n    one_bar &lt;- matrix(0,\n                      nrow = 128,\n                      ncol = 96 * 4)\n  \n  if(sum(new_times) &gt; 1){\n    # get row column ids\n    sampled_notes &lt;- which(new_pitches == 1)\n    sampled_times &lt;- which(new_times == 1)\n    \n    # combine, make sure equal length\n    if (length(sampled_notes) &gt;= length(sampled_times)) {\n      sampled_ids &lt;-\n        tibble(notes = sampled_notes[1:length(sampled_times)],\n               times = sampled_times)\n    } else {\n      sampled_ids &lt;- tibble(notes = sampled_notes,\n                            times = sampled_times[1:length(sampled_notes)])\n    }\n    \n    # shuffle the notes across the times so the sampling is uniform\n    sampled_ids$notes &lt;- sample(sampled_ids$notes)\n    \n    # assign 1s to note locations in time\n    for (r in 1:dim(sampled_ids)[1]) {\n      one_bar[sampled_ids$notes[r], sampled_ids$times[r]] &lt;- 1\n    }\n  }\n  \n  song &lt;- cbind(song,one_bar)\n}\n\n#####################\n# transform back to midi\ntrack_0 &lt;- copy_midi_df_track(midi_df,track_num = 0)\n\nmidi_time_df &lt;- matrix_to_midi_time(midi_matrix = song,\n                                    smallest_time_unit = 1,\n                                    note_off_length = 32)\n\nmeta_messages_df &lt;- get_midi_meta_df(track_0)\n\nmeta_messages_df &lt;- set_midi_tempo_meta(meta_messages_df,update_tempo = 500000)\n\nsplit_meta_messages_df &lt;- split_meta_df(meta_messages_df)\n\nnew_midi_df &lt;- matrix_to_midi_track(midi_time_df = midi_time_df,\n                                    split_meta_list = split_meta_messages_df,\n                                    channel = 0,\n                                    velocity = 100)\nnew_midi_df &lt;- new_midi_df %&gt;%\n  mutate(i_track = t)\n\nall_track_midi &lt;- rbind(all_track_midi,\n                        new_midi_df)\n}\n\n#### bounce\n\n# update miditapyr df\nmiditapyr_object$midi_frame_unnested$update_unnested_mf(all_track_midi)\n\n#write midi file to disk\nmiditapyr_object$write_file(\"fifths_4_tracks_prob_2.mid\")\n\n######\n# using Ableton for sounds\n\n\n  \nThis one is again using ableton for sounds, with a little bit more care taken to make it sound mildly interesting. Had fun. Still sounds like halting robot music that drones on without much change in direction.\nAlright, finished here for now."
  },
  {
    "objectID": "blog/46_2_12_24_dplyr_midi/index.html",
    "href": "blog/46_2_12_24_dplyr_midi/index.html",
    "title": "MIDI composition with {dplyr}, {midiblender}, and {pyramidi}",
    "section": "",
    "text": "Code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"composing music with rstats. dplyr musical notes. tidyverse. hexagons everywhere. 3d. music inside hexagons floating in the universe. musical notes inside the hexagons. cartoon. linocut\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nThis morning I added some dplyr style functions to midiblender for row-by-row, explicit construction of a data frame containing midi information. I’ll provide some examples here, and use this post to try a few compositional goals with the dplyr approach.\npyramidi also has examples of composition with dplyr syntax that are worth checking out. My approach relies on several pyramidi internal functions, and is pretty similar overall."
  },
  {
    "objectID": "blog/46_2_12_24_dplyr_midi/index.html#midi-dataframe-construction-with-dplyr",
    "href": "blog/46_2_12_24_dplyr_midi/index.html#midi-dataframe-construction-with-dplyr",
    "title": "MIDI composition with {dplyr}, {midiblender}, and {pyramidi}",
    "section": "Midi dataframe construction with {dplyr}",
    "text": "Midi dataframe construction with {dplyr}\nSome of these examples are also in the vignette, Midi data frame constructors with dplyr.\nThe following block of code shows an example of systematically creating a midi data frame, on a row-by-row basis. The functions are mostly wrappers to dplyr::add_row, but they have midi variables in them.\nFunctions that add a row start with “add”, which makes them easy to find with autocomplete while programming. These functions also pass ..., which is useful for placing a row .before or .after another row.\nThis code block writes important meta messages, shows that it is possible to write program_change and control_change messages, and writes a couple notes before ending the track.\n\n\nCode\nlibrary(midiblender)\nlibrary(pyramidi)\n\nnew_midi_df &lt;- create_empty_midi_df() %&gt;% # initialize columns\n  add_meta_track_name(name = \"My track\") %&gt;%\n  add_meta_tempo(tempo = 500000) %&gt;%\n  add_meta_time_sig(numerator = 4,\n                    denominator = 4,\n                    clocks_per_click = 36,\n                    notated_32nd_notes_per_beat = 8) %&gt;%\n  add_program_change(program = 0,\n                     channel = 0) %&gt;%\n  add_control_change(control = 0,value = 0) %&gt;%\n  add_note_on(time = 0,note = 60) %&gt;%\n  add_note_off(time = 8,note = 60) %&gt;%\n  add_meta_end_of_track()\n\n# print the data frame.\nknitr::kable(new_midi_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ni_track\nmeta\ntype\nname\ntime\nnumerator\ndenominator\nclocks_per_click\nnotated_32nd_notes_per_beat\nprogram\nchannel\ncontrol\nvalue\nnote\nvelocity\ntempo\n\n\n\n\n0\nTRUE\ntrack_name\nMy track\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n0\nTRUE\nset_tempo\nNA\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5e+05\n\n\n0\nTRUE\ntime_signature\nNA\n0\n4\n4\n36\n8\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n0\nFALSE\nprogram_change\nNA\n0\nNA\nNA\nNA\nNA\n0\n0\nNA\nNA\nNA\nNA\nNA\n\n\n0\nFALSE\ncontrol_change\nNA\n0\nNA\nNA\nNA\nNA\nNA\n0\n0\n0\nNA\nNA\nNA\n\n\n0\nFALSE\nnote_on\nNA\n0\nNA\nNA\nNA\nNA\nNA\n0\nNA\nNA\n60\n0\nNA\n\n\n0\nFALSE\nnote_off\nNA\n8\nNA\nNA\nNA\nNA\nNA\n0\nNA\nNA\n60\n0\nNA\n\n\n0\nTRUE\nend_of_track\nNA\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "blog/46_2_12_24_dplyr_midi/index.html#pyramidi-object-creation-and-export",
    "href": "blog/46_2_12_24_dplyr_midi/index.html#pyramidi-object-creation-and-export",
    "title": "MIDI composition with {dplyr}, {midiblender}, and {pyramidi}",
    "section": "Pyramidi object creation and export",
    "text": "Pyramidi object creation and export\nThis shows how to take a midi dataframe like the above, and export it to disk as a .mid file.\nFirst, pyramidi::MidiFramer$new() creates a new pyramidi object.\nUpon creation, it is possible to update new_pyramidi_object$ticks_per_beat with a new value. I believe the default value is 960L, and the code below updates it to a smaller value.\nThe new_midi_df from above is then updated within the pyramidi object using new_pyramidi_object$mf$midi_frame_unnested$update_unnested_mf(new_midi_df).\nFinally, new_pyramidi_object$mf$write_file(\"file_name.mid\") writes the file to disk.\n\n\nCode\n#Initialize new pyramidi object\nnew_pyramidi_object &lt;- pyramidi::MidiFramer$new()\n\n# update ticks per beat\nnew_pyramidi_object$ticks_per_beat &lt;- 96L\n\n# update object with new midi df\nnew_pyramidi_object$mf$midi_frame_unnested$update_unnested_mf(new_midi_df)\n\n# write to midi file\nnew_pyramidi_object$mf$write_file(\"file_name.mid\")"
  },
  {
    "objectID": "blog/46_2_12_24_dplyr_midi/index.html#composing-with-a-wide-data-frame-then-pivoting-to-long",
    "href": "blog/46_2_12_24_dplyr_midi/index.html#composing-with-a-wide-data-frame-then-pivoting-to-long",
    "title": "MIDI composition with {dplyr}, {midiblender}, and {pyramidi}",
    "section": "Composing with a wide data frame, then pivoting to long",
    "text": "Composing with a wide data frame, then pivoting to long\nMidi files writes note_on and note_off messages in succession with time stamps coding relative time since the last message. For composition, it may be convenient to work with a wider data frame.\nHere’s an example of using dplyr to create a series of notes over time. The notes are randomly chosen from the vector possible_notes. The rhythm defining when a note occurs is generated by bresenham_euclidean(), which can produce some nice sounding rhythms. The code chunk will generate as many bars as defined in bars, and the shortest note is a 16th note.\n\n\nCode\nlibrary(dplyr)\n\n# note parameters\nbars &lt;- 4 # number of bars\nbar_time_steps &lt;- 16 # number of time steps in a bar\nnote_duration &lt;- 24 # note duration in ticks\npossible_notes &lt;- c(60, 63, 65, 66, 67, 70, 72, 75) # midi note values to pick\n\n# create a tibble to compose notes in time \ncompose_notes &lt;- tibble::tibble(note_id = integer(),\n                             note = integer(),\n                             beat_on = integer(),\n                             note_on = integer(),\n                             note_off = integer()) %&gt;%\n  # add multiple bars worth of notes\n  add_row(beat_on = c(replicate(bars,bresenham_euclidean(sample(c(2:15),1),\n                                            bar_time_steps,\n                                            start=1))),\n          note = sample(possible_notes, \n                        size = bar_time_steps*bars, \n                        replace= TRUE)\n          ) %&gt;%\n  # handle note times\n  mutate(note_id = 1:n(),\n         note_on = (1:n()-1)*note_duration,\n         note_off = note_on+note_duration) %&gt;%\n  # keep events where a beat occurred\n  filter(beat_on == 1) \n\n#print to show\nknitr::kable(head(compose_notes))\n\n\n\n\n\nnote_id\nnote\nbeat_on\nnote_on\nnote_off\n\n\n\n\n1\n67\n1\n0\n24\n\n\n5\n72\n1\n96\n120\n\n\n9\n65\n1\n192\n216\n\n\n13\n66\n1\n288\n312\n\n\n17\n67\n1\n384\n408\n\n\n19\n63\n1\n432\n456\n\n\n\n\n\nAt this point the compose_notes dataframe contains note_on and note_off information in wide format. A quick call to tidyr::pivot_longer, along with subtracting the time stamps to get them into relative time, and we have a dataframe that is nearly ready for export.\n\n\nCode\ncompose_notes &lt;- compose_notes %&gt;%\n  # pivot to long\n  tidyr::pivot_longer(c(\"note_on\",\"note_off\"),names_to=\"type\",values_to=\"time\") %&gt;%\n  # relative time\n  mutate(time = time - lag(time,default=0))\n\n#print to show\nknitr::kable(head(compose_notes))\n\n\n\n\n\nnote_id\nnote\nbeat_on\ntype\ntime\n\n\n\n\n1\n67\n1\nnote_on\n0\n\n\n1\n67\n1\nnote_off\n24\n\n\n5\n72\n1\nnote_on\n72\n\n\n5\n72\n1\nnote_off\n24\n\n\n9\n65\n1\nnote_on\n72\n\n\n9\n65\n1\nnote_off\n24\n\n\n\n\n\nNow we have a body of midi messages. The last step is to put them into a full-fledged midi dataframe, and export them.\n\n\nCode\n# create new midi_df\n\n## add to a new midi df\nnew_midi_df &lt;- create_empty_midi_df() %&gt;% # initialize\n  add_meta_track_name(name = \"My track\") %&gt;%\n  add_meta_tempo(tempo = 500000) %&gt;%\n  add_meta_time_sig(\n    numerator = 4,\n    denominator = 4,\n    clocks_per_click = 36,\n    notated_32nd_notes_per_beat = 8\n  ) %&gt;%\n  add_program_change(program = 0,\n                     channel = 0) %&gt;%\n  add_control_change(control = 0, value = 0) %&gt;%\n  # add new notes &lt;---------------Adding stuff from compose_notes\n  add_row(i_track = rep(0,dim(compose_notes)[1]), \n          meta = rep(FALSE,dim(compose_notes)[1]),\n          note = compose_notes$note,\n          type = compose_notes$type,\n          time = compose_notes$time,\n          velocity = 64) %&gt;%\n  add_meta_end_of_track()\n\n#write midi\n\n#Initialize new pyramidi object\nnew_pyramidi_object &lt;- pyramidi::MidiFramer$new()\n# update ticks per beat\nnew_pyramidi_object$ticks_per_beat &lt;- 96L\n# update object with new midi df\nnew_pyramidi_object$mf$midi_frame_unnested$update_unnested_mf(new_midi_df)\n# write to midi file\nnew_pyramidi_object$mf$write_file(\"file_name.mid\")"
  },
  {
    "objectID": "blog/46_2_12_24_dplyr_midi/index.html#making-it-less-of-a-wall-of-code",
    "href": "blog/46_2_12_24_dplyr_midi/index.html#making-it-less-of-a-wall-of-code",
    "title": "MIDI composition with {dplyr}, {midiblender}, and {pyramidi}",
    "section": "Making it less of a wall of code",
    "text": "Making it less of a wall of code\nI’m not working on this right now. Going with walls of code."
  },
  {
    "objectID": "blog/46_2_12_24_dplyr_midi/index.html#frequency-biased-sequences",
    "href": "blog/46_2_12_24_dplyr_midi/index.html#frequency-biased-sequences",
    "title": "MIDI composition with {dplyr}, {midiblender}, and {pyramidi}",
    "section": "Frequency biased sequences",
    "text": "Frequency biased sequences\nI’ve got a project coming up where I will likely need to generate a whole bunch of “musical” sequences with specific kinds of statistical structure. I’m not sure whether I will use the {dplyr} style code for this. This is a note to future self about what it might look like.\n\ngot something that will work later\nneed to write functions for creating unequal frequency vectors with specific constraints\nMove development on this issue to another castle.\n\n\n\nCode\n# note parameters\nbars &lt;- 4\npossible_time_steps &lt;- 16\nnote_duration &lt;- 24\npossible_notes &lt;- c(60, 63, 65, 66, 67, 70, 72, 75)\n\ntotal_notes &lt;- 8\ntotal_beats &lt;- bars*possible_time_steps\n\n# need to work out some algorithms for unequal frequency distribution generation, these are enough for an example\nequal_frequencies &lt;- rep(total_beats/8,8)\nhalf_frequencies &lt;- equal_frequencies + (equal_frequencies * rep(c(.5,-.5),each = total_notes/2))\nmost_unequal_frequencies &lt;- c(rep(1,(total_notes-1)),(total_notes*(total_notes-1)+1))\n\nnote_frequency_matrix &lt;- rbind(equal_frequencies,\n                              half_frequencies,\n                              most_unequal_frequencies)\n\ncompose_notes &lt;- tibble::tibble(note_id = integer(),\n                             note = integer(),\n                             beat_on = integer(),\n                             note_on = integer(),\n                             note_off = integer()) %&gt;%\n  # 1 beat every time_step\n  rowwise() %&gt;%\n  add_row(beat_on = 1,\n          note = sample(rep(sample(possible_notes),\n                     times = note_frequency_matrix[3,])),\n          ) %&gt;%\n  ungroup() %&gt;%\n  # handle note times\n  mutate(note_id = 1:n(),\n         note_on = (1:n()-1)*note_duration,\n         note_off = note_on+note_duration) %&gt;%\n  filter(beat_on == 1) %&gt;%\n  #pivot to long\n  tidyr::pivot_longer(c(\"note_on\",\"note_off\"),names_to=\"type\",values_to=\"time\") %&gt;%\n  mutate(time = time - lag(time,default=0))\n\n## add to a new midi df\nnew_midi_df &lt;- create_empty_midi_df() %&gt;% # initialize\n  add_meta_track_name(name = \"My track\") %&gt;%\n  add_meta_tempo(tempo = 500000) %&gt;%\n  add_meta_time_sig(\n    numerator = 4,\n    denominator = 4,\n    clocks_per_click = 36,\n    notated_32nd_notes_per_beat = 8\n  ) %&gt;%\n  add_program_change(program = 0,\n                     channel = 0) %&gt;%\n  add_control_change(control = 0, value = 0) %&gt;%\n  #use dplyr::add_row to add a bunch of notes\n  add_row(i_track = rep(0,dim(compose_notes)[1]), \n          meta = rep(FALSE,dim(compose_notes)[1]),\n          note = compose_notes$note,\n          type = compose_notes$type,\n          time = compose_notes$time,\n          velocity = 64) %&gt;%\n  add_meta_end_of_track()\n\n#write midi\n#Initialize new pyramidi object\nnew_pyramidi_object &lt;- pyramidi::MidiFramer$new()\n# update ticks per beat\nnew_pyramidi_object$ticks_per_beat &lt;- 96L\n# update object with new midi df\nnew_pyramidi_object$mf$midi_frame_unnested$update_unnested_mf(new_midi_df)\n# write to midi file\nnew_pyramidi_object$mf$write_file(\"most_unequal.mid\")"
  },
  {
    "objectID": "blog/46_2_12_24_dplyr_midi/index.html#bar-blues-sequence",
    "href": "blog/46_2_12_24_dplyr_midi/index.html#bar-blues-sequence",
    "title": "MIDI composition with {dplyr}, {midiblender}, and {pyramidi}",
    "section": "12-bar blues sequence",
    "text": "12-bar blues sequence\nI like a good blues scale. Undoubtedly, whatever happens next won’t be bluesy, but that’s ok.\nAlgorithm components\n\nSample notes from a blues scale\nEavery bar, randomly sample a euclidean rhythm to fill the bar\nrandomly assign notes to the beats in the euclidean rhythm\nCreate a few tracks of this.\ntry shifting chords, do 12 bar blues\n\nGot it working, nice\n\n\nCode\nall_tracks &lt;- data.frame()\n# loop for each track\nfor(t in 1:3) {\n  \n  # note parameters\n  bars &lt;- 12 * 4 * 4\n  possible_time_steps &lt;- 16\n  note_duration &lt;- 24\n  possible_notes &lt;- c(60, 63, 65, 66, 67, 70, 72, 75)\n  \n  key_vector &lt;- rep(rep(c(0,0,0,0,5,5,0,0,7,5,0,7),each=possible_time_steps),4*4)\n  \n  compose_notes &lt;- tibble::tibble(\n    note_id = integer(),\n    note = integer(),\n    beat_on = integer(),\n    note_on = integer(),\n    note_off = integer()\n  ) %&gt;%\n    # use euclidean rhythm\n    rowwise() %&gt;%\n    add_row(\n      beat_on = c(replicate(\n        bars,\n        bresenham_euclidean(sample(c(1, 2, 2, 2, 3, 4,5,5,6, 8, 15), 1),\n                            possible_time_steps,\n                            start = 1)\n      )),\n      note = sample(possible_notes,\n                    size = possible_time_steps * bars,\n                    replace = TRUE) + key_vector\n    ) %&gt;%\n    ungroup() %&gt;%\n    # handle note times\n    mutate(\n      note_id = 1:n(),\n      note_on = (1:n() - 1) * note_duration,\n      note_off = note_on + note_duration\n    ) %&gt;%\n    filter(beat_on == 1) %&gt;%\n    #pivot to long\n    tidyr::pivot_longer(c(\"note_on\", \"note_off\"),\n                        names_to = \"type\",\n                        values_to = \"time\") %&gt;%\n    mutate(time = time - lag(time, default = 0))\n  ######################\n  # End composition \n  \n  #######################\n  ## add composition to a new midi df\n  new_midi_df &lt;- create_empty_midi_df() %&gt;% # initialize\n    add_meta_track_name(name = \"My track\") %&gt;%\n    add_meta_tempo(tempo = 500000) %&gt;%\n    add_meta_time_sig(\n      numerator = 4,\n      denominator = 4,\n      clocks_per_click = 36,\n      notated_32nd_notes_per_beat = 8\n    ) %&gt;%\n    add_program_change(program = 0,\n                       channel = 0) %&gt;%\n    add_control_change(control = 0, value = 0) %&gt;%\n    # Composition added here\n    add_row(\n      i_track = rep(0, dim(compose_notes)[1]),\n      meta = rep(FALSE, dim(compose_notes)[1]),\n      note = compose_notes$note,\n      type = compose_notes$type,\n      time = compose_notes$time,\n      velocity = 64\n    ) %&gt;%\n    add_meta_end_of_track() %&gt;%\n    mutate(i_track = t) # set current track number\n  \n  all_tracks &lt;- rbind(all_tracks,new_midi_df)\n  \n}\n\n#write midi\n#Initialize new pyramidi object\nnew_pyramidi_object &lt;- pyramidi::MidiFramer$new()\n# update ticks per beat\nnew_pyramidi_object$ticks_per_beat &lt;- 96L\n# update object with new midi df\nnew_pyramidi_object$mf$midi_frame_unnested$update_unnested_mf(all_tracks)\n# write to midi file\nnew_pyramidi_object$mf$write_file(\"bluesy_A.mid\")\n\n\n  \nI put three midi tracks into ableton, added synth voices and drums, and that’s what it sounds like.\nHmmm, the dplyr style worked out. I didn’t think it would be so easy to get a 12-bar blues thing going. It still sounds like a computer made it, but a computer did make it, so that’s fair.\n\nAdding parameters per track in a list.\n\n\nCode\nlibrary(midiblender)\nlibrary(dplyr)\nlibrary(pyramidi)\n\nall_tracks &lt;- data.frame()\n\n# note parameters\n  bars &lt;- 12 * 4 * 4\n  possible_time_steps &lt;- 16\n  note_duration &lt;- 24\n  \n  track_params &lt;- list(track_1_bass =\n                         list(possible_notes = rep(60:72,   #1     2     3  4     5     6     7  8 \n                                                   times = c(6, 0, 3, 0, 0, 3, 0, 3, 0, 0, 3, 0, 6)), \n                            key_vector = rep(rep(c(0, 0, 0, 0, 5, 5, 0, 0, 7, 5, 0, 7), \n                                                 each = possible_time_steps), \n                                             times = 4 * 4)\n                           ),\n                       track_2_mid =\n                         list(possible_notes = rep(60:72,   #1     2     3  4     5     6     7  8 \n                                                   times = c(6, 0, 2, 0, 0, 2, 0, 3, 0, 0, 3, 0, 6)), \n                            key_vector = rep(rep(c(0, 0, 0, 0, 5, 5, 0, 0, 7, 5, 0, 7), \n                                                 each = possible_time_steps), \n                                             times = 4 * 4)\n                          ),\n                       track_3_mid =\n                         list(possible_notes = rep(60:72,   #1     2     3  4     5     6     7  8 \n                                                   times = c(1, 0, 0, 2, 0, 3, 2, 3, 0, 0, 2, 0, 1)), \n                            key_vector = rep(rep(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), \n                                                 each = possible_time_steps), \n                                             times = 4 * 4)\n                          )\n  )\n                       \n                           \n# loop for each track\nfor(t in 1:3) {\n  \n  compose_notes &lt;- tibble::tibble(\n    note_id = integer(),\n    note = integer(),\n    beat_on = integer(),\n    note_on = integer(),\n    note_off = integer()\n  ) %&gt;%\n    # use euclidean rhythm\n    rowwise() %&gt;%\n    add_row(\n      beat_on = c(replicate(\n        bars,\n        bresenham_euclidean(sample(c(1, 2, 2, 3, 4, 5, 5, 6, 8, 11, 13), 1), \n                            possible_time_steps,\n                            start = 1)\n      )),\n      note = sample(track_params[[t]]$possible_notes,\n                    size = possible_time_steps * bars,\n                    replace = TRUE) + track_params[[t]]$key_vector\n    ) %&gt;%\n    ungroup() %&gt;%\n    # handle note times\n    mutate(\n      note_id = 1:n(),\n      note_on = (1:n() - 1) * note_duration,\n      note_off = note_on + note_duration\n    ) %&gt;%\n    filter(beat_on == 1) %&gt;%\n    #pivot to long\n    tidyr::pivot_longer(c(\"note_on\", \"note_off\"),\n                        names_to = \"type\",\n                        values_to = \"time\") %&gt;%\n    mutate(time = time - lag(time, default = 0))\n  ######################\n  # End composition \n  \n  #######################\n  ## add composition to a new midi df\n  new_midi_df &lt;- create_empty_midi_df() %&gt;% # initialize\n    add_meta_track_name(name = \"My track\") %&gt;%\n    add_meta_tempo(tempo = 500000) %&gt;%\n    add_meta_time_sig(\n      numerator = 4,\n      denominator = 4,\n      clocks_per_click = 36,\n      notated_32nd_notes_per_beat = 8\n    ) %&gt;%\n    add_program_change(program = 0,\n                       channel = 0) %&gt;%\n    add_control_change(control = 0, value = 0) %&gt;%\n    # Composition added here\n    add_row(\n      i_track = rep(0, dim(compose_notes)[1]),\n      meta = rep(FALSE, dim(compose_notes)[1]),\n      note = compose_notes$note,\n      type = compose_notes$type,\n      time = compose_notes$time,\n      velocity = 64\n    ) %&gt;%\n    add_meta_end_of_track() %&gt;%\n    mutate(i_track = t) # set current track number\n  \n  all_tracks &lt;- rbind(all_tracks,new_midi_df)\n  \n}\n\n#write midi\n#Initialize new pyramidi object\nnew_pyramidi_object &lt;- pyramidi::MidiFramer$new()\n# update ticks per beat\nnew_pyramidi_object$ticks_per_beat &lt;- 96L\n# update object with new midi df\nnew_pyramidi_object$mf$midi_frame_unnested$update_unnested_mf(all_tracks)\n# write to midi file\nnew_pyramidi_object$mf$write_file(\"bluesy_C.mid\")"
  },
  {
    "objectID": "blog/47_2_13_24_midi_player/index.html",
    "href": "blog/47_2_13_24_midi_player/index.html",
    "title": "Playing a MIDI file on a Quarto blog",
    "section": "",
    "text": "Code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"Player piano. piano roll. Automatic self playing piano. white background. transformers cartoon. 3d.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\n\n\n\n\n\n\n\n\n\nThis is just a picture. Scroll down to try the MIDI player for real.\nI happened upon this midi player for webpages. Will it work here too? https://cifkao.github.io/html-midi-player/\nFollowing the instructions from that website, I am adding this script in the yml for this .qmd file, along with a resources line to make sure the midi file is copied to the docs folder.\n\n\nCode\nresources: \n  - bluesy_C.mid\ninclude-before-body:\n  text: |\n    &lt;script src=\"https://cdn.jsdelivr.net/combine/npm/tone@14.7.58,npm/@magenta/music@1.23.1/es6/core.js,npm/focus-visible@5,npm/html-midi-player@1.4.0\"&gt;&lt;/script&gt;\n\n\nAdded an html block that has this code:\n\n\nCode\n&lt;midi-player\n  src=\"bluesy_C.mid\"\n  sound-font visualizer=\"#myVisualizer\"&gt;\n&lt;/midi-player&gt;\n&lt;midi-visualizer type=\"piano-roll\" id=\"myVisualizer\"&gt;&lt;/midi-visualizer&gt;\n\n\nAnd, it works!\n\n\n\n\nCool, I have not looked deeply into this at all, so I have no idea what’s behind the sound generation. I’m just curious about what happens if I play a midi file that should have a few different voices. I choose the moon theme from the NES ducktales game. Such a great song.\n\n\n\nOH YA!!!!!"
  },
  {
    "objectID": "blog/48_2_15_24_r_fluid_synth/index.html",
    "href": "blog/48_2_15_24_r_fluid_synth/index.html",
    "title": "Testing out {fluidsynth} for R!",
    "section": "",
    "text": "Code\nfrom diffusers import DiffusionPipeline\nfrom transformers import set_seed\nfrom PIL import Image\nimport torch\nimport random\nimport ssl\nimport os\nssl._create_default_https_context = ssl._create_unverified_context\n\n#locate library\n#model_id = \"./stable-diffusion-v1-5\"\nmodel_id = \"dreamshaper-xl-turbo\"\n\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path = \"../../../../bigFiles/huggingface/dreamshaper-xl-turbo/\"\n)\n\npipeline = pipeline.to(\"mps\")\n\n# Recommended if your computer has &lt; 64 GB of RAM\npipeline.enable_attention_slicing(\"max\")\n\nprompt = \"a synthesizer made out of water. 3d. white background. super cool.\"\n\nfor s in range(30):\n  for n in [5,10]:\n    seed = s+21\n    num_steps = n+1\n    set_seed(seed)\n    \n    image = pipeline(prompt,height = 1024,width = 1024,num_images_per_prompt = 1,num_inference_steps=num_steps)\n    \n    image_name = \"images/synth_{}_{}.jpeg\"\n    \n    image_save = image.images[0].save(image_name.format(seed,num_steps))\nThanks to Jeroen Ooms, there is a new {fluidsynth} package for R!\nHere’s the github repo: https://github.com/ropensci/fluidsynth/\nThe package is only four days old and could change, but it is working for me!"
  },
  {
    "objectID": "blog/48_2_15_24_r_fluid_synth/index.html#playback-midi-locally",
    "href": "blog/48_2_15_24_r_fluid_synth/index.html#playback-midi-locally",
    "title": "Testing out {fluidsynth} for R!",
    "section": "playback midi locally",
    "text": "playback midi locally\n\n\nCode\nlibrary(fluidsynth)\n\n# this works for local playback!\n\nmidi_play(\n  midi = \"the-moon.mid\",\n  soundfont = \"~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2\",\n  audio.driver = NULL,\n  settings = list(),\n  progress = interactive()\n)"
  },
  {
    "objectID": "blog/48_2_15_24_r_fluid_synth/index.html#write-midi-to-wav",
    "href": "blog/48_2_15_24_r_fluid_synth/index.html#write-midi-to-wav",
    "title": "Testing out {fluidsynth} for R!",
    "section": "write midi to wav",
    "text": "write midi to wav\nI’m on a mac m2. The midi_convert function produces a wav file that will not play. However, the {av} package does successfully convert the wav file to a listenable mp3. Glad it works out in the end.\nI’m not sure why the wav file isn’t rendering as playable. I wonder if it isn’t finding my installation of libsndfile, and the wav file is being rendered as raw?\n\n\nCode\nmidi_convert(\n  midi = \"the-moon.mid\",\n  soundfont = \"~/Library/Audio/Sounds/Banks/FluidR3_GM.sf2\",\n  output = \"output.wav\",\n  settings = list(\n    audio.driver = \"coreaudio\",\n    audio.file.format = \"double\",\n    audio.file.type = \"wav\"),\n  progress = interactive()\n)\n\nav::av_audio_convert(\"output.wav\",\"output.mp3\")"
  },
  {
    "objectID": "blog/48_2_15_24_r_fluid_synth/index.html#fluid-synth-settings",
    "href": "blog/48_2_15_24_r_fluid_synth/index.html#fluid-synth-settings",
    "title": "Testing out {fluidsynth} for R!",
    "section": "fluid synth settings",
    "text": "fluid synth settings\nAll of these fluid synth parameters can now be easily modified from R!\n\n\nCode\nknitr::kable(fluidsynth::fluidsynth_setting_list())\n\n\n\n\n\nname\ntype\n\n\n\n\naudio.coreaudio.channel-map\nstring\n\n\naudio.coreaudio.device\nstring\n\n\naudio.driver\nstring\n\n\naudio.file.endian\nstring\n\n\naudio.file.format\nstring\n\n\naudio.file.name\nstring\n\n\naudio.file.type\nstring\n\n\naudio.period-size\ninteger\n\n\naudio.periods\ninteger\n\n\naudio.portaudio.device\nstring\n\n\naudio.realtime-prio\ninteger\n\n\naudio.sample-format\nstring\n\n\nmidi.autoconnect\ninteger\n\n\nmidi.coremidi.id\nstring\n\n\nmidi.driver\nstring\n\n\nmidi.portname\nstring\n\n\nmidi.realtime-prio\ninteger\n\n\nplayer.reset-synth\ninteger\n\n\nplayer.timing-source\nstring\n\n\nshell.port\ninteger\n\n\nshell.prompt\nstring\n\n\nsynth.audio-channels\ninteger\n\n\nsynth.audio-groups\ninteger\n\n\nsynth.chorus.active\ninteger\n\n\nsynth.chorus.depth\ndouble\n\n\nsynth.chorus.level\ndouble\n\n\nsynth.chorus.nr\ninteger\n\n\nsynth.chorus.speed\ndouble\n\n\nsynth.cpu-cores\ninteger\n\n\nsynth.default-soundfont\nstring\n\n\nsynth.device-id\ninteger\n\n\nsynth.dynamic-sample-loading\ninteger\n\n\nsynth.effects-channels\ninteger\n\n\nsynth.effects-groups\ninteger\n\n\nsynth.gain\ndouble\n\n\nsynth.ladspa.active\ninteger\n\n\nsynth.lock-memory\ninteger\n\n\nsynth.midi-bank-select\nstring\n\n\nsynth.midi-channels\ninteger\n\n\nsynth.min-note-length\ninteger\n\n\nsynth.overflow.age\ndouble\n\n\nsynth.overflow.important\ndouble\n\n\nsynth.overflow.important-channels\nstring\n\n\nsynth.overflow.percussion\ndouble\n\n\nsynth.overflow.released\ndouble\n\n\nsynth.overflow.sustained\ndouble\n\n\nsynth.overflow.volume\ndouble\n\n\nsynth.polyphony\ninteger\n\n\nsynth.reverb.active\ninteger\n\n\nsynth.reverb.damp\ndouble\n\n\nsynth.reverb.level\ndouble\n\n\nsynth.reverb.room-size\ndouble\n\n\nsynth.reverb.width\ndouble\n\n\nsynth.sample-rate\ndouble\n\n\nsynth.threadsafe-api\ninteger\n\n\nsynth.verbose\ninteger"
  }
]